#------------------------------------------------------------------
# Пример 3.1: Параллельное решение системы линейных уравнений
#             методом сопряжённых градиентов.
# Реализация №1: Пытаемся распараллелить все вычисления.
#
# Задача: найти вектор x, являющийся решением системы линейных
#         уравнений A*x = b, максимально эффективно параллелизуя
#         работу между процессами с использованием MPI интерфейса.
#
# Решение системы линейных уравнений A*x = b методом сопряжённых
# градиентов требует выполнения N итераций, где N - это размер
# вектора x. На каждой итерации мы будем обновлять значение
# вектора x в виде последовательности приближённых решений x_{s}
# на итерациях s от 1 до N+1.
#
# Чтобы начать итерации, нам нужно взять какое-то начальное
# приближение для x_{1} на первой итерации - но в целом, оно может
# быть любым, и обычно начинают с нулевого значения.
#
# Нам также понадобятся три вспомогательных вектора r, p, и q,
# значения которых также будут обновляться на каждой итерации s
# в виде последовательностей r_{s}, p_{s}, и q_{s}.
# Считаем при этом, что вектор p инициализирован нулём до начала
# итераций (то есть, для s=0).
#
# Тогда решение системы линейных уравнений может быть найдено
# методом сопряжённых градиентов по такому алгоритму:
#
#    x_{1} = 0 # или ваше приближение для решения!
#    p_{0} = 0
#    for s in range(1, N+1):
#        if s == 1:
#            r_{s} = dot(A.T, dot(A, x_{s}) - b)
#        else:
#            dot_pq = dot(p_{s-1}, q_{s-1})
#            r_{s} = r_{s-1} - q_{s-1} / dot_pq
#        dot_rr = dot(r_{s}, r_{s})
#        p_{s} = p_{s-1} + r_{s} / dot_rr
#        q_{s} = dot(A.T, dot(A, p_{s}))
#        dot_pq = dot(p_{s}, q_{s})
#        x_{s+1} = x_{s} - p_{s} / dot_pq
#
# После выполнения этого алгоритма, мы получим приближённое
# решение x_{N+1}, которое для случая квадратной матрицы A будет
# в действительности точным решением (с точностью до ошибок
# численного округления при выполнении арифметических операций).
#
# При параллелизации этого алгоритма мы будем считать, что общее
# число MPI процессов равно P.
# При этом процесс 0 будет занят исключительно синхронизацией
# данных между процессами, а реальная работа будет выполняться
# процессами от 1 до P-1 (будем называть их "рабочими" процессами).
#
# Размер матрицы A равен (M, N) - M строк и N колонок.
# Мы распределим эту матрицу между всеми "рабочими" процессами
# по строкам в форме массива A_part, как делалось в примере 1.1.
# Эта же матрица, но транспонированная (и при этом точно так же
# распределённая между процессами в форме тех же частей A_part),
# будет умножаться на векторы размером N, как делалось в примере 2.2.
# Скалярное произведение векторов будет выполняться также
# параллельно, как делалось в примере 2.1.
#
# Реализация самого метода сопряжённых градиентов вынесена в функцию
# `conjugate_gradient_method()` - остальная же часть кода занимается
# подготовкой и пересылкой данных между MPI процессами.
#
#------------------------------------------------------------------
# Этот пример (в его оригинальном виде) детально обсуждается
# в лекции Д.В. Лукьяненко "3. Параллельная реализация метода
# сопряжённых градиентов":
# https://youtu.be/rQz7xov3_rU?list=PLcsjsqLLSfNCxGJjuYNZRzeDIFQDQ9WvC
#
# Объяснения математической части самого метода сопряжённых
# градиентов даются в лекциях Д.В. Лукьяненко:
# Лекция 9. Методы минимизации:
# https://teach-in.ru/lecture/2021-06-10-Lukyanenko
# Лекция 26. Интегральные уравнения
# https://teach-in.ru/lecture/2021-12-03-Lukyanenko
#
#------------------------------------------------------------------
# ВАЖНО ЗНАТЬ:
# Вышеописанная форма метода сопряжённых градиентов была впервые
# предложена в 2011 году российскими математиками в статье:
# Н.Н. Калиткин, Л. . Кузьмина, “Улучшенная форма метода сопряженных
# градиентов”, Матем. моделирование, 23:7 (2011), 33–51
# https://www.mathnet.ru/mm3129
#------------------------------------------------------------------

import numpy as np
from mpi4py import MPI
from cpu_timer import CPU_TimerMap

mpi_timer = CPU_TimerMap()

#------------------------------------------------------------------
def auxiliary_arrays_determination(M, P):
    """
    Расчёт списков числа элементов `rcounts` и соответствующих
    смещений "displs", определяющих распределение больших матриц
    и векторов по процессам MPI коммуникатора.

    :param M: Общее число элементов вдоль нужной оси матрицы.
    :param P: Общее число процессов, работающих над параллелизацией
              вычислений. Предполагаем при этом, что поток 0 только
              "дирижирует" работу, а "рабочие" потоки от 1 до P-1
              её выполняют.
    :return: Рассчитанные списки числа элементов `rcounts` и
             соответствующих смещений "displs", определяющие
             передачу данных каждому процессу.
    """
    mpi_timer.start('auxiliary_arrays_determination')
    # Считая, что M = (P-1) * K + L, где K и L - это целые числа,
    # причём 0 <= L <= P-2, мы можем держать на каждом процессе
    # либо по K+1 либо по К строк, для максимальной балансировки памяти
    # и вычислений по всем "рабочим" процессам.
    # Найдём целые числа K и L из описания алгоритма выше:
    K, L = divmod(np.int32(M), P - 1)

    # Введём два новых списка для описания того, как именно
    # матрицы и векторы будут распределяться по всем процессам.
    # Здесь `rcounts` будет содержать число элементов, хранимое
    # каждым процессом (это 0 для процесса 0, K+1 для первых L
    # "рабочих" процессов, и K для оставшихся процессов).
    # Другой список `displs` будет содержать индекс смещений
    # - то есть, номер первой строки, начиная с которой будут
    # храниться `rcounts[m]` строк на процессе `m`.
    # При этом мы предполагаем, что все элементы, которые
    # хранятся на каждом процессе, идут подряд.
    rcounts = np.empty(P, dtype=np.int32)
    displs = np.empty(P, dtype=np.int32)

    # Процесс 0 не рабочий, и он будет содержать ноль элементов:
    rcounts[0] = displs[0] = 0

    # Цикл по всем "рабочим" процессам:
    for m in range(1, P):
        if m <= L:
            # Процессы от 1 до L содержат по K+1 строк матрицы
            # (если L=0, то таких процессов не будет!):
            rcounts[m] = K + 1
        else:
            # Оставшиеся процессы от L+1 до P-1 содержат по K строк:
            rcounts[m] = K
        # Индекс смещений сдвигается каждый раз на число
        # строк, хранимых в процессе:
        displs[m] = displs[m - 1] + rcounts[m - 1]
    mpi_timer.stop('auxiliary_arrays_determination')
    return rcounts, displs


#------------------------------------------------------------------
def conjugate_gradient_method(A_part, b_part, x_part,
                              N, N_part, rcounts_N, displs_N):
    """
    Параллельное решение системы линейных уравнений A*x = b
    методом сопряжённых градиентов.

    Реализация №1: Пытаемся распараллелить все вычисления.

    Считаем, что на каждом процессе хранится только часть `A_part`
    матрицы `A`, разбитая между процессами по строкам.
    Также хранится только часть `b_part` вектора `b` и часть
    `x_part` вектора `x`.

    Полный вектор решения `x` имеет размер `N`, а его часть `x_part`
    на текущем процессе имеет размер `N_part`. Для правильной сборки
    полного вектора `x` на каждой итерации, нам передаются также
    списки `rcounts_N` и `displs_N`.

    :param A_part: Часть матрицы `A`, хранимая на текущем процессе.
                   Предполагается, что матрица `A` была разбита
                   между процессами по строкам.
    :param b_part: Часть вектора `b`, хранимая на текущем процессе.
    :param x_part: Часть вектора `x`, хранимая на текущем процессе.
    :param N:      Размер полного вектора `x`.
    :param N_part: Размер части `x_part` вектора `x`, хранимой на
                   текущем процессе.
    :param rcounts_N: Список размеров всех частей `x_part` вектора `x`.
                      Элемент `rcounts_N[m]` определяет размер `x_part`
                      на процессе m.
    :param displs_N:  Список смещений всех частей `x_part` вектора `x`.
                      Элемент `displs_N[m]` определяет смещение первого
                      элемента вектора `x_part` внутри вектора `x`
                      на процессе m.

    :return: Часть `x_part` приближённого (или точного, для квадратной матрицы)
             решения. Окончательное решение в виде полного вектора `x` должно
             быть собрано за пределами этой функции.
    """
    mpi_timer.start('conjugate_gradient_method')

    # Для нахождения произведения матрицы `A` на вектор решения `x` и
    # на вспомогательный вектор `p`, нам понадобится знать `x` и `p`
    # полностью на каждом процессе - сделаем "хранилища" для них:
    x = np.empty(N, dtype=np.float64)
    p = np.empty(N, dtype=np.float64)

    # Также каждый процесс будет хранить свою часть вспомогательных
    # векторов `r` и `q` в массивах `r_part` и `q_part`:
    r_part = np.empty(N_part, dtype=np.float64)
    q_part = np.empty(N_part, dtype=np.float64)

    # Нам также будет удобно ввести массив `p_part` для хранения
    # на каждом процессе своей части вспомогательного вектора `p`
    # (полный вектор `p`, который будет полностью храниться на
    # КАЖДОМ процессе, будет всё же на каждой итерации собираться
    # из кусочков, рассчитанных на каждом процессе отдельно).
    # Сразу инициализируем его нулём (на итерации s=0):
    p_part = np.zeros(N_part, dtype=np.float64)

    # Объявим также переменные `dot_vec` (для хранения окончательного
    # значения скалярного произведения векторов) и `dot_vec_temp` (для
    # хранения частичных сумм скалярного произведения векторов,
    # собранных на каждом из процессов для пополнения значения `dot_vec`):
    dot_vec = np.array(0, dtype=np.float64)
    dot_vec_temp = np.empty(1, dtype=np.float64)

    # Цикл по итерациям s от 1 до N включительно:
    for s in range(1, N + 1):
        #------------------------------------------------------------
        # Шаг 1:
        # Обновим или создадим вектор `r`.
        if s == 1:
            # Сначала, на первой итерации s=1, нужно создать вектор `r`:
            #   r_{s} = A.T.dot(A.dot(x_{s}) - b)

            # Для этого нужно знать полный вектор `x` - сейчас же каждый
            # процесс хранит только свой кусочек `x_part` вектора `x`.
            # Командой `comm.Allgatherv` мы можем собрать все эти кусочки
            # вместе в полный вектор `x` (суффикс `gatherv`!) и одновременно
            # раздать его всем процессам (префикс `All`!):
            comm.Allgatherv([x_part, N_part, MPI.DOUBLE],
                            [x, rcounts_N, displs_N, MPI.DOUBLE])

            # Теперь на каждом процессе мы можем собрать частично
            # просуммированный полный вектор `r` с размером N, поместив
            # эту часть в локальный для каждого процесса вектор `r_temp`:
            r_temp = np.dot(A_part.T, np.dot(A_part, x) - b_part)

            # и затем командой `comm.Reduce_scatter` просуммировать
            # (префикс `Reduce`!) все `r_temp` на всех процессах, получив
            # внутри себя полный вектор `r`. Но никому не возвращая этот
            # вектор в полном виде, сразу же разбросать его частями
            # (суффикс `scatter`!) по всем процессам, отдав каждому
            # только нужную ему часть `r_part`:
            comm.Reduce_scatter([r_temp, N, MPI.DOUBLE],
                                [r_part, N_part, MPI.DOUBLE],
                                recvcounts=rcounts_N, op=MPI.SUM)
        else:
            # На всех последующих итерациях, вектор `r` нужно просто
            # обновить:
            #    dot_pq = dot(p_{s-1}, q_{s-1})
            #    r_{s} = r_{s-1} - q_{s-1} / dot_pq

            # Это можно сделать на каждом процессе для каждого кусочка
            # `r_part` отдельно - нет нужды собирать полный вектор `r`.

            # Первым делом нам нужно знать скалярное произведение
            # векторов `p_{s-1}` и `q_{s-1}`, вычисленных на предыдущей
            # итерации.
            # Важно, что именно это скалярное произведение мы уже вычислили
            # в самом конце предыдущей итерации и поместили его в переменную
            # `dot_vec`, где оно до сих пор ещё и хранится (одно и то же
            # значение на каждом процессе!).
            # Просто воспользуемся им для обновления `r_part`:
            r_part = r_part - q_part/dot_vec

        #------------------------------------------------------------
        # Шаг 2:
        # Посчитаем скалярное произведение векторов dot(r, r): 
        #    dot_rr = dot(r_{s}, r_{s})

        # Первым делом, на каждом процессе получим скалярное
        # произведение кусочков `r_part` с самими собой:
        dot_vec_temp[0] = np.dot(r_part, r_part)

        # и командой `comm.Allreduce` сначала просуммируем все
        # полученные произведения (суффикс `reduce`!), собрав
        # их в скалярное произведение полного вектора `r` с самим собой,
        # а затем раздадим полученное значение каждому процессу
        # (префикс `All`):
        comm.Allreduce([dot_vec_temp, 1, MPI.DOUBLE],
                       [dot_vec, 1, MPI.DOUBLE], op=MPI.SUM)

        #------------------------------------------------------------
        # Шаг 3:
        # Обновим вектор `p`: 
        #    p_{s} = p_{s-1} + r_{s} / dot_rr

        # Первым делом, обновим на каждом процессе свой кусочек
        # `p_part` вектора `p`:
        p_part = p_part + r_part/dot_vec

        # И теперь командой `comm.Allgatherv` мы можем собрать все кусочки
        # вместе в вектор `p` (суффикс `gatherv`!) и одновременно раздать его
        # всем процессам (префикс `All`!):
        comm.Allgatherv([p_part, N_part, MPI.DOUBLE],
                        [p, rcounts_N, displs_N, MPI.DOUBLE])

        #------------------------------------------------------------
        # Шаг 4:
        # Обновим вектор `q`: 
        #    q_{s} = A.T.dot(A.dot(p_{s}))

        # На каждом процессе мы можем собрать частично просуммированный
        # полный вектор `q` с размером N, поместив эту часть в локальный
        # для каждого процесса вектор `q_temp`:
        q_temp = np.dot(A_part.T, np.dot(A_part, p))

        # и затем командой `comm.Reduce_scatter` просуммировать
        # (префикс `Reduce`!) все `q_temp` на всех процессах, получив
        # внутри себя полный вектор `q` - но не возвращая этот вектор,
        # сразу же разбросать его (суффикс `scatter`!) по всем процессам,
        # отдав каждому только нужную ему часть `q_part`:
        comm.Reduce_scatter([q_temp, N, MPI.DOUBLE],
                            [q_part, N_part, MPI.DOUBLE],
                            recvcounts=rcounts_N, op=MPI.SUM)

        #------------------------------------------------------------
        # Шаг 5:
        # Посчитаем скалярное произведение векторов dot(p, q): 
        #    dot_pq = dot(p_{s}, q_{s})

        # Первым делом, на каждом процессе получим скалярное
        # произведение кусочков `p_part` и `q_part`:
        dot_vec_temp[0] = np.dot(p_part, q_part)

        # и командой `comm.Allreduce` сначала просуммируем все
        # полученные произведения (суффикс `reduce`!), собрав
        # их в скалярное произведение полных векторов `p` и `q`,
        # а затем раздадим полученное значение каждому процессу
        # (префикс `All`):
        comm.Allreduce([dot_vec_temp, 1, MPI.DOUBLE],
                       [dot_vec, 1, MPI.DOUBLE], op=MPI.SUM)

        #------------------------------------------------------------
        # Шаг 6:
        # Обновим вектор `x`:
        #    x_{s+1} = x_{s} - p_{s} / dot_pq

        # Сейчас мы, наконец, можем обновить `x_part` на каждом процессе:
        x_part = x_part - p_part / dot_vec

        # Объединять эти кусочки в полный вектор `x` нет нужды -
        # в полном виде этот вектор был нам нужен только на самой
        # первой итерации, и будет потом нужен только после завершения
        # работы этой функции.

    mpi_timer.stop('conjugate_gradient_method')
    return x_part


#------------------------------------------------------------------
# Начинаем выполнение программы - первым делом, настроим MPI:
#------------------------------------------------------------------
mpi_timer.start('ALL')

# Работаем с коммуникатором по всем доступным процессам:
comm = MPI.COMM_WORLD

# Число P доступных процессов в этом коммуникаторе:
P = comm.Get_size()

# Номер текущего процесса (от 0 до P-1):
rank = comm.Get_rank()

#------------------------------------------------------------------
# Шаг 0:
# Реализуемый алгоритм требует для работы хотя бы два MPI процесса -
# один будет командовать, а второй работать. Если процесс всего один,
# то работать просто некому!

if P == 1:
    raise ValueError(
        "\nAt least 2 MPI processes are needed to run this program!\n"
        "Please launch it with the command:\n"
        "mpiexec.exe -n * python.exe Example-03-1.py"
    )

#------------------------------------------------------------------
# Шаг 1 (точно такой же, как в примерах 1.1 и 2.2):
# Зачитаем из файла размер (M, N) матрицы A.
# Сделаем это только на процессе 0 - считаем,
# что входной файл доступен только на нём:
mpi_timer.start('Step 1: read N, M')

if rank == 0:
    with open('../examples/Example-03_in.dat', 'r') as f1:
        M = np.array(np.int32(f1.readline()))
        N = np.array(np.int32(f1.readline()))
else:
    # На "рабочих" процессах значение `M` не
    # используется - ставим пустую "заглушку":
    M = None
    # и подготавливаем на них "хранилище" для `N`:
    N = np.array(0, dtype=np.int32)

# Раздадим значение `N`, зачитанное процессом 0,
# всем остальным процессам (включая и сам процесс 0):
comm.Bcast(N, root=0)
# Альтернативно, более общая форма записи:
# comm.Bcast([N, 1, MPI.INT], root=0)

# NOTE: Значение `M` не используется на "рабочих" процессах,
#       так что мы его не будем раздавать - лишняя трата времени.
mpi_timer.stop('Step 1: read N, M')

#------------------------------------------------------------------
# Шаг 2:
# Разберёмся, сколько из имеющихся M строк (и какие именно!)
# матрицы `A` и вектора `b` мы будем хранить в форме частичной
# матрицы `A_part` и частичного вектора `b_part` на каждом
# процессе - посчитаем для этого списки числа строк
# `rcounts_M` и их смещений `displs_M` (они объяснялись в
# примерах 1.1-2.2).
# Точно так же разберёмся, сколько из имеющихся N элементов
# (и какие именно!) вектора `x` мы будем хранить в форме
# частичного вектора `x_part` на каждом процессе -
# посчитаем для этого списки `rcounts_N` и `displs_N`.
mpi_timer.start('Step 2: rcounts, displs')

if rank == 0:
    # Сделаем такой анализ только на процессе 0.
    # Всю логику расчёта этих списков мы перенесём в отдельную
    # функцию `auxiliary_arrays_determination()`, определённую
    # выше в этом файле:
    rcounts_M, displs_M = auxiliary_arrays_determination(M, P)
    rcounts_N, displs_N = auxiliary_arrays_determination(N, P)
else:
    # На "рабочих" процессах списки `rcounts_M` и `displs_M`
    # используются (в нескольких `comm.Scatterv` ниже) в качестве
    # пустых заглушек:
    rcounts_M = displs_M = None
    # Подготовим "хранилища" для других двух списков:
    rcounts_N = np.empty(P, dtype=np.int32)
    displs_N = np.empty(P, dtype=np.int32)

# Подготовим "хранилище" для `M_part` на всех процессах:
M_part = np.array(0, dtype=np.int32)
# NOTE: Отдельного значения типа N_part мы готовить не будем,
# поскольку каждый процесс получит полную копию `rcounts_N`.

# И разбросаем рассчитанные выше `rcounts_M` по всем процессам
# в виде одного значения `M_part = rcounts_M[m]` на каждом
# процессе `m`:
comm.Scatter([rcounts_M, 1, MPI.INT],
             [M_part, 1, MPI.INT], root=0)

# Передадим полные версии списков `rcounts_N` и `displs_N`
# всем процессам:
comm.Bcast([rcounts_N, P, MPI.INT], root=0)
comm.Bcast([displs_N, P, MPI.INT], root=0)

mpi_timer.stop('Step 2: rcounts, displs')

#------------------------------------------------------------------
# Шаг 3:
# Зачитаем из файла матрицу `A`. Как и в примере 2.2, мы будем
# экономить память - будем зачитывать матрицу по кусочкам,
# сразу отдавая каждый кусочек нужному процессу.
mpi_timer.start('Step 3: read A')

# Подготовим "хранилище" для кусочков `A_part` на всех процессах -
# каждое со своим числом строк `M_part`:
A_part = np.empty((M_part, N), dtype=np.float64)
# NOTE: Поскольку на процессе 0 значение M_part равно нулю, то
# матрица `A_part` здесь будет пустой и не займёт памяти.

if rank == 0:
    # Зачитаем на процессе 0 файл с матрицей `A` не сразу весь,
    # а по кусочкам - сразу отдавая каждый кусочек своему
    # "рабочему" процессу (на процессе 0 при этом данных
    # матрицы `A` совсем не останется - экономим память!):
    with open('../examples/Example-03_AData.dat', 'r') as f2:
        for m in range(1, P):
            # Кусочек матрицы `A`, который мы отдадим процессу `m`:
            A_part_m = np.empty((rcounts_M[m], N), dtype=np.float64)
            # Зачитали данные:
            for j in range(rcounts_M[m]):
                for i in range(N):
                    A_part_m[j,i] = np.float64(f2.readline())
            # И сразу же отдали их процессу, причём с блокировкой (!):
            comm.Send([A_part_m, rcounts_M[m]*N, MPI.DOUBLE], dest=m, tag=0)

            # Может показаться, что более эффективной будет такая же,
            # но неблокирующая пересылка:
            #comm.Isend([A_part_m, rcounts_M[m]*N, MPI.DOUBLE], dest=m, tag=0)
            # Но её использование здесь приведёт к ошибке! - массив `A_part_m`
            # будет сразу же пересоздан в цикле по `m`, и нужные данные
            # в большинстве случаев просто не успеют переслаться процессу `m`.
            # (Поирайте с этим!)

            # Поможем сборщику мусора поскорее избавиться от больших
            # ненужных данных:
            del A_part_m
else:
    # Каждый "рабочий" процесс получает свою часть строк и записывает
    # их в свой массив `A_part` - опять с блокировкой:
    comm.Recv([A_part, M_part*N, MPI.DOUBLE], source=0, tag=0, status=None)

mpi_timer.stop('Step 3: read A')

#------------------------------------------------------------------
# Шаг 4:
# Зачитаем из файла вектор `b`. Зачитаем его полностью на процессе 0
# (считаем, что входной файл доступен только на нём) - и потом
# раздадим его по кусочкам `b_part` всем "рабочим" процессам.
mpi_timer.start('Step 4: read b')

if rank == 0:
    # Зачитаем файл `b` на процессе 0:
    b = np.empty(M, dtype=np.float64)
    with open('../examples/Example-03_bData.dat', 'r') as f3:
        for j in range(M):
            b[j] = np.float64(f3.readline())
else:
    # На "рабочих" процессах вектор `b` используется
    # (в `comm.Scatterv` ниже) в качестве пустой заглушки:
    b = None

# Подготовим "хранилище" для `b_part` на всех процессах:
b_part = np.empty(M_part, dtype=np.float64)

# И разбросаем зачитанный выше вектор `b` по всем процессам m
# в виде кусочков `b_part` с размерами `M_part = rcounts_M[m]`,
# используя смещения `displs_M[m]`:
comm.Scatterv([b, rcounts_M, displs_M, MPI.DOUBLE], 
              [b_part, M_part, MPI.DOUBLE], root=0)

mpi_timer.stop('Step 4: read b')

#------------------------------------------------------------------
# Шаг 5:
# Подготовим "хранилища" для вектора `x`.
# Полная версия вектора `x` будет храниться только на процессе 0
# (в этой части программы - внутри же функции `conjugate_gradient_method()`
# нам такая полная версия вектора `x` понадобится на всех процессах -
# но пусть это будет личным делом самой функции, незачем усложнять здесь):
mpi_timer.start('Step 5: prepare x')

if rank == 0:
    # ВАЖНО: если вы знаете хорошее начальное приближение для `x`,
    # дайте его здесь!
    x = np.zeros(N, dtype=np.float64)
else:
    # На "рабочих" процессах вектор `x` используется
    # (в `comm.Scatterv` ниже) в качестве пустой заглушки:
    x = None

# На рабочих же процессах будут храниться только кусочки вектора `x`
# в виде векторов `x_part` (который будет создан и на процессе 0,
# но с нулевым размером).

# Подготовим "хранилище" для `x_part` на всех процессах:
x_part = np.empty(rcounts_N[rank], dtype=np.float64)

# И разбросаем вектор `x` по всем процессам в виде кусочков
# `x_part` (с размером `M_part`) этого вектора:
comm.Scatterv([x, rcounts_N, displs_N, MPI.DOUBLE],
              [x_part, rcounts_N[rank], MPI.DOUBLE], root=0)

mpi_timer.stop('Step 5: prepare x')
#------------------------------------------------------------------
# Шаг 6:
# Собственно, и сама нужная нам работа - решение системы линейных
# уравнений итерационным методом сопряжённых градиентов:

# На каждом процессе, алгоритм отработает со своими кусками массивов
# `A`, `b`, и `x` в виде `A_part`, `b_part`, и `x_part`, и возвратит
# свой кусок найденного решения `x_part` (детали описаны внутри самой
# функции):
mpi_timer.start('Step 6: solve A*x = b')

x_part = conjugate_gradient_method(A_part, b_part, x_part,
                                   N, rcounts_N[rank], rcounts_N, displs_N)

# Проверим, кто что посчитал:
print(f"Vector `x_part` on process {rank} consists of {len(x_part)} elements:")
print(f"  x_part = {x_part}")

# Соберём вектор `x` на процессе 0 (root=0) из кусочков `x_part`, присланными
# всеми процессами (включая и пустой кусочек от самого процесса 0):
comm.Gatherv([x_part, rcounts_N[rank], MPI.DOUBLE],
             [x, rcounts_N, displs_N, MPI.DOUBLE], root=0)

mpi_timer.stop('Step 6: solve A*x = b')

#------------------------------------------------------------------
# Шаг 7:
# Окончательно, нарисуем посчитанный вектор `x`, используя
# библиотеку matplotlib. Делаем это только на процессе 0.
mpi_timer.start('Step 7: plot x')

if rank == 0:
    # Для контроля, напечатаем найденное решение на консоли:
    print(f"\nFinal solution:\nx = {x}")

    # Подготовим рисунок и данные для него:
    import matplotlib.pyplot as plt
    plt.style.use('dark_background')
    fig = plt.figure()
    ax = plt.axes(xlim=(0, N), ylim=(-1.5, 1.5))
    ax.set_xlabel('i'); ax.set_ylabel('x[i]')
    # индексы элементов вектора `x`:
    ii = np.arange(np.int32(N))

    # Нарисуем полное решение:
    #ax.plot(ii, x, '-y', lw=3)

    # А лучше, нарисуем кусочки, которые были посчитаны
    # на каждом отдельном процессе (поиграйтесь с числом
    # процессов!):
    for m in range(1, P):
        r = rcounts_N[m]
        d = displs_N[m]
        ax.plot(ii[d:d+r], x[d:d+r], '-', lw=3,
                label=f"Process {m} (size {r})")
    plt.legend()
    #plt.show()

mpi_timer.stop('Step 7: plot x')
mpi_timer.stop('ALL')
mpi_timer.show()
#------------------------------------------------------------------
