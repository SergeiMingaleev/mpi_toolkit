# ------------------------------------------------------------------
# Решение системы линейных алгебраических уравнений (СЛАУ)
#     A*x = b
# методом сопряжённых градиентов.
#
# Реализация №1: Пытаемся распараллелить все вычисления,
#                используя ленточный метод умножения матрицы
#                на вектор.
# ------------------------------------------------------------------
# Эта реализация детально разбирается в примере
# mpi_toolkit/examples/Example-03-1.py
# ------------------------------------------------------------------

# import numpy as np
from mpi4py import MPI

from .cpu_timer import BenchmarkTimer
from .solver_base import SolverBase


# =============================================================================
class SolverParallelBand1(SolverBase):
    """
    Решалка, реализующая последовательное решение системы
    линейных уравнений `A*x = b` методом сопряжённых градиентов.

    :param numpy_lib: Ссылка на библиотеку `numpy`, которую
                      нужно использовать для вычислений
                      (это может быть как `numpy`, так и `cupy`).

    :return: Приближённое (или точное, для квадратной матрицы) решение
             системы уравнений в виде полного вектора `x`.
    """
    # -------------------------------------------------------------------------
    def __init__(self, numpy_lib=None):
        super().__init__(numpy_lib)

    # -------------------------------------------------------------------------
    def _cgm_mpi_part(self, A_part, b_part, x_part,
                      N, N_part, rcounts_N, displs_N):
        """
        Параллельное решение системы линейных уравнений A*x = b
        методом сопряжённых градиентов.

        Реализация №1: Пытаемся распараллелить все вычисления.

        Считаем, что на каждом процессе хранится только часть `A_part`
        матрицы `A`, разбитая между процессами по строкам.
        Также хранится только часть `b_part` вектора `b` и часть
        `x_part` вектора `x`.

        Полный вектор решения `x` имеет размер `N`, а его часть `x_part`
        на текущем процессе имеет размер `N_part`. Для правильной сборки
        полного вектора `x` на каждой итерации, нам передаются также
        списки `rcounts_N` и `displs_N`.

        :param A_part: Часть матрицы `A`, хранимая на текущем процессе.
                       Предполагается, что матрица `A` была разбита
                       между процессами по строкам.
        :param b_part: Часть вектора `b`, хранимая на текущем процессе.
        :param x_part: Часть вектора `x`, хранимая на текущем процессе.
        :param N:      Размер полного вектора `x`.
        :param N_part: Размер части `x_part` вектора `x`, хранимой на
                       текущем процессе.
        :param rcounts_N: Список размеров всех частей `x_part` вектора `x`.
                          Элемент `rcounts_N[m]` определяет размер `x_part`
                          на процессе m.
        :param displs_N:  Список смещений всех частей `x_part` вектора `x`.
                          Элемент `displs_N[m]` определяет смещение первого
                          элемента вектора `x_part` внутри вектора `x`
                          на процессе m.

        :return: Часть `x_part` приближённого (или точного, для квадратной матрицы)
                 решения. Окончательное решение в виде полного вектора `x` должно
                 быть собрано за пределами этой функции.
        """
        np = self._numpy_lib
        comm = self._comm

        self._timer.start('conjugate_gradient_method')
        self._timer.start('conj step 0: prepare data')
        # Для нахождения произведения матрицы `A` на вектор решения `x` и
        # на вспомогательный вектор `p`, нам понадобится знать `x` и `p`
        # полностью на каждом процессе - сделаем "хранилища" для них:
        x = np.empty(N, dtype=np.float64)
        p = np.empty(N, dtype=np.float64)

        # Также каждый процесс будет хранить свою часть вспомогательных
        # векторов `r` и `q` в массивах `r_part` и `q_part`:
        r_part = np.empty(N_part, dtype=np.float64)
        q_part = np.empty(N_part, dtype=np.float64)

        # Нам также будет удобно ввести массив `p_part` для хранения
        # на каждом процессе своей части вспомогательного вектора `p`
        # (полный вектор `p`, который будет полностью храниться на
        # КАЖДОМ процессе, будет всё же на каждой итерации собираться
        # из кусочков, рассчитанных на каждом процессе отдельно).
        # Сразу инициализируем его нулём (на итерации s=0):
        p_part = np.zeros(N_part, dtype=np.float64)

        # Объявим также переменные `dot_vec` (для хранения окончательного
        # значения скалярного произведения векторов) и `dot_vec_temp` (для
        # хранения частичных сумм скалярного произведения векторов,
        # собранных на каждом из процессов для пополнения значения `dot_vec`):
        dot_vec = np.array(0, dtype=np.float64)
        dot_vec_temp = np.empty(1, dtype=np.float64)
        self._timer.stop('conj step 0: prepare data')

        # Цикл по итерациям s от 1 до N включительно:
        for s in range(1, N + 1):
            #------------------------------------------------------------
            # Шаг 1:
            # Обновим или создадим вектор `r`.
            if s == 1:
                self._timer.start('conj step 1: update r at s=1')
                # Сначала, на первой итерации s=1, нужно создать вектор `r`:
                #   r_{s} = A.T.dot(A.dot(x_{s}) - b)

                # Для этого нужно знать полный вектор `x` - сейчас же каждый
                # процесс хранит только свой кусочек `x_part` вектора `x`.
                # Командой `comm.Allgatherv()` мы можем собрать все эти кусочки
                # вместе в полный вектор `x` (суффикс `gatherv`!) и одновременно
                # раздать его всем процессам (префикс `All`!):
                comm.Allgatherv([x_part, N_part, MPI.DOUBLE],
                                [x, rcounts_N, displs_N, MPI.DOUBLE])
                # ВАЖНО: последняя команда пересылает *полный* вектор
                #        с размером N *всем* процессам - такая операция
                #        плохо масштабируется! В примере 5.1 мы это улучшим.

                # Теперь на каждом процессе мы можем собрать частично
                # просуммированный полный вектор `r` с размером N, поместив
                # эту часть в локальный для каждого процесса вектор `r_temp`:
                r_temp = np.dot(A_part.T, np.dot(A_part, x) - b_part)
                if self._alpha != 0.0:
                    r_temp = r_temp + self._alpha * x

                # и затем командой `comm.Reduce_scatter()` просуммировать
                # (префикс `Reduce`!) все `r_temp` на всех процессах, получив
                # внутри себя полный вектор `r`. Но никому не возвращая этот
                # вектор в полном виде, сразу же разбросать его частями
                # (суффикс `scatter`!) по всем процессам, отдав каждому
                # только нужную ему часть `r_part`:
                comm.Reduce_scatter([r_temp, N, MPI.DOUBLE],
                                    [r_part, N_part, MPI.DOUBLE],
                                    recvcounts=rcounts_N, op=MPI.SUM)
                # ВАЖНО: последняя команда пересылает *полный* вектор
                #        с размером N *всем* процессам - такая операция
                #        плохо масштабируется! В примере 5.1 мы это улучшим.
                self._timer.stop('conj step 1: update r at s=1')
            else:
                # На всех последующих итерациях, вектор `r` нужно просто
                # обновить:
                #    dot_pq = dot(p_{s-1}, q_{s-1})
                #    r_{s} = r_{s-1} - q_{s-1} / dot_pq

                # Это можно сделать на каждом процессе для каждого кусочка
                # `r_part` отдельно - нет нужды собирать полный вектор `r`.

                # Первым делом нам нужно знать скалярное произведение
                # векторов `p_{s-1}` и `q_{s-1}`, вычисленных на предыдущей
                # итерации.
                # Важно, что именно это скалярное произведение мы уже вычислили
                # в самом конце предыдущей итерации и поместили его в переменную
                # `dot_vec`, где оно до сих пор ещё и хранится (одно и то же
                # значение на каждом процессе!).
                # Просто воспользуемся им для обновления `r_part`:
                self._timer.start('conj step 1: update r at s>1')
                r_part = r_part - q_part/dot_vec
                self._timer.stop('conj step 1: update r at s>1')

            #------------------------------------------------------------
            # Шаг 2:
            # Посчитаем скалярное произведение векторов dot(r, r):
            #    dot_rr = dot(r_{s}, r_{s})
            self._timer.start('conj step 2: dot(r,r)')

            # Первым делом, на каждом процессе получим скалярное
            # произведение кусочков `r_part` с самими собой:
            dot_vec_temp[0] = np.dot(r_part, r_part)

            # и командой `comm.Allreduce` сначала просуммируем все
            # полученные произведения (суффикс `reduce`!), собрав
            # их в скалярное произведение полного вектора `r` с самим собой,
            # а затем раздадим полученное значение каждому процессу
            # (префикс `All`):
            comm.Allreduce([dot_vec_temp, 1, MPI.DOUBLE],
                           [dot_vec, 1, MPI.DOUBLE], op=MPI.SUM)
            self._timer.stop('conj step 2: dot(r,r)')

            #------------------------------------------------------------
            # Шаг 3:
            # Обновим вектор `p`:
            #    p_{s} = p_{s-1} + r_{s} / dot_rr
            self._timer.start('conj step 3: update p')

            # Первым делом, обновим на каждом процессе свой кусочек
            # `p_part` вектора `p`:
            p_part = p_part + r_part/dot_vec

            # И теперь командой `comm.Allgatherv` мы можем собрать все кусочки
            # вместе в вектор `p` (суффикс `gatherv`!) и одновременно раздать его
            # всем процессам (префикс `All`!):
            comm.Allgatherv([p_part, N_part, MPI.DOUBLE],
                            [p, rcounts_N, displs_N, MPI.DOUBLE])
            # ВАЖНО: последняя команда пересылает *полный* вектор
            #        с размером N *всем* процессам - такая операция
            #        плохо масштабируется! В примере 5.1 мы это улучшим.
            self._timer.stop('conj step 3: update p')

            #------------------------------------------------------------
            # Шаг 4:
            # Обновим вектор `q`:
            #    q_{s} = A.T.dot(A.dot(p_{s}))
            self._timer.start('conj step 4: update q')

            # На каждом процессе мы можем собрать частично просуммированный
            # полный вектор `q` с размером N, поместив эту часть в локальный
            # для каждого процесса вектор `q_temp`:
            q_temp = np.dot(A_part.T, np.dot(A_part, p))
            if self._alpha != 0.0:
                q_temp = q_temp + self._alpha * p

            # и затем командой `comm.Reduce_scatter()` просуммировать
            # (префикс `Reduce`!) все `q_temp` на всех процессах, получив
            # внутри себя полный вектор `q` - но не возвращая этот вектор,
            # сразу же разбросать его (суффикс `scatter`!) по всем процессам,
            # отдав каждому только нужную ему часть `q_part`:
            comm.Reduce_scatter([q_temp, N, MPI.DOUBLE],
                                [q_part, N_part, MPI.DOUBLE],
                                recvcounts=rcounts_N, op=MPI.SUM)
            # ВАЖНО: последняя команда пересылает *полный* вектор
            #        с размером N *всем* процессам - такая операция
            #        плохо масштабируется! В примере 5.1 мы это улучшим.
            self._timer.stop('conj step 4: update q')

            #------------------------------------------------------------
            # Шаг 5:
            # Посчитаем скалярное произведение векторов dot(p, q):
            #    dot_pq = dot(p_{s}, q_{s})
            self._timer.start('conj step 5: dot(p,q)')

            # Первым делом, на каждом процессе получим скалярное
            # произведение кусочков `p_part` и `q_part`:
            dot_vec_temp[0] = np.dot(p_part, q_part)

            # и командой `comm.Allreduce` сначала просуммируем все
            # полученные произведения (суффикс `reduce`!), собрав
            # их в скалярное произведение полных векторов `p` и `q`,
            # а затем раздадим полученное значение каждому процессу
            # (префикс `All`):
            comm.Allreduce([dot_vec_temp, 1, MPI.DOUBLE],
                           [dot_vec, 1, MPI.DOUBLE], op=MPI.SUM)
            self._timer.stop('conj step 5: dot(p,q)')

            #------------------------------------------------------------
            # Шаг 6:
            # Обновим вектор `x`:
            #    x_{s+1} = x_{s} - p_{s} / dot_pq
            self._timer.start('conj step 6: update x')

            # Сейчас мы, наконец, можем обновить `x_part` на каждом процессе:
            x_part = x_part - p_part / dot_vec

            # Объединять эти кусочки в полный вектор `x` нет нужды -
            # в полном виде этот вектор был нам нужен только на самой
            # первой итерации, и будет потом нужен только после завершения
            # работы этой функции.
            self._timer.stop('conj step 6: update x')

        self._timer.stop('conjugate_gradient_method')
        return x_part

    # -------------------------------------------------------------------------
    def _conjugate_gradient_method(self, A, b, x):
        np = self._numpy_lib
        # Использование в этом алгоритме ленточного перемножения
        # матрицы на вектор не позволяет использовать симметрию
        # матрицы - её приходится домножать на A_part.T, чтобы
        # получить снова вектор с размером `x`, а не `x_part`.
        self._is_symmetric = False

        self._timer = BenchmarkTimer()
        self._timer.is_active = self._verbose
        self._timer.start('ALL')

        comm = self._comm
        rank = self._rank
        P = self._P

        #------------------------------------------------------------------
        # Шаг 0:
        # Реализуемый алгоритм требует для работы хотя бы два MPI процесса -
        # один будет командовать, а второй работать. Если процесс всего один,
        # то работать просто некому!

        if P == 1:
            raise ValueError(
                "\nAt least 2 MPI processes are needed to run this solver!"
            )

        #------------------------------------------------------------------
        # Шаг 1: Размер (M, N) матрицы A.
        self._timer.start('Step 1: set N, M')

        if rank == 0:
            M = np.array(A.shape[0], dtype=np.int32)
            N = np.array(A.shape[1], dtype=np.int32)
        else:
            # На "рабочих" процессах значение `M` не
            # используется - ставим пустую "заглушку":
            M = None
            # и подготавливаем на них "хранилище" для `N`:
            N = np.array(0, dtype=np.int32)

        # Раздадим значение `N`, зачитанное процессом 0,
        # всем остальным процессам:
        comm.Bcast([N, 1, MPI.INT], root=0)

        # NOTE: Значение `M` не используется на "рабочих" процессах,
        #       так что мы его не будем раздавать - лишняя трата времени.
        self._timer.stop('Step 1: set N, M')

        #------------------------------------------------------------------
        # Шаг 2:
        # Разберёмся, сколько из имеющихся M строк (и какие именно!)
        # матрицы `A` и вектора `b` мы будем хранить в форме частичной
        # матрицы `A_part` и частичного вектора `b_part` на каждом
        # процессе - посчитаем для этого списки числа строк
        # `rcounts_M` и их смещений `displs_M` (они объяснялись в
        # примерах 1.1-2.2).
        # Точно так же разберёмся, сколько из имеющихся N элементов
        # (и какие именно!) вектора `x` мы будем хранить в форме
        # частичного вектора `x_part` на каждом процессе -
        # посчитаем для этого списки `rcounts_N` и `displs_N`.
        self._timer.start('Step 2: rcounts, displs')

        if rank == 0:
            # Сделаем такой анализ только на процессе 0.
            # Всю логику расчёта этих списков мы перенесём в отдельную
            # функцию `auxiliary_arrays_determination()`, определённую
            # выше в этом файле:
            self._timer.start('auxiliary_arrays_determination')
            rcounts_M, displs_M = self._auxiliary_arrays_determination(M, P)
            rcounts_N, displs_N = self._auxiliary_arrays_determination(N, P)
            self._timer.stop('auxiliary_arrays_determination')
        else:
            # На "рабочих" процессах списки `rcounts_M` и `displs_M`
            # используются (в нескольких `comm.Scatterv` ниже) в качестве
            # пустых заглушек:
            rcounts_M = displs_M = None
            # Подготовим "хранилища" для других двух списков:
            rcounts_N = np.empty(P, dtype=np.int32)
            displs_N = np.empty(P, dtype=np.int32)

        # Подготовим "хранилище" для `M_part` на всех процессах:
        M_part = np.array(0, dtype=np.int32)
        # NOTE: Отдельного значения типа N_part мы готовить не будем,
        # поскольку каждый процесс получит полную копию `rcounts_N`.

        # И разбросаем рассчитанные выше `rcounts_M` по всем процессам
        # в виде одного значения `M_part = rcounts_M[m]` на каждом
        # процессе `m`:
        comm.Scatter([rcounts_M, 1, MPI.INT],
                     [M_part, 1, MPI.INT], root=0)

        # Передадим полные версии списков `rcounts_N` и `displs_N`
        # всем процессам:
        comm.Bcast([rcounts_N, P, MPI.INT], root=0)
        comm.Bcast([displs_N, P, MPI.INT], root=0)

        self._timer.stop('Step 2: rcounts, displs')

        #------------------------------------------------------------------
        # Шаг 3:
        # Зачитаем из файла матрицу `A`. Как и в примере 2.2, мы будем
        # экономить память - будем зачитывать матрицу по кусочкам,
        # сразу отдавая каждый кусочек нужному процессу.

        # Подготовим "хранилище" для кусочков `A_part` на всех процессах -
        # каждое со своим числом строк `M_part`:
        A_part = np.empty((M_part, N), dtype=np.float64)
        # NOTE: Поскольку на процессе 0 значение M_part равно нулю, то
        # матрица `A_part` здесь будет пустой и не займёт памяти.

        if rank == 0:
            # Мы получили полную матрицу `A` на процессе 0 -
            # раздадим его по кусочкам всем "рабочим" процессам
            self._timer.start('Step 3: pass A')
            for m in range(1, P):
                # Кусочек матрицы `A`, который мы отдадим процессу `m`:
                A_part_m = A[displs_M[m]:displs_M[m]+rcounts_M[m], :]
                # Сразу же отдали его процессу, причём с блокировкой (!):
                comm.Send([A_part_m, rcounts_M[m]*N, MPI.DOUBLE], dest=m, tag=0)
                del A_part_m
                self._timer.stop('Step 3: pass A')
        else:
            # Каждый "рабочий" процесс получает свою часть строк и записывает
            # их в свой массив `A_part` - опять с блокировкой:
            self._timer.start('Step 3: pass A')
            comm.Recv([A_part, M_part*N, MPI.DOUBLE], source=0, tag=0, status=None)
            self._timer.stop('Step 3: pass A')

        #------------------------------------------------------------------
        # Шаг 4:
        # Мы получили полный вектор `b` на процессе 0 - на остальных
        # процессах `b` is None.

        self._timer.start('Step 4: pass b')
        # Подготовим "хранилище" для `b_part` на всех процессах:
        b_part = np.empty(M_part, dtype=np.float64)

        # И разбросаем зачитанный выше вектор `b` по всем процессам m
        # в виде кусочков `b_part` с размерами `M_part = rcounts_M[m]`,
        # используя смещения `displs_M[m]`:
        comm.Scatterv([b, rcounts_M, displs_M, MPI.DOUBLE],
                      [b_part, M_part, MPI.DOUBLE], root=0)

        self._timer.stop('Step 4: pass b')

        #------------------------------------------------------------------
        # Шаг 5:
        # Мы получили полный вектор `x` на процессе 0 - на остальных
        # процессах `x` is None.

        self._timer.start('Step 5: pass x')
        # На рабочих же процессах будут храниться только кусочки вектора `x`
        # в виде векторов `x_part` (который будет создан и на процессе 0,
        # но с нулевым размером).

        # Подготовим "хранилище" для `x_part` на всех процессах:
        x_part = np.empty(rcounts_N[rank], dtype=np.float64)

        # И разбросаем вектор `x` по всем процессам в виде кусочков
        # `x_part` (с размером `M_part`) этого вектора:
        comm.Scatterv([x, rcounts_N, displs_N, MPI.DOUBLE],
                      [x_part, rcounts_N[rank], MPI.DOUBLE], root=0)

        self._timer.stop('Step 5: pass x')
        #------------------------------------------------------------------
        # Шаг 6:
        # Собственно, и сама нужная нам работа - решение системы линейных
        # уравнений итерационным методом сопряжённых градиентов:

        # На каждом процессе, алгоритм отработает со своими кусками массивов
        # `A`, `b`, и `x` в виде `A_part`, `b_part`, и `x_part`, и возвратит
        # свой кусок найденного решения `x_part` (детали описаны внутри самой
        # функции):
        self._timer.start('Step 6: solve A*x = b')

        if self._skip_init_time:
            self._timer_calc.start()

        x_part = self._cgm_mpi_part(A_part, b_part, x_part,
                                    N, rcounts_N[rank], rcounts_N, displs_N)

        # Соберём вектор `x` на процессе 0 (root=0) из кусочков `x_part`, присланными
        # всеми процессами (включая и пустой кусочек от самого процесса 0):
        comm.Gatherv([x_part, rcounts_N[rank], MPI.DOUBLE],
                     [x, rcounts_N, displs_N, MPI.DOUBLE], root=0)

        if self._skip_init_time:
            self._timer_calc.stop()

        self._timer.stop('Step 6: solve A*x = b')
        self._timer.stop('ALL')

        if self._verbose:
            self._timer.print_status()

        # Проверим, кто что посчитал:
        # print(f"Vector `x_part` on process {rank} consists of {len(x_part)} elements:")
        # print(f"  x_part = {x_part}")
        return x

# =============================================================================
