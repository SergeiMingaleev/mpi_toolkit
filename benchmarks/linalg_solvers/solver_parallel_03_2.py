# ------------------------------------------------------------------
# Решение системы линейных алгебраических уравнений (СЛАУ)
#     A*x = b
# методом сопряжённых градиентов.
#
# Реализация №2: Пытаемся распараллелить только произведения матрицы
#                на вектор, используя ленточный метод.
#                Скалярные же произведения двух векторов
#                будут выполнять на каждом процессе независимо
#                (полностью повторяя одни и те же вычисления! -
#                но экономя время на пересылке частичных векторов
#                между процессами).
# ------------------------------------------------------------------
# Эта реализация детально разбирается в примере
# mpi_toolkit/examples/Example-03-2.py
# ------------------------------------------------------------------

from mpi4py import MPI

from .cpu_timer import BenchmarkTimer
from .solver_base import SolverBase


# =============================================================================
class SolverParallelBand2(SolverBase):
    """
    Решалка, реализующая параллельное решение системы
    линейных уравнений `A*x = b` методом сопряжённых градиентов.

    :param numpy_lib: Ссылка на библиотеку `numpy`, которую
                      нужно использовать для вычислений
                      (это может быть как `numpy`, так и `cupy`).

    :return: Приближённое (или точное, для квадратной матрицы) решение
             системы уравнений в виде полного вектора `x`.
    """
    # -------------------------------------------------------------------------
    def __init__(self, numpy_lib=None):
        super().__init__(numpy_lib)

    # -------------------------------------------------------------------------
    def _cgm_mpi_part(self, A_part, b_part, x, N):
        """
        Параллельное решение системы линейных уравнений A*x = b
        методом сопряжённых градиентов.

        Реализация №2: Пытаемся распараллелить только произведения матрицы
                       на вектор, используя ленточный метод.
                       Скалярные же произведения двух векторов
                       будут выполнять на каждом процессе независимо
                       (полностью повторяя одни и те же вычисления! -
                       но экономя время на пересылке частичных векторов
                       между процессами).

        Считаем, что на каждом процессе хранится только часть `A_part`
        матрицы `A`, разбитая между процессами по строкам.
        Также хранится только часть `b_part` вектора `b`.

        Полный вектор решения `x` имеет размер `N`, и его копия хранится
        на каждом процессе.

        :param A_part: Часть матрицы `A`, хранимая на текущем процессе.
                       Предполагается, что матрица `A` была разбита
                       между процессами по строкам.
        :param b_part: Часть вектора `b`, хранимая на текущем процессе.
        :param x:      Полный вектор `x` (его копия хранится на каждом процессе).
        :param N:      Размер полного вектора `x`.

        :return: Приближённое (или точное, для квадратной матрицы) решение
                 системы уравнений в виде полного вектора `x`.
        """
        np = self._numpy_lib
        comm = self._comm

        # Сделаем "хранилища" для полных векторов `p`, `r`, и `q`
        # на всех процессах. Вектор `p` сразу инициализируем
        # нулём (на итерации s=0):
        p = np.zeros(N, dtype=np.float64)
        r = np.empty(N, dtype=np.float64)
        q = np.empty(N, dtype=np.float64)


        # Чтобы сэкономить одно скалярное произведение векторов `p` и `q`
        # На каждой итерации, введём здесь временную переменную для него:
        dot_pq = 0.0

        # Цикл по итерациям s от 1 до N включительно:
        for s in range(1, N + 1):
            #------------------------------------------------------------
            # Шаг 1:
            # Обновим или создадим вектор `r`.
            if s == 1:
                # Сначала, на первой итерации s=1, нужно создать вектор `r`:
                #   r_{s} = A.T.dot(A.dot(x_{s}) - b)

                # На каждом процессе мы можем собрать частично
                # просуммированный полный вектор `r` с размером N, поместив
                # эту часть в локальный для каждого процесса вектор `r_temp`:
                r_temp = np.dot(A_part.T, np.dot(A_part, x) - b_part)
                if self._alpha != 0.0:
                    r_temp = r_temp + self._alpha * x

                # И затем командой `comm.Allreduce` просуммировать
                # (суффикс `reduce`!) все `r_temp` на всех процессах, получив
                # полный вектор `r` и отдав его полную копию всем процессам
                # (префикс `All`!):
                comm.Allreduce([r_temp, N, MPI.DOUBLE],
                               [r, N, MPI.DOUBLE], op=MPI.SUM)
                # ВАЖНО: последняя команда пересылает *полный* вектор
                #        с размером N *всем* процессам - такая операция
                #        плохо масштабируется! В примере 5.1 мы это улучшим.
            else:
                # На всех последующих итерациях, вектор `r` нужно просто
                # обновить:
                #    dot_pq = dot(p_{s-1}, q_{s-1})
                #    r_{s} = r_{s-1} - q_{s-1} / dot_pq

                # Важно, что скалярное произведение `dot(p_{s-1}, q_{s-1})`
                # мы уже вычислили в самом конце предыдущей итерации
                # и поместили его в переменную `dot_pq`, где оно до сих
                # пор ещё и хранится. Просто воспользуемся им для обновления `r`:
                r = r - q / dot_pq

            #------------------------------------------------------------
            # Шаг 2:
            # Обновим вектор `p`:
            #    dot_rr = dot(r_{s}, r_{s})
            #    p_{s} = p_{s-1} + r_{s} / dot_rr
            p = p + r / np.dot(r, r)

            #------------------------------------------------------------
            # Шаг 3:
            # Обновим вектор `q`:
            #    q_{s} = A.T.dot(A.dot(p_{s}))

            # На каждом процессе мы можем собрать частично просуммированный
            # полный вектор `q` с размером N, поместив эту часть в локальный
            # для каждого процесса вектор `q_temp`:
            q_temp = np.dot(A_part.T, np.dot(A_part, p))
            if self._alpha != 0.0:
                q_temp = q_temp + self._alpha * p

            # И затем командой `comm.Allreduce` просуммировать
            # (суффикс `reduce`!) все `q_temp` на всех процессах, получив
            # полный вектор `q` и отдав его полную копию всем процессам
            # (префикс `All`!):
            comm.Allreduce([q_temp, N, MPI.DOUBLE],
                           [q, N, MPI.DOUBLE], op=MPI.SUM)
            # ВАЖНО: последняя команда пересылает *полный* вектор
            #        с размером N *всем* процессам - такая операция
            #        плохо масштабируется! В примере 5.1 мы это улучшим.

            #------------------------------------------------------------
            # Шаг 4:
            # Посчитаем скалярное произведение векторов dot(p, q):
            #    dot_pq = dot(p_{s}, q_{s})

            # Запишем это скалярное произведение в переменную `dot_pq` -
            # оно будет переиспользовано в начале цикла для обновления
            # вектора `r`:
            dot_pq = np.dot(p, q)

            #------------------------------------------------------------
            # Шаг 5:
            # Обновим вектор `x`:
            #    x_{s+1} = x_{s} - p_{s} / dot_pq
            x = x - p / dot_pq

        return x

    # -------------------------------------------------------------------------
    def _conjugate_gradient_method(self, A, b, x):
        np = self._numpy_lib
        # Использование в этом алгоритме ленточного перемножения
        # матрицы на вектор не позволяет использовать симметрию
        # матрицы - её приходится домножать на A_part.T, чтобы
        # получить снова вектор с размером `x`, а не `x_part`.
        self._is_symmetric = False

        self._timer = BenchmarkTimer()
        self._timer.is_active = self._verbose
        self._timer.start('ALL')

        comm = self._comm
        rank = self._rank
        P = self._P

        #------------------------------------------------------------------
        # Шаг 0:
        # Реализуемый алгоритм требует для работы хотя бы два MPI процесса -
        # один будет командовать, а второй работать. Если процесс всего один,
        # то работать просто некому!

        if P == 1:
            raise ValueError(
                "\nAt least 2 MPI processes are needed to run this solver!"
            )

        #------------------------------------------------------------------
        # Шаг 1: Размер (M, N) матрицы A.
        if rank == 0:
            M = np.array(A.shape[0], dtype=np.int32)
            N = np.array(A.shape[1], dtype=np.int32)
        else:
            # На "рабочих" процессах значение `M` не
            # используется - ставим пустую "заглушку":
            M = None
            # и подготавливаем на них "хранилище" для `N`:
            N = np.array(0, dtype=np.int32)

        # Раздадим значение `N`, зачитанное процессом 0,
        # всем остальным процессам (включая и сам процесс 0):
        comm.Bcast([N, 1, MPI.INT], root=0)

        # NOTE: Значение `M` не используется на "рабочих" процессах,
        #       так что мы его не будем раздавать - лишняя трата времени.

        #------------------------------------------------------------------
        # Шаг 2:
        # Разберёмся, сколько из имеющихся M строк (и какие именно!)
        # матрицы `A` и вектора `b` мы будем хранить в форме частичной
        # матрицы `A_part` и частичного вектора `b_part` на каждом
        # процессе - посчитаем для этого списки числа строк
        # `rcounts_M` и их смещений `displs_M` (они объяснялись в
        # примерах 1.1-2.2).

        if rank == 0:
            # Сделаем такой анализ только на процессе 0.
            # Всю логику расчёта этих списков мы перенесём в отдельную
            # функцию `auxiliary_arrays_determination()`, определённую
            # выше в этом файле:
            rcounts_M, displs_M = self._auxiliary_arrays_determination(M, P)
        else:
            # На "рабочих" процессах списки `rcounts_M` и `displs_M`
            # используются (в нескольких `comm.Scatterv` ниже) в качестве
            # пустых заглушек:
            rcounts_M = displs_M = None

        # Подготовим "хранилище" для `M_part` на всех процессах:
        M_part = np.array(0, dtype=np.int32)

        # И разбросаем рассчитанные выше `rcounts_M` по всем процессам
        # в виде одного значения `M_part = rcounts_M[m]` на каждом
        # процессе `m`:
        comm.Scatter([rcounts_M, 1, MPI.INT],
                     [M_part, 1, MPI.INT], root=0)

        #------------------------------------------------------------------
        # Шаг 3:
        # Зачитаем из файла матрицу `A`. Как и в примере 2.2, мы будем
        # экономить память - будем зачитывать матрицу по кусочкам,
        # сразу отдавая каждый кусочек нужному процессу.

        # Подготовим "хранилище" для кусочков `A_part` на всех процессах -
        # каждое со своим числом строк `M_part`:
        A_part = np.empty((M_part, N), dtype=np.float64)
        # NOTE: Поскольку на процессе 0 значение M_part равно нулю, то
        # матрица `A_part` здесь будет пустой и не займёт памяти.

        if rank == 0:
            # Мы получили полную матрицу `A` на процессе 0 -
            # раздадим его по кусочкам всем "рабочим" процессам
            for m in range(1, P):
                # Кусочек матрицы `A`, который мы отдадим процессу `m`:
                A_part_m = A[displs_M[m]:displs_M[m]+rcounts_M[m], :]
                # Сразу же отдали его процессу, причём с блокировкой (!):
                comm.Send([A_part_m, rcounts_M[m]*N, MPI.DOUBLE], dest=m, tag=0)
                del A_part_m
        else:
            # Каждый "рабочий" процесс получает свою часть строк и записывает
            # их в свой массив `A_part` - опять с блокировкой:
            comm.Recv([A_part, M_part*N, MPI.DOUBLE], source=0, tag=0, status=None)

        #------------------------------------------------------------------
        # Шаг 4:
        # Мы получили полный вектор `b` на процессе 0 - на остальных
        # процессах `b` is None.

        # Подготовим "хранилище" для `b_part` на всех процессах:
        b_part = np.empty(M_part, dtype=np.float64)

        # И разбросаем зачитанный выше вектор `b` по всем процессам m
        # в виде кусочков `b_part` с размерами `M_part = rcounts_M[m]`,
        # используя смещения `displs_M[m]`:
        comm.Scatterv([b, rcounts_M, displs_M, MPI.DOUBLE],
                      [b_part, M_part, MPI.DOUBLE], root=0)

        #------------------------------------------------------------------
        # Шаг 5:
        # Мы получили полный вектор `x` на процессе 0 - на остальных
        # процессах `x` is None.

        # TODO: передать вектор с процесса 0 на другие!
        # ВАЖНО: если вы знаете хорошее начальное приближение для `x`,
        # дайте его здесь!
        x = np.zeros(N, dtype=np.float64)

        #------------------------------------------------------------------
        # Шаг 6:
        # Собственно, и сама нужная нам работа - решение системы линейных
        # уравнений итерационным методом сопряжённых градиентов:

        # На каждом процессе, алгоритм отработает со своими кусками массивов
        # `A` и `b` в виде `A_part` и `b_part`, но возвратит полное решение `x`
        # (детали описаны внутри самой функции):
        if self._skip_init_time:
            self._timer_calc.start()

        x = self._cgm_mpi_part(A_part, b_part, x, N)

        # Проверим, кто что посчитал:
        #print(f"x on process {rank} = {x}")
        return x

# =============================================================================
