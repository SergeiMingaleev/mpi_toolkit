# ------------------------------------------------------------------
# Решение системы линейных алгебраических уравнений (СЛАУ)
#     A*x = b
# методом сопряжённых градиентов.
#
# Реализация №4: Пытаемся распараллелить все вычисления, используя
#                более эффективный с точки зрения межпроцессорных
#                коммуникаций блочный алгоритм умножения матрицы
#                на вектор и нахождения скалярного произведения
#                векторов. При этом скалярные произведения векторов
#                будут, с одной стороны, распараллелены (а именно,
#                будут выполняться параллельно на всех процессах
#                одной строки нашей сетки процессов), но с другой
#                стороны, одни и те же вычисления будут повторяться
#                на каждой такой строке процессов (в отличие от
#                примера 5.1, где работала только первая строка
#                процессов - а остальные строки просто ждали
#                получения результатов её работы).
#
#                С точки зрения эффективности распараллеливания,
#                данный алгоритм должен быть более-менее эквивалентен
#                алгоритму из примера 5.1 - но при этом он подготовит
#                нас к предстоящим более существенным изменениям
#                алгоритма работы в примере 6.2.
# ------------------------------------------------------------------
# Эта реализация детально разбирается в примере
# mpi_toolkit/examples/Example-06-1.py
# ------------------------------------------------------------------

from mpi4py import MPI

from .cpu_timer import BenchmarkTimer
from .solver_base import SolverBase


# =============================================================================
class SolverParallelBlock2(SolverBase):
    """
    Решалка, реализующая параллельное решение системы
    линейных уравнений `A*x = b` методом сопряжённых градиентов.

    :param numpy_lib: Ссылка на библиотеку `numpy`, которую
                      нужно использовать для вычислений
                      (это может быть как `numpy`, так и `cupy`).

    :return: Приближённое (или точное, для квадратной матрицы) решение
             системы уравнений в виде полного вектора `x`.
    """
    # -------------------------------------------------------------------------
    def __init__(self, numpy_lib=None):
        super().__init__(numpy_lib)

    # -------------------------------------------------------------------------
    def _cgm_mpi_part(self, A_part, b_part, x_part,
                      N_part, M_part, N,
                      comm_row, comm_col):
        """
        Параллельное решение системы линейных уравнений A*x = b
        методом сопряжённых градиентов.

        Реализация №4: Пытаемся распараллелить все вычисления, используя
                   более эффективный с точки зрения межпроцессорных
                   коммуникаций блочный алгоритм умножения матрицы
                   на вектор и нахождения скалярного произведения
                   векторов. При этом скалярные произведения векторов
                   будут, с одной стороны, распараллелены (а именно,
                   будут выполняться параллельно на всех процессах
                   одной строки нашей сетки процессов), но с другой
                   стороны, одни и те же вычисления будут повторяться
                   на каждой такой строке процессов (в отличие от
                   примера 5.1, где работала только первая строка
                   процессов - а остальные строки просто ждали
                   получения результатов её работы).

        Считаем, что на каждом процессе хранится только часть `A_part`
        матрицы `A`, разбитая между процессами по блокам.
        Также на каждом процессе хранится свой кусочек `b_part` вектора `b`
        (в точности повторяясь в каждом столбце сетки процессов) и свой
        кусочек `x_part` вектора `x` (в точности повторяясь в каждой
        строке сетки процессов).

        :param A_part: Часть матрицы `A`, хранимая на текущем процессе.
                       Предполагается, что матрица `A` была разбита
                       между процессами по блокам.
        :param b_part: Часть вектора `b`, хранимая на текущем процессе.
        :param x_part: Часть вектора `x`, хранимая на текущем процессе.
        :param N_part: Размер части `x_part` вектора `x` и ему подобных,
                       используемых на текущем процессе.
        :param M_part: Размер части `b_part` вектора `b` и ему подобных,
                       используемых на текущем процессе.
        :param N:      Требуемое число итераций.
        :param comm_row:  Коммуникатор для связи данного процесса с другими
                          процессами из своей строки сетки процессов.
        :param comm_col:  Коммуникатор для связи данного процесса с другими
                          процессами из своего столбца сетки процессов.

        :return: Часть `x_part` приближённого (или точного, для квадратной матрицы)
                 решения. Окончательное решение в виде полного вектора `x` должно
                 быть собрано за пределами этой функции.
        """
        np = self._numpy_lib

        # На момент вызова этой функции на каждом процессе всей сетки процессов
        # уже есть свой кусочек `A_part` матрицы `A`. Также на каждом процессе
        # хранится свой кусочек `b_part` вектора `b` (в точности повторяясь
        # в каждом столбце сетки процессов) и свой кусочек `x_part` вектора `x`
        # (в точности повторяясь в каждой строке сетки процессов).

        # Каждый процесс будет хранить свою часть вспомогательных векторов
        # `r` и `q` в массивах `r_part` и `q_part`:
        r_part = np.empty(N_part, dtype=np.float64)
        q_part = np.empty(N_part, dtype=np.float64)

        # Также каждый процесс будет хранить свою часть
        # вспомогательного вектора `p` в массиве `p_part`.
        # Сразу инициализируем его нулём (на итерации s=0):
        p_part = np.zeros(N_part, dtype=np.float64)

        # Объявим также переменные `dot_vec` (для хранения окончательного
        # значения скалярного произведения векторов) и `dot_vec_temp`
        # (для хранения частичных сумм скалярного произведения векторов,
        # собранных на отдельных процессах для пополнения значения `dot_vec`).
        # В отличие от примера 5.1, в этот раз мы будем делать скалярное
        # умножение векторов на всех процессах, так что создадим
        # такие переменные также на всех процессах:
        dot_vec = np.array(0, dtype=np.float64)
        dot_vec_temp = np.empty(1, dtype=np.float64)

        # Цикл по итерациям s от 1 до N включительно:
        for s in range(1, N + 1):
            #------------------------------------------------------------
            # Шаг 1:
            # Обновим или создадим вектор `r`.
            if s == 1:
                # Сначала, на первой итерации s=1, нужно создать вектор `r`:
                #   r_{s} = A.T.dot(A.dot(x_{s}) - b)

                # Но сейчас у нас нет полного вектора `x` - и для
                # уменьшения коммуникаций между процессами, мы не
                # хотим его собирать из кусочков на каждом из процессов,
                # как делали это в примере 3.1.
                # Поэтому разобъём вычисление по формуле выше на
                # несколько этапов.

                # Используем тот факт, что в отличие от примера 5.1,
                # сейчас каждый процесс уже хранит нужную ему копию `x_part`.
                # Поэтому мы сразу можем сделать параллельно на каждом
                # процессе умножение матрицы `A_part`на вектор `x_part`:
                Ax_part_temp = np.dot(A_part, x_part)
                # И просуммировать полученные вектора по всем
                # процессам каждой строки сетки процессов, чтобы собрать
                # на каждом процессе свою часть вектора `Ax_part`,
                # соответствующего произведению `A.dot(x_{s})`.
                # Отметим, что при этом векторы `Ax_part` в каждом стролбце
                # сетки процессов будут просто повторяться:
                Ax_part = np.empty(M_part, dtype=np.float64)
                comm_row.Allreduce([Ax_part_temp, M_part, MPI.DOUBLE],
                                   [Ax_part, M_part, MPI.DOUBLE], op=MPI.SUM)
                # Теперь на каждом процессе можно найти свою часть
                # вектора `A.dot(x_{s}) - b`. При этом вычисления
                # в каждом стролбце сетки процессов будут просто
                # повторяться - но зато не нужно будет передавать
                # найденный вектор своим соседям по строке процессов:
                Axb_part = Ax_part - b_part
                # Наконец, мы можем сделать сразу на всех процессах
                # параллельное умножение матрицы `A_part.T` на вектор
                # `Axb_part`:
                r_part_temp = np.dot(A_part.T, Axb_part)
                if self._alpha != 0.0:
                    r_part_temp = r_part_temp + self._alpha * x_part
                # И просуммировать полученные временные вектора по всем
                # процессам своего столбца сетки процессов, одновременно
                # раздав всем этим же процессам нужную им часть вектора `r_part`:
                comm_col.Allreduce([r_part_temp, N_part, MPI.DOUBLE],
                                   [r_part, N_part, MPI.DOUBLE], op=MPI.SUM)
            else:
                # На всех последующих итерациях, вектор `r` нужно просто
                # обновить:
                #    dot_pq = dot(p_{s-1}, q_{s-1})
                #    r_{s} = r_{s-1} - q_{s-1} / dot_pq

                # Это можно сделать для каждого кусочка `r_part` отдельно,
                # параллельно на каждом процессе каждой строки сетки процессов
                # - нет нужды собирать полный вектор `r`.

                # Но первым делом нам нужно знать скалярное произведение
                # векторов `p_{s-1}` и `q_{s-1}`, вычисленных на предыдущей
                # итерации.
                # Важно, что именно это скалярное произведение мы уже вычислили
                # в самом конце предыдущей итерации и поместили его в переменную
                # `dot_vec`, где оно до сих пор ещё и хранится (одно и то же
                # значение на каждом процессе!).
                # Просто воспользуемся им для обновления `r_part`:
                r_part = r_part - q_part/dot_vec

            #------------------------------------------------------------
            # Шаг 2:
            # Посчитаем скалярное произведение векторов dot(r, r):
            #    dot_rr = dot(r_{s}, r_{s})
            # И обновим вектор `p`:
            #    p_{s} = p_{s-1} + r_{s} / dot_rr

            # Сделаем это независимо на процессах каждой строки
            # сетки процессов (повторяя для каждой строки одни
            # и те же расчёты - но без необходимости передавать
            # потом результаты этих расчётов на другие строки):
            dot_vec_temp[0] = np.dot(r_part, r_part)
            comm_row.Allreduce([dot_vec_temp, 1, MPI.DOUBLE],
                               [dot_vec, 1, MPI.DOUBLE], op=MPI.SUM)
            p_part = p_part + r_part / dot_vec

            #------------------------------------------------------------
            # Шаг 3:
            # Обновим вектор `q`:
            #    q_{s} = A.T.dot(A.dot(p_{s}))

            # Для минимизации объёма данных, которые мы должны будем
            # передавать между процессами, разобъём этот шаг на два этапа.
            # Сначала параллельно на каждом процессе умножим `A_part` на `p_part`
            # и соберём в пределах каждой строки нашей сетки процессов свою
            # часть `Ap_part` такого произведения, раздав его всем процессам
            # в пределах своей строки сетки процессов:
            Ap_part_temp = np.dot(A_part, p_part)
            Ap_part = np.empty(M_part, dtype=np.float64)
            comm_row.Allreduce([Ap_part_temp, M_part, MPI.DOUBLE],
                               [Ap_part, M_part, MPI.DOUBLE], op=MPI.SUM)
            # ВАЖНО: последняя команда пересылает *частичный* вектор
            #        с размером `M_part` *всем* процессам - такая операция
            #        плохо масштабируется, если `M_part` будет большим.
            #        Из-за этого данный алгоритм может хуже параллелизоваться,
            #        чем алгоритмы в примерах 3.1-3.2 при M >> N.

            # Теперь снова параллельно на каждом процессе умножим `A_part.T`
            # на `Ap_part`:
            q_part_temp = np.dot(A_part.T, Ap_part)
            if self._alpha != 0.0:
                q_part_temp = q_part_temp + self._alpha * p_part
            # И соберём - но на этот раз в пределах каждого *столбца*
            # нашей сетки процессов (из-за того, что `A_part.T` - это
            # *транспонированная* часть матрицы) свою часть `q_part`
            # такого произведения, раздав его копии всем процессам
            # каждого столбца:
            comm_col.Allreduce([q_part_temp, N_part, MPI.DOUBLE],
                               [q_part, N_part, MPI.DOUBLE], op=MPI.SUM)

            #------------------------------------------------------------
            # Шаг 4:
            # Посчитаем скалярное произведение векторов dot(p, q):
            #    dot_pq = dot(p_{s}, q_{s})
            # и обновим вектор `x`:
            #    x_{s+1} = x_{s} - p_{s} / dot_pq

            # Сделаем это одновременно на всех процессах (повторяя
            # одни и те же вычисления в каждой строке сетки процессов).
            # Первым делом, на каждом процессе получим скалярное
            # произведение кусочков `p_part` и `q_part`:
            dot_vec_temp[0] = np.dot(p_part, q_part)
            # и командой `comm_row.Allreduce()` сначала просуммируем все
            # полученные произведения (суффикс `reduce`!), собрав
            # их в скалярное произведение полных векторов `p` и `q`,
            # а затем раздадим полученное значение каждому процессу
            # (префикс `All`) из своей строки сетки процессов:
            comm_row.Allreduce([dot_vec_temp, 1, MPI.DOUBLE],
                               [dot_vec, 1, MPI.DOUBLE], op=MPI.SUM)

            # Сейчас мы, наконец, можем обновить `x_part` на каждом
            # процессе:
            x_part = x_part - p_part / dot_vec
            # Объединять эти кусочки в полный вектор `x` нет нужды -
            # в полном виде этот вектор будет нам нужен только
            # после завершения работы этой функции.

        return x_part

    # -------------------------------------------------------------------------
    def _conjugate_gradient_method(self, A, b, x):
        np = self._numpy_lib
        # Использование в этом алгоритме ленточного перемножения
        # матрицы на вектор не позволяет использовать симметрию
        # матрицы - её приходится домножать на A_part.T, чтобы
        # получить снова вектор с размером `x`, а не `x_part`.
        self._is_symmetric = False

        self._timer = BenchmarkTimer()
        self._timer.is_active = self._verbose
        self._timer.start('ALL')

        comm = self._comm
        rank = self._rank
        P = self._P

        #------------------------------------------------------------------
        # Шаг 0:
        # Реализуемый в этот раз алгоритм будет работать даже на одном
        # MPI процессе - процесс 0 в этот раз будет и командовать и работать!
        # Однако (см. Шаг 2 ниже), он использует простую квадратную сетку
        # разбиения матрицы на процессы, и поэтому требует использования R**2
        # процессов, где R - положительное целое число. То есть, допустимое
        # число процессов: 1, 4, 9, 16, 25, 36, 49, 64, 81, 100 и т.д.

        if np.int32(np.sqrt(P))**2 != P:
            raise ValueError(
                "\nThe number of processes is not a square of some integer "
                "number. Please launch the program with the command:\n"
                ">> mpiexec.exe -n P python.exe program_name.py\n"
                "where `P=R*R` with an integer `R`\n"
                "(say, P=1, 4, 9, 16, 25, 36, 49, 64, 81, 100)"
            )

        #------------------------------------------------------------------
        # Шаг 1: Размер (M, N) матрицы A.
        if rank == 0:
            M = np.array(A.shape[0], dtype=np.int32)
            N = np.array(A.shape[1], dtype=np.int32)
        else:
            # На "рабочих" процессах значение `M` не
            # используется - ставим пустую "заглушку":
            M = None
            # и подготавливаем на них "хранилище" для `N`:
            N = np.array(0, dtype=np.int32)

        # Раздадим значение `N`, зачитанное процессом 0,
        # всем остальным процессам. Это значение будет передаваться
        # каждым процессом внутрь функции `conjugate_gradient_method()`
        # для указания требуемого числа итераций:
        comm.Bcast([N, 1, MPI.INT], root=0)

        # NOTE: Значение `M` не используется на "рабочих" процессах,
        #       так что мы его не будем раздавать - лишняя трата времени.

        #------------------------------------------------------------------
        # Шаг 2:
        # Считаем для простоты разбиения матрицы на блоки, что наше число
        # процессов P=R*R является квадратом некоторого целого числа R:
        R = np.int32(np.sqrt(P))

        # Тогда будет естественно разбить матрицу на R*R блоков, так что
        # число `num_col` столбцов блоков и число `num_row` строк блоков
        # будет одинаковым и равным `R`:
        num_col = num_row = R
        # Понадобится внутри `self._cgm_mpi_part()`:
        self._num_col = num_col

        # Разберёмся для нашей матрицы `A` с размером MxN, как именно
        # `M` строк элементов матрицы должны разбиться на `num_row` блоков
        # по вертикали - посчитаем для этого списки числа строк `rcounts_M`
        # и их смещений `displs_M`.
        # Точно так же разберёмся, как именно `N` столбцов элементов матрицы
        # должны разбиться на `num_col` блоков по горизонтали - посчитаем для
        # этого списки числа столбцов `rcounts_N` и их смещений `displs_N`.

        if rank == 0:
            # Сделаем такой анализ только на процессе 0.
            # Всю логику расчёта этих списков мы перенесём в отдельную
            # функцию `auxiliary_arrays_determination()`, определённую
            # выше в этом файле:
            rcounts_M, displs_M = self._auxiliary_arrays_determination2(M, num_row)
            rcounts_N, displs_N = self._auxiliary_arrays_determination2(N, num_col)
        else:
            # На "рабочих" процессах все эти списки используются только
            # в качестве пустых заглушек (в нескольких `Scatter` ниже):
            rcounts_M = displs_M = None
            rcounts_N = displs_N = None

        # Подготовим "хранилища" для `M_part` и `N_part` на всех процессах:
        M_part = np.array(0, dtype=np.int32)
        N_part = np.array(0, dtype=np.int32)

        # ВАЖНЫЙ МОМЕНТ:
        # На каждом процессе создадим два новых коммуникатора,
        # которые позволят общаться этому процессу со своими соседями
        # в пределах одной и той же строки или одного и того же
        # столбца.

        # Один коммуникатор, `comm_col`, будет использоваться для
        # общения данного процесса со всеми процессами, которые
        # образуют один и тот же столбец блочной матрицы.
        # Всего будет `num_row` процессов, связанных между собой
        # таким образом - при этом всего будет создано `num_col`
        # таких коммуникаторов:

        color_col = rank % num_col
        comm_col = comm.Split(color_col, rank)

        # Например, для случая 9 процессов:
        #
        #          Processes     Blocks of matrix A
        #          0  1  2       (0,0) (0,1) (0,2)
        #          3  4  5  ==>  (1,0) (1,1) (1,2)
        #          6  7  8       (2,0) (2,1) (2,2)
        #
        # мы получим три возможных значения для `color_col`
        # (каждое такое значение создаёт новый коммуникатор):
        #   0 для процессов 0, 3, 6 - они образуют первый столбец блочной матрицы
        #   1 для процессов 1, 4, 7 - они образуют второй столбец блочной матрицы
        #   2 для процессов 2, 5, 8 - они образуют третий столбец блочной матрицы

        # Второй коммуникатор, `comm_row`, будет использоваться для
        # общения данного процесса со всеми процессами, которые
        # образуют одну и ту же строку блочной матрицы.
        # Всего будет `num_col` процессов, связанных между собой
        # таким образом - при этом всего будет создано `num_row`
        # таких коммуникаторов:

        color_row = rank // num_col
        comm_row = comm.Split(color_row, rank)

        # Например, для случая 9 процессов, мы получим три возможных значения
        # для `color_row`:
        #   0 для процессов 0, 1, 2 - они образуют первую строку блочной матрицы
        #   1 для процессов 3, 4, 5 - они образуют вторую строку блочной матрицы
        #   2 для процессов 6, 7, 8 - они образуют третью строку блочной матрицы

        # Проверим созданные коммуникаторы:
        # print(f"comm = {comm}")
        # print(f"comm.Get_size() = {comm.Get_size()}")
        # print(f"comm.Get_rank() = {comm.Get_rank()}")
        # print(f"comm_col = {comm_col}")
        # print(f"comm_col.Get_size() = {comm_col.Get_size()}")
        # print(f"comm_col.Get_rank() = {comm_col.Get_rank()}")
        # print(f"comm_row = {comm_row}")
        # print(f"comm_row.Get_size() = {comm_row.Get_size()}")
        # print(f"comm_row.Get_rank() = {comm_row.Get_rank()}")

        # Для удобства, введём список процессов из первой строки
        # сетки процессов:
        procs_first_row = np.arange(num_col)
        # и список процессов из первого столбца сетки процессов:
        procs_first_col = np.arange(0, P, num_col)

        # Разбросаем теперь список числа столбцов `rcounts_N`
        # элементов в блоках матрицы в виде значений `N_part`
        # по всем процессам, которые образуют первую строку
        # блочной матрицы:
        if rank in procs_first_row:
            comm_row.Scatter([rcounts_N, 1, MPI.INT],
                             [N_part, 1, MPI.INT], root=0)

        # И размножим полученное `N_part` (разное для разных столбцов,
        # но одинаковое в пределах одного и того же столбца) на все
        # процессы в пределах одного и того же столбца:
        comm_col.Bcast([N_part, 1, MPI.INT], root=0)

        # Аналогично, разбросаем теперь список числа строк
        # `rcounts_M` элементов в блоках матрицы в виде значений
        # `M_part` по всем процессам, которые образуют первый
        # столбец блочной матрицы:
        if rank in procs_first_col:
            comm_col.Scatter([rcounts_M, 1, MPI.INT],
                             [M_part, 1, MPI.INT], root=0)

        # И размножим полученное `M_part` (разное для разных строк,
        # но одинаковое в пределах одной и той же строки) на все
        # процессы в пределах одной и той же строки:
        comm_row.Bcast([M_part, 1, MPI.INT], root=0)

        #------------------------------------------------------------------
        # Шаг 3:
        # Мы получили полную матрицу `A` на процессе 0 -
        # раздадим его по кусочкам всем "рабочим" процессам.
        # Подготовим "хранилище" для кусочков `A_part` на всех процессах -
        # каждое со своим числом строк `M_part` и столбцов `N_part`:
        A_part = np.empty((M_part, N_part), dtype=np.float64)
        # NOTE: В это раз процесс 0 рабочий - так что матрица `A_part`
        # здесь также будет занимать память.

        # Поскольку теперь мы должны разбросать матрицу `A` по процессам
        # в виде блоков `A_part`, элементы для которых лежат в памяти
        # не одним сплошным куском, то алгоритм работы существенно усложнится.

        # Для облегчения его реализации нам будет удобно создавать временные
        # вспомогательные коммуникаторы для обмена сообщения между временными
        # подгруппами процессов, каждый из которых будут включать в себя
        # процесс 0 (раздающий матрицу `A`) и все процессы, образующие одну
        # из строк блочной матрицы.

        # Для создания таких подгрупп нам нужно будет знать всю
        # текущую группу процессов:
        group = comm.Get_group()

        if rank == 0:
            # На процессе 0 мы получили полную матрицу `A`.
            # Раздадим её по кусочкам всем рабочим процессам.
            # Дополнительная сложность при этом заключается в том, что
            # теперь мы хотим, чтобы процесс 0 был также и "рабочим" -
            # то есть, он тоже должен получить свой кусочек матрицы `A`:
            # Цикл по строкам блоков матрицы `A`:
            for m in range(num_row):
                # Зачитаем сначала всю строку блоков `m` и поместим
                # её во временный массив `A_row_m`:
                A_row_m = np.empty(rcounts_M[m]*N, dtype=np.float64)
                # Цикл по столбцам блоков матрицы `A`:
                for n in range(num_col):
                    A_row_m[rcounts_M[m]*displs_N[n]:
                            rcounts_M[m]*displs_N[n]+rcounts_M[m]*rcounts_N[n]] = \
                        A[displs_M[m]:displs_M[m] + rcounts_M[m],
                          displs_N[n]:displs_N[n] + rcounts_N[n]].flatten()
                # Разбросаем эту строку блоков по отдельным процессам,
                # входящим в эту строку сетки процессов.
                if m == 0:
                    # Первую строку блоков разбросать легко - можно просто
                    # воспользоваться уже созданным коммуникатором `comm_row` -
                    # для процесса 0 (на котором мы сейчас работаем) он связывает
                    # между собой все процессы, входящие в первую строку
                    # сетки процессов.
                    # Размеры и смещения для разбрасывания `A_row_m` по частям `A_part`:
                    size_m = rcounts_M[m]*rcounts_N
                    displ_m = rcounts_M[m]*displs_N
                    # Процесс 0 разбросает `A_row_m` по частям, отдав свой кусочек
                    # также и себе (то есть, он будет и посылать и получать данные):
                    comm_row.Scatterv([A_row_m, size_m, displ_m, MPI.DOUBLE],
                                      [A_part, M_part*N_part, MPI.DOUBLE], root=0)
                else:
                    # Вторую и последующие строки блоков разбросать сложнее -
                    # для них нужно создать новый временный коммуникатор, который
                    # будет включать в себя процесс 0 и все процессы, образующие
                    # строку `m` сетки процессов.
                    # Создадим сначала нужную подгруппу процессов так, чтобы
                    # глобальный процесс 0 остался процессом 0 и в новой подгруппе:
                    group_temp = group.Range_incl([(0,0,1), (m*num_col,(m+1)*num_col-1,1)])
                    # Теперь создадим временный коммуникатор для этой подгруппы:
                    comm_temp = comm.Create(group_temp)
                    # Временная подгруппа процессов больше не нужна - освободим её
                    # не откладывая, чтобы не забыть:
                    group_temp.Free()
                    # Размеры и смещения для разбрасывания `A_row_m` по частям `A_part` -
                    # теперь вычисляются чуть сложнее:
                    rcounts_N_temp = np.hstack((np.array(0, dtype=np.int32), rcounts_N))
                    displs_N_temp = np.hstack((np.array(0, dtype=np.int32), displs_N))
                    size_m_temp = rcounts_M[m]*rcounts_N_temp
                    displ_m_temp = rcounts_M[m]*displs_N_temp
                    # Пустая заглушка для процесса 0 - на этот раз он ничего получать не будет:
                    A_empty = np.empty(0, dtype=np.float64)
                    # Теперь процесс 0 разбросает `A_row_m` по частям всем процессам
                    # временного коммуникатора, кроме себя:
                    comm_temp.Scatterv([A_row_m, size_m_temp, displ_m_temp, MPI.DOUBLE],
                                       [A_empty, 0, MPI.DOUBLE], root=0)
                    # Не забудем обязательно освободить временный коммуникатор:
                    comm_temp.Free()
                # Поможем сборщику мусора поскорее найти мусор:
                del A_row_m
        else:
            # Все остальные процессы должны просто получить свою часть матрицы
            # `A` и записать её в свой массив `A_part`.

            # Но здесь также возникают тонкости. Если процесс входит в состав
            # первой строки сетки процессов, то он должен получить данные
            # от процесса 0, который также входит в эту строку сетки процессов,
            # через коммуникатор `comm_row` для данной строки сетки процессов:
            if rank in procs_first_row:
                comm_row.Scatterv([None, None, None, None],
                                  [A_part, M_part*N_part, MPI.DOUBLE], root=0)
            # Все остальные процессы должны получить свои данные через временный
            # коммуникатор, который будет включать в себя глобальный процесс 0 и
            # все процессы, образующие строку `m` сетки процессов.
            for m in range(1, num_row):
                # Как мы уже делали для процесса 0, создадим сначала нужную подгруппу
                # процессов так, чтобы глобальный процесс 0 остался процессом 0 и
                # в новой подгруппе:
                group_temp = group.Range_incl([(0,0,1), (m*num_col,(m+1)*num_col-1,1)])
                # И создадим временный коммуникатор для этой подгруппы:
                comm_temp = comm.Create(group_temp)
                # Временная подгруппа процессов больше не нужна - освободим её
                # не откладывая, чтобы не забыть:
                group_temp.Free()

                # Теперь мы готовы принять данные, посланные нам процессом 0 через
                # этот временный коммуникатор:
                if rank in range(m*num_col, (m+1)*num_col):
                    # Принимаем данные только на процессах, входящих в подгруппу
                    # строки `m` сетки процессов - для всех других процессов созданный выше
                    # `comm_temp` будет "чужим" - им нужно дождаться создания "своего"
                    # временного коммуникатора:
                    comm_temp.Scatterv([None, None, None, None],
                                       [A_part, M_part*N_part, MPI.DOUBLE], root=0)
                    # Не забудем обязательно освободить временный коммуникатор:
                    comm_temp.Free()

        #------------------------------------------------------------------
        # Шаг 4:
        # Мы получили полный вектор `b` на процессе 0 - на остальных
        # процессах `b` is None.

        # На всех процессах (включая и сам процесс 0) будут храниться
        # кусочки вектора `b` в виде векторов `b_part`. Подготовим
        # "хранилище" для них:
        b_part = np.empty(M_part, dtype=np.float64)

        # Разбросаем вектор `b` по всем процессам
        # первого столбца сетки процессов в виде кусочков `b_part`
        # с размерами `M_part = rcounts_M[m]`, используя смещения
        # `displs_M[m]`:
        if rank in procs_first_col:
            comm_col.Scatterv([b, rcounts_M, displs_M, MPI.DOUBLE],
                              [b_part, M_part, MPI.DOUBLE], root=0)

        # И раздадим копии полученных `b_part` с процессов первого столбца
        # (root=0) всем своим соседям в каждой строке сетки процессов:
        comm_row.Bcast([b_part, M_part, MPI.DOUBLE], root=0)

        #------------------------------------------------------------------
        # Шаг 5:
        # Мы получили полный вектор `x` на процессе 0 - на остальных
        # процессах `x` is None.
        # На всех процессах (включая и сам процесс 0) будут храниться
        # кусочки вектора `x` в виде векторов `x_part`. Подготовим
        # "хранилище" для них:
        x_part = np.empty(N_part, dtype=np.float64)

        # И разбросаем вектор `x` по всем процессам первой строки сетки процессов
        # в виде кусочков `x_part` с размерами `N_part = rcounts_N[m]`,
        # используя смещения `displs_N[m]`:
        if rank in procs_first_row:
            comm_row.Scatterv([x, rcounts_N, displs_N, MPI.DOUBLE],
                            [x_part, N_part, MPI.DOUBLE], root=0)

        # Наконец, раздадим копии полученных `x_part` с процессов первой
        # строки (root=0) всем своим соседям в каждом столбце сетки процессов:
        comm_col.Bcast([x_part, N_part, MPI.DOUBLE], root=0)

        #------------------------------------------------------------------
        # Шаг 6:
        # Собственно, и сама нужная нам работа - решение системы линейных
        # уравнений итерационным методом сопряжённых градиентов.

        # Вызываемый на каждом процессе нашей сетки процессов, этот алгоритм
        # отработает со своими кусками массивов `A`, `b`, и `x` в виде `A_part`,
        # `b_part`, и `x_part`, и возвратит свой кусок найденного решения
        # `x_part` (детали описаны внутри самой функции):
        if self._skip_init_time:
            self._timer_calc.start()

        self._timer.start('conj_grad_method')
        x_part = self._cgm_mpi_part(A_part, b_part, x_part, N_part, M_part,
                                    N, comm_row, comm_col)

        # ВАЖНО: Реальные значения для `x_part` вернут все процессы из
        # каждой строки сетки процессов - проверим это (детали описаны
        # внутри самой функции):
        # print(f"Vector `x_part` on process {rank} consists of {len(x_part)} elements:")
        # print(f"  x_part = {x_part}")

        # Соберём вектор `x` на процессе 0 (root=0) из кусочков `x_part`,
        # присланных всеми процессами первой строки сетки процессов
        # (хотя мы могли бы собрать такой полный вектор `x` из кусочков
        # `x_part` любой строки сетки процессов - каждая строка хранит
        # сейчас одинаковые копии `x_part`):
        if rank in procs_first_row:
            comm_row.Gatherv([x_part, N_part, MPI.DOUBLE],
                             [x, rcounts_N, displs_N, MPI.DOUBLE], root=0)

        self._timer.stop('conj_grad_method')
        self._timer.stop('ALL')

        return x

# =============================================================================
