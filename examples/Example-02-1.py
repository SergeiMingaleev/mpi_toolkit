#------------------------------------------------------------------
# Пример 2.1: Параллельное скалярное произведение вектора x
#             на вектор y.
#
# Задача: найти число dot_xy = x*y, максимально эффективно параллелизуя
#         работу с использованием MPI интерфейса.
#
# Считаем, что общее число MPI процессов равно P.
# При этом процесс 0 будет занят исключительно синхронизацией
# данных между процессами, а реальная работа будет выполняться
# процессами от 1 до P-1 (будем называть их "рабочими" процессами).
#
# Размер векторов x и y равен N. Оба вектора будут первоначально созданы
# (и будут храниться в полном виде в переменных x и y) на процессе 0.
# Мы также распределим эти векторы между всеми "рабочими" процессами
# по частям в форме векторов x_part и y_part (с одинаковым названием,
# но разным содержимым на каждом процессе).
# Считая, что N = (P-1) * K + L, где K и L - это целые числа,
# причём 0 <= L <= P-2, мы можем держать на каждом процессе
# либо по K+1 либо по К элементов каждого вектора, для максимальной
# балансировки памяти и вычислений по всем "рабочим" процессам.
# Конкретно, пусть процессы от 1 до L содержат по K+1 элементов (если L=0,
# то таких процессов не будет!), а процессы от L+1 до P-1 содержат
# по K элементов. Тогда на первой группе процессов размер векторов
# x_part и y_part будет равен K+1, а на второй - K.
#
# Каждый "рабочий" процесс посчитает свою часть скалярного произведения
# вектора x_part на y_part:
#   dot_xy_part = np.dot(x_part, y_part)
# а потом мы соберём из dot_xy_part полное значение скалярного произведения
#   dot_xy = sum(dot_xy_part)
# (только на процессе 0), который и будет требуемым нам результатом
# вычислений.
#------------------------------------------------------------------

import numpy as np
from mpi4py import MPI

# Работаем с коммуникатором по всем доступным процессам:
comm = MPI.COMM_WORLD

# Число P доступных процессов в этом коммуникаторе:
P = comm.Get_size()

# Номер текущего процесса (от 0 до P-1):
rank = comm.Get_rank()

#------------------------------------------------------------------
# Шаг 1:
# Разберёмся, сколько элементов (и какие именно!) из векторов `x`
# и `y` мы будем хранить в форме частичных векторов `x_part` и
# `y_part` на каждом из "рабочих" процессов.
# Сделаем такой анализ только на процессе 0:

if rank == 0:
    # Для простоты, зададим размер `N` векторов `x` и `y` руками:
    N = 20
    # Найдём целые числа K и L из описания алгоритма выше:
    K, L = divmod(N, P - 1)

    # Введём два новых списка для описания того,
    # как именно векторы `x` и `y` распределяются по всем
    # процессам. Здесь `rcounts` будет содержать
    # число элементов, хранимое каждым процессом в векторах
    # `x_part` и `y_part` (это 0 для процесса 0, K+1 для первых L
    # "рабочих" процессов, и K для оставшихся процессов).
    # Другой список `displs` будет содержать индекс смещений
    # - то есть номер первого элемента, начиная с которого будут
    # храниться `rcounts[m]` элементов на процессе `m`.
    # При этом мы предполагаем, что все элементы, которые
    # хранятся на каждом процессе, идут подряд.
    rcounts = np.empty(P, dtype=np.int32)
    displs = np.empty(P, dtype=np.int32)

    # Процесс 0 не рабочий, и он не будет содержать
    # никаких элементов в `x_part` и `y_part` - хотя сами
    # вектора (но пустые - ноль элементов!) мы будем создавать
    # и на нём:
    rcounts[0] = displs[0] = 0

    # Цикл по всем "рабочим" процессам:
    for m in range(1, P):
        if m <= L:
            rcounts[m] = K + 1
        else:
            rcounts[m] = K
        # Индекс смещений сдвигается каждый раз на число
        # элементов вектора, хранимых в процессе:
        displs[m] = displs[m - 1] + rcounts[m - 1]
else:
    # На "рабочих" процессах список смещений `displs`
    # не используется и значит не нужен - а из списка
    # `rcounts` каждый процесс использует только одно
    # число - ниже мы его назовём `N_part`, так что
    # сам список `rcounts` тоже не понадобится:
    rcounts = displs = None

# Подготовим "хранилище" для `N_part` на всех процессах:
N_part = np.array(0, dtype=np.int32)

# И разбросаем рассчитанные выше `rcounts` по всем процессам
# в виде одного значения `N_part = rcounts[m]` на каждом
# процессе `m`:
comm.Scatter([rcounts, 1, MPI.INT],
             [N_part, 1, MPI.INT], root=0)

#------------------------------------------------------------------
# Шаг 2:
# Зададим векторы `x` и `y`. Будем различать случаи, когда y==x и
# когда два вектора различны (чтобы не передавать дважды вектор x
# на другие процессы, если y==x):
#y_is_x = False
y_is_x = True

# Для простоты, расчитаем оба вектора на процессе 0:
if rank == 0:
    x = np.arange(1, N+1, dtype=np.float64)
    # Вектор y может отличаться от x:
    # y = np.arange(5, N+5, dtype=np.float64)
    # а может и совпадать:
    y = x
else:
    # На "рабочих" процессах векторов `x` и `y` не будет -
    # вместо них мы хотим создать векторы `x_part` и `y_part`,
    # которые будут содержать только частьи векторов `x` и `y`:
    x = y = None

# Подготовим "хранилища" для `x_part` и `y_part` на всех процессах -
# каждое со своим числом строк `N_part`:
x_part = np.empty(N_part, dtype=np.float64)
if y_is_x:
    # Это один и тот же вектор - не тратим время и память на `y_part`:
    y_part = x_part
else:
    y_part = np.empty(N_part, dtype=np.float64)

# Теперь разобъём векторы `x` и `y` на под-вектора, и раздадим каждому
# "рабочему" процессу свою часть элементов, записав их в векторы
# с одинаковыми на всех процессах именами `x_part` и `y_part`:

comm.Scatterv([x, rcounts, displs, MPI.DOUBLE],
              [x_part, N_part, MPI.DOUBLE], root=0)

if not y_is_x:
    comm.Scatterv([y, rcounts, displs, MPI.DOUBLE],
                  [y_part, N_part, MPI.DOUBLE], root=0)

# NOTE: процесс 0 тоже получает свой набор данных из 0 элементов!

#------------------------------------------------------------------
# Шаг 3:
# Собственно и сама нужная нам работа - скалярное произведение векторов.

# Первым делом, подготовим "хранилище" для `dot_xy_part` на всех процессах:
dot_xy_part = np.empty(1, dtype=np.float64)

# И теперь можем взять скалярное произведение вектора `x_part` на `y_part`
# на каждом процессе. На процессе 0 результат скалярного произведения пустых
# векторов `x_part` и `y_part` будет нулём. А на каждом "рабочем" процессе
# мы получим значение `dot_xy_part[0]`:

# NOTE: поскольку результатом операции np.dot(x_part, y_part) будет
# просто число - мы должны положить его внутрь массива `dot_xy_part`,
# чтобы можно было дальше передать его по ссылке в `comm.Reduce()`:
dot_xy_part[0] = np.dot(x_part, y_part)

# Любопытства ради, напечатаем его на всех процессах:
print('dot_xy_part[0] = {0:6.1f} on process {1}'.format(dot_xy_part[0], rank))

# Подготовим "хранилище" для результирующего значения `dot_xy`:
dot_xy = np.array(0, dtype=np.float64)

# И соберём значение `dot_xy` на процессе 0 (root), просуммировав кусочки
# `dot_xy_part`, присланные всеми процессами (включая и равный нулю кусочек
# от самого процесса 0):
comm.Reduce([dot_xy_part, 1, MPI.DOUBLE],
            [dot_xy, 1, MPI.DOUBLE], op=MPI.SUM, root=0)

# Альтернативно:
# comm.Allreduce([dot_xy_part, 1, MPI.DOUBLE],
#                [dot_xy, 1, MPI.DOUBLE], op=MPI.SUM)

#------------------------------------------------------------------
# Шаг 4:
# Окончательно, напечатаем полученное значение `dot_xy` в консоли.
# Любопытства ради, напечатаем его на всех процессах - заметим при
# этом, что правильное ненулевое значение получилось только на
# процессе 0, куда и передавались для суммирования через Reduce()
# все посчитанные значения `dot_xy_part`:

print('dot_xy = {0:6.1f} on process {1}'.format(dot_xy, rank))

#------------------------------------------------------------------
