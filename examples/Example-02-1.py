#------------------------------------------------------------------
# Пример 2.1: Параллельное скалярное произведение двух векторов.
#
# Задача: найти число dot_xy = dot(x, y), максимально эффективно
#         параллелизуя работу между процессами с использованием
#         MPI интерфейса.
#
# Считаем, что общее число MPI процессов равно P.
# При этом процесс 0 будет занят исключительно синхронизацией
# данных между процессами, а реальная работа будет выполняться
# процессами от 1 до P-1 (будем называть их "рабочими" процессами).
#
# Пусть размер векторов x и y равен M. Оба вектора будут первоначально
# созданы (и будут храниться в полном виде в переменных x и y) на
# процессе 0.
# Мы также распределим эти векторы между всеми "рабочими" процессами
# по частям в форме частичных векторов x_part и y_part (с одинаковым
# названием, но разным содержимым на каждом процессе).
# Считая, что M = (P-1) * K + L, где K и L - это целые числа,
# причём 0 <= L <= P-2, мы можем держать на каждом процессе
# либо по K+1 либо по К элементов каждого вектора, для максимальной
# балансировки памяти и вычислений по всем "рабочим" процессам.
# Конкретно, пусть процессы от 1 до L содержат по K+1 элементов (если L=0,
# то таких процессов не будет!), а процессы от L+1 до P-1 содержат
# по K элементов. Тогда на первой группе процессов размер векторов
# x_part и y_part будет равен K+1, а на второй - K.
#
# Каждый "рабочий" процесс посчитает свою часть скалярного произведения
# вектора x_part на y_part и запишет его во временную переменную
# dot_xy_temp как часть нужного нам значения:
#   dot_xy_temp = np.dot(x_part, y_part)
# а потом мы соберём из dot_xy_temp полное значение скалярного произведения
#   dot_xy = sum(dot_xy_temp)
# (только на процессе 0), который и будет требуемым нам результатом
# вычислений.
#
#------------------------------------------------------------------
# Этот пример (в его оригинальном виде) детально обсуждается
# в лекции Д.В. Лукьяненко "2. Введение в основы MPI на Python":
# https://youtu.be/6gkDvIrQqDc?list=PLcsjsqLLSfNCxGJjuYNZRzeDIFQDQ9WvC
#------------------------------------------------------------------

import numpy as np
from mpi4py import MPI

# Работаем с коммуникатором по всем доступным процессам:
comm = MPI.COMM_WORLD

# Число P доступных процессов в этом коммуникаторе:
P = comm.Get_size()

# Номер текущего процесса (от 0 до P-1):
rank = comm.Get_rank()

#------------------------------------------------------------------
# Шаг 0:
# Реализуемый алгоритм требует для работы хотя бы два MPI процесса -
# один будет командовать, а второй работать. Если процесс всего один,
# то работать просто некому!

if P == 1:
    raise ValueError(
        "\nAt least 2 MPI processes are needed to run this program!\n"
        "Please launch it with the command:\n"
        "mpiexec.exe -n * python.exe Example-02-1.py"
    )

#------------------------------------------------------------------
# Шаг 1:
# Разберёмся, сколько элементов (и какие именно!) из векторов `x`
# и `y` мы будем хранить в форме частичных векторов `x_part` и
# `y_part` на каждом из "рабочих" процессов.
# Сделаем такой анализ только на процессе 0:

if rank == 0:
    # Для простоты, зададим размер `M` векторов `x` и `y` руками:
    M = 20
    # Найдём целые числа K и L из описания алгоритма выше:
    K, L = divmod(M, P - 1)

    # Введём два новых списка для описания того,
    # как именно векторы `x` и `y` распределяются по всем
    # процессам. Здесь `rcounts` будет содержать
    # число элементов, хранимое каждым процессом в векторах
    # `x_part` и `y_part` (это 0 для процесса 0, K+1 для первых L
    # "рабочих" процессов, и K для оставшихся процессов).
    # Другой список `displs` будет содержать индекс смещений
    # - то есть номер первого элемента, начиная с которого будут
    # храниться `rcounts[m]` элементов на процессе `m`.
    # При этом мы предполагаем, что все элементы, которые
    # хранятся на каждом процессе, идут подряд.
    rcounts = np.empty(P, dtype=np.int32)
    displs = np.empty(P, dtype=np.int32)

    # Процесс 0 не рабочий, и он не будет содержать
    # никаких элементов в `x_part` и `y_part` - хотя сами
    # вектора (но пустые - ноль элементов!) мы будем создавать
    # и на нём:
    rcounts[0] = displs[0] = 0

    # Цикл по всем "рабочим" процессам:
    for m in range(1, P):
        if m <= L:
            rcounts[m] = K + 1
        else:
            rcounts[m] = K
        # Индекс смещений сдвигается каждый раз на число
        # элементов вектора, хранимых в процессе:
        displs[m] = displs[m - 1] + rcounts[m - 1]
else:
    # На "рабочих" процессах списки `rcounts` и `displs`
    # используются (в `comm.Scatterv` ниже) в качестве
    # пустых заглушек:
    rcounts = displs = None

# Подготовим "хранилище" для `M_part` на всех процессах:
M_part = np.array(0, dtype=np.int32)

# И разбросаем рассчитанный выше список `rcounts` по всем
# процессам (включая процесс 0), отдавая по одному элементу
# каждому процессу m, и записывая его в переменную
# `M_part = rcounts[m]`:
comm.Scatter([rcounts, 1, MPI.INT],
             [M_part, 1, MPI.INT], root=0)

#------------------------------------------------------------------
# Шаг 2:
# Зададим векторы `x` и `y`. Будем различать случаи, когда y is x
# (вектор `x` умножается на самого себя) и когда два вектора различны.
# Делаем это, чтобы не передавать дважды вектор x на другие процессы,
# если y тождественен x:
#y_is_x = False
y_is_x = True

# Для простоты, рассчитаем оба вектора на процессе 0:
if rank == 0:
    x = np.arange(1, M + 1, dtype=np.float64)
    # Вектор y может отличаться от x:
    # y = np.arange(5, M+5, dtype=np.float64)
    # а может и совпадать:
    y = x
else:
    # На "рабочих" процессах векторы `x` и `y`
    # используются (в `comm.Scatterv` ниже) в качестве
    # пустых заглушек:
    x = y = None

# Подготовим "хранилища" для `x_part` и `y_part` на всех процессах -
# каждое со своим числом строк `M_part`:
x_part = np.empty(M_part, dtype=np.float64)
if y_is_x:
    # Это один и тот же вектор - не тратим время и память на `y_part`:
    y_part = x_part
else:
    y_part = np.empty(M_part, dtype=np.float64)

# Теперь разобъём векторы `x` и `y` на части, и раздадим каждому
# "рабочему" процессу свою часть элементов, записав их в векторы
# с одинаковыми на всех процессах именами `x_part` и `y_part`
# (смотри детальные комментарии по использованию метода
# `comm.Scatterv()` в примере 1.1):
comm.Scatterv([x, rcounts, displs, MPI.DOUBLE],
              [x_part, M_part, MPI.DOUBLE], root=0)

if not y_is_x:
    comm.Scatterv([y, rcounts, displs, MPI.DOUBLE],
                  [y_part, M_part, MPI.DOUBLE], root=0)

# NOTE: процесс 0 тоже получает свой набор данных из 0 элементов!

#------------------------------------------------------------------
# Шаг 3:
# Собственно, и сама нужная нам работа - скалярное произведение векторов.

# Первым делом, подготовим "хранилище" для `dot_xy_temp` на всех процессах:
dot_xy_temp = np.empty(1, dtype=np.float64)

# И теперь можем взять скалярное произведение вектора `x_part` на `y_part`
# на каждом процессе. При этом, поскольку результатом операции 
# `np.dot(x_part, y_part)` будет просто число, мы должны положить 
# его внутрь массива `dot_xy_temp` как значение элемента `dot_xy_temp[0]`,
# чтобы его можно было дальше передать по ссылке в `comm.Reduce()`:

dot_xy_temp[0] = np.dot(x_part, y_part)

# NOTE: На процессе 0 результат скалярного произведения пустых
# векторов `x_part` и `y_part` будет нулём - проверьте! :-)

# Любопытства ради, напечатаем его на всех процессах:
print(f'dot_xy_temp[0] = {dot_xy_temp[0]} on process {rank}')

# Подготовим "хранилище" для результирующего значения `dot_xy`:
dot_xy = np.array(0, dtype=np.float64)

# И соберём значение `dot_xy` на процессе 0 (root), просуммировав кусочки
# `dot_xy_temp`, присланные всеми процессами (включая и равный нулю кусочек
# от самого процесса 0):
comm.Reduce([dot_xy_temp, 1, MPI.DOUBLE],
            [dot_xy, 1, MPI.DOUBLE], op=MPI.SUM, root=0)

# Альтернативно, можно не только просуммировать их и послать одному только
# процессу `root`, но и переслать это значение вообще всем процессам -
# для этого замените метод `comm.Reduce` на `comm.Allreduce` - в этом
# случае аргумент `root` уже не нужен (выделенного процесса нет - все равны!):
# comm.Allreduce([dot_xy_temp, 1, MPI.DOUBLE],
#                [dot_xy, 1, MPI.DOUBLE], op=MPI.SUM)

# Любопытства ради, напечатаем его на всех процессах - сравните,
# как меняется это значение на "рабочих" процессах при замене
# вызова метода `Reduce` на `Allreduce`:

print(f'dot_xy = {dot_xy} on process {rank}')

#------------------------------------------------------------------
