#------------------------------------------------------------------
# ИЗУЧИТЬ:
# -- Способы расчёта списков числа элементов `rcounts*` и
#    соответствующих смещений "displs*" на шаге 3 кода ниже,
#    определяющих распределение векторов по всем процессам
#    MPI коммуникатора внутри функций
#    `auxiliary_arrays_determination()` и `auxiliary_arrays_from_0()`.
#    Первая функция полностью совпадает с такой же функцией в
#    примере 6.2. Вторая же функция новая и требует изучения!
# -- Сравниваем функции `fast_pde_solution()` и `slow_pde_solution()`.
#    Первая функция отличается только использованием "срезов"
#    в массивах библиотеки `numpy` вместо циклов по индексу `n`.
#    Обратите внимание на использование `comm.Scatterv()` и
#    `comm.Gatherv()` в начале и конце этих функций. Именно они
#    связывают между собой MPI процессы - и делают это очень
#    неэффективным способом (смотри комментарии в коде).
#------------------------------------------------------------------
#    Пример 8.1: Параллельное решение одномерного (1D)
#                дифференциального уравнения в частных производных
#                параболического типа с использованием "явной" схемы
#                метода конечных разностей.
# Реализация №1: Очень неэффективно, в качестве теста, собираем
#                все насчитанные данные на каждом шаге по времени
#                на процессе 0, а затем раздаём их обратно каждому
#                процессу. Реализация служит для демонстрации её
#                численной неэффективности при распараллеливании,
#                особенно на нескольких компьютерах (когда затраты
#                на передачу большого объёма данных будут особенно
#                велики).
#
# Задача: найти решение уравнения:
#            du/dt = eps*(d^2 u/dx^2) + u*(du/dx) + u^3
#         где `eps` - это некоторая константа, а `u(x,t)` - это
#         функция от времени `t` и одной пространственной
#         координаты `x`. Считаем, что эта функция определена
#         на интервале по `x` от `a` до `b` и на интервале
#         по времени от `t0` до `T`.
#
# Для решения такой задачи нам нужно дополнительно знать начальное
# и граничные условия. Будем считать, что в начальный момент времени
# значения этой функции определяются начальным условием:
#    u(x, t0) = u_init(x)
# где функция `u_init(x)` определяется ниже в этом файле.
# Также считаем, что значения функции на границах `x=a` и `x=b`
# определяются граничными условиями:
#    u(a, t) = u_left(t)
#    u(b, t) = u_right(t)
# где функции `u_left(t)` и `u_right(t)` также определяется ниже
# в этом файле.
#
# Для решения уравнения будем использовать метод конечных разностей,
# разбивая диапазон по времени [t0, T] на `M` интервалов и диапазон
# по пространству [a, b] на `N` интервалов.
#
# Для простоты реализации, в этом примере мы будем использовать
# "явную" разностную схему решения уравнения, которая обладает
# первым порядком точности по времени и вторым порядком точности
# по координате:
#    error ~ O(dt, dx^2)
# где `dt = (T-t0) / M` - это шаг по времени `t`,
# а `dx = (b-a) / N` - это шаг по координате `x`.
#
# Такая "явная" схема является условно устойчивой только при выполнении
# жёсткого условия на максимальный шаг по времени:
#    dt < dx^2 / (2*eps)
# что приводит к требованию, что число интервалов по времени `M` должно
# расти как квадрат числа интервалов по пространству `N`: M ~ N^2.
# То есть, при увеличении `N` в 10 раз, нужно увеличить `M` в сто раз!
#
# Более эффективная "неявная" схема решения этого же уравнения будет
# рассмотрена в примерах 9.1 и 9.2 (но она потребует более сложной
# программной реализации, основанной на решении трёхдиагональной
# системы линейных уравнений).
#
#------------------------------------------------------------------
# Этот пример (в его оригинальном виде "Example-8-1.py")
# детально обсуждается в лекции Д.В. Лукьяненко "8. Решение задач
# для уравнений в частных производных. Ч.1" начиная с 37-й минуты:
# https://youtu.be/6SYN28B_iyE?list=PLcsjsqLLSfNCxGJjuYNZRzeDIFQDQ9WvC&t=2253
#
# Объяснения математической части разностных методов для решения
# такого уравнения даётся в лекции Д.В. Лукьяненко
# "Лекция 19. Уравнения в частных производных: продолжение":
# https://teach-in.ru/lecture/2021-07-19-Lukyanenko
# Короткое изложение этих методов можно также найти, например, в:
# https://www.rsatu.ru/upload/medialibrary/ac1/Lektsiya-13.pdf
#------------------------------------------------------------------

import argparse
import numpy as np
from mpi4py import MPI


#------------------------------------------------------------------
def u_init(x):
    """
    Функция, определяющая начальное условие для u(x, t):
               u(x, t0) = u_init(x)
    :param x: Значение пространственной координаты `x`.
              Может быть либо одним числом, либо `numpy`
              массивом, задающим все нужные точки.
    :return: Значение функции для задания `u(x, t0)`.
    """
    res = np.sin(3*np.pi*(x - 1/6))
    return res


#------------------------------------------------------------------
def u_left(t):
    """
    Функция, определяющая граничное условие для u(x, t)
    слева, в точке `x=a`:
               u(a, t) = u_left(t)
    :param t: Значение момента времени `t`.
              Может быть либо одним числом, либо `numpy`
              массивом, задающим все нужные точки.
    :return: Значение функции для задания `u(a, t)`.
    """
    res = -1.0
    return res


#------------------------------------------------------------------
def u_right(t):
    """
    Функция, определяющая граничное условие для u(x, t)
    справа, в точке `x=b`:
               u(b, t) = u_left(t)
    :param t: Значение момента времени `t`.
              Может быть либо одним числом, либо `numpy`
              массивом, задающим все нужные точки.
    :return: Значение функции для задания `u(b, t)`.
    """
    res = 1.0
    return res


#------------------------------------------------------------------
def slow_pde_solution():
    """
    Медленная реализация "явной" разностной схемы - основана
    на использовании медленного цикла по отдельным элементам.
    """
    global P, rank, M, u, u_part, u_part_aux, N_part, N_part_aux, \
           rcounts_from_0, displs_from_0, \
           eps_dt_dx2, dt_2dx, dt

    # Цикл по всем моментам времени:
    for m in range(M):
        # Разбрасываем вектор `u[m]` в частичные векторы `u_part_aux`
        # по всем процессам таким образом, чтобы нулевому процессу достался
        # один дополнительный элемент справа, последнему процессу достался
        # один дополнительный элемент слева, а всем промежуточным процессам
        # досталось по одному дополнительному элементу и слева и справа:
        comm.Scatterv([u[m], rcounts_from_0, displs_from_0, MPI.DOUBLE],
                      [u_part_aux, N_part_aux, MPI.DOUBLE], root=0)
        # NOTE: в реальности, нам было бы достаточно передать каждому процессу
        # только дополнительные элементы слева и/или справа - все остальные
        # элементы (после первой итерации по `m`) уже есть в векторе `u_part`:
        #   u_part_aux[1:-1] == u_part[:] (для случая промежуточных процессов)
        # Здесь реализация намеренно неэффективная, чтобы увидеть последствия
        # такой массированной передачи данных между процессами.

        # Обновляем внутренние точки на каждом процессе (медленно, циклом по `n`):
        for n in range(1, N_part_aux-1):
            u_part[n-1] = u_part_aux[n] + \
                          eps_dt_dx2 * (u_part_aux[n+1] - 2*u_part_aux[n] + u_part_aux[n-1]) + \
                          dt_2dx * u_part_aux[n] * (u_part_aux[n+1] - u_part_aux[n-1]) + \
                          dt * u_part_aux[n]**3

        if rank == 0:
            # Добавляем к решению граничное условие слева на нулевом процессе:
            u_part = np.hstack((np.array(u_left(t[m+1]), dtype=np.float64),
                                u_part[0:N_part-1]))
        elif rank == P-1:
            # Добавляем к решению граничное условие справа на последнем процессе:
            u_part = np.hstack((u_part[0:N_part-1],
                                np.array(u_right(t[m+1]), dtype=np.float64)))

        # Собираем части решения из векторов `u_part` на всех процессах в
        # вектор `u[m+1]` на нулевом процессе:
        comm.Gatherv([u_part, N_part, MPI.DOUBLE],
                     [u[m+1], rcounts, displs, MPI.DOUBLE], root=0)
        # NOTE: продолжение намеренно неэффективной реализации вычислений,
        # чтобы увидеть последствия такой массированной передачи данных между
        # процессами. Смотрите пример 8.2 для более эффективной реализации.


#------------------------------------------------------------------
def fast_pde_solution():
    """
    Быстрая реализация "явной" разностной схемы - основана
    на использовании "срезов" вместо цикла по отдельным элементам.
    """
    global P, rank, M, u, u_part, u_part_aux, N_part, N_part_aux, \
           rcounts_from_0, displs_from_0, \
           eps_dt_dx2, dt_2dx, dt

    # Цикл по всем моментам времени:
    for m in range(M):
        # Разбрасываем вектор `u[m]` в частичные векторы `u_part_aux`
        # по всем процессам таким образом, чтобы нулевому процессу достался
        # один дополнительный элемент справа, последнему процессу достался
        # один дополнительный элемент слева, а всем промежуточным процессам
        # досталось по одному дополнительному элементу и слева и справа:
        comm.Scatterv([u[m], rcounts_from_0, displs_from_0, MPI.DOUBLE],
                      [u_part_aux, N_part_aux, MPI.DOUBLE], root=0)

        # Обновляем внутренние точки на каждом процессе (быстро, используя "срезы"):
        u_part[:N_part_aux-2] = u_part_aux[1:-1] + \
                                eps_dt_dx2 * (u_part_aux[2:] - 2*u_part_aux[1:-1] + u_part_aux[:-2]) + \
                                dt_2dx * u_part_aux[1:-1] * (u_part_aux[2:] - u_part_aux[:-2]) + \
                                dt * u_part_aux[1:-1]**3

        if rank == 0:
            # Добавляем к решению граничное условие слева на нулевом процессе:
            u_part = np.hstack((np.array(u_left(t[m+1]), dtype=np.float64),
                                u_part[0:N_part-1]))
        elif rank == P-1:
            # Добавляем к решению граничное условие справа на последнем процессе:
            u_part = np.hstack((u_part[0:N_part-1],
                                np.array(u_right(t[m+1]), dtype=np.float64)))

        # Собираем части решения из векторов `u_part` на всех процессах в
        # вектор `u[m+1]` на нулевом процессе:
        comm.Gatherv([u_part, N_part, MPI.DOUBLE],
                     [u[m+1], rcounts, displs, MPI.DOUBLE], root=0)
        # NOTE: продолжение намеренно неэффективной реализации вычислений,
        # чтобы увидеть последствия такой массированной передачи данных между
        # процессами. Смотрите пример 8.2 для более эффективной реализации.


# ------------------------------------------------------------------
def auxiliary_arrays_determination(M, P):
    """
    Расчёт списков числа элементов `rcounts` и соответствующих
    смещений "displs", определяющих распределение частичных
    векторов `u_part` по всем процессам MPI коммуникатора,
    включая и процесс 0.

    :param M: Общее число элементов вдоль нужной оси матрицы.
    :param P: Общее число процессов вдоль нужной оси сетки процессов,
              работающих над параллелизацией вычислений.
    :return: Рассчитанные списки числа элементов `rcounts` и
             соответствующих смещений "displs", определяющие
             передачу данных каждому процессу.
    """
    # Считая, что M = P * K + L, где K и L - это целые числа,
    # причём 0 <= L <= P-1, мы можем держать на каждом процессе
    # либо по K+1 либо по К элементов, для максимальной балансировки
    # памяти и вычислений по всем "рабочим" процессам.
    # Найдём целые числа K и L из описания алгоритма выше:
    K, L = divmod(np.int32(M), P)

    # Введём два новых списка для описания того, как именно
    # матрицы и векторы будут распределяться по всем процессам.
    # Здесь `rcounts` будет содержать число элементов, хранимое
    # каждым процессом (это K+1 для первых L процессов,
    # и K для оставшихся процессов).
    # Другой список `displs` будет содержать индекс смещений
    # - то есть, номер первого элемента, начиная с которой будут
    # храниться `rcounts[m]` элементов на процессе `m`.
    # При этом мы предполагаем, что все элементы, которые
    # хранятся на каждом процессе, идут подряд.
    rcounts = np.empty(P, dtype=np.int32)
    displs = np.empty(P, dtype=np.int32)

    # Цикл по всем процессам (они все "рабочие"):
    for m in range(0, P):
        if m < L:
            # Процессы от 0 до L-1 содержат по K+1 элементов
            # (если L=0, то таких процессов не будет!):
            rcounts[m] = K + 1
        else:
            # Оставшиеся процессы от L до P-1 содержат по K элементов:
            rcounts[m] = K
        # Индекс смещений равен 0 для процесса 0 и сдвигается
        # для каждого следующего процесса на число элементов,
        # хранимых в предыдущем процессе:
        if m == 0:
            displs[m] = 0
        else:
            displs[m] = displs[m - 1] + rcounts[m - 1]
    return rcounts, displs


# ------------------------------------------------------------------
def auxiliary_arrays_from_0(P, rcounts, displs):
    """
    Расчёт списков числа элементов `rcounts_from_0` и соответствующих
    смещений "displs_from_0", определяющих распределение вспомогательных
    частичных векторов `u_part_aux` по всем процессам MPI коммуникатора,
    включая и процесс 0. ПРи этом


    больших матриц
    и векторов по всем процессам MPI коммуникатора, включая
    и процесс 0.

    :param M: Общее число элементов вдоль нужной оси матрицы.
    :param P: Общее число процессов вдоль нужной оси сетки процессов,
              работающих над параллелизацией вычислений.
    :return: Рассчитанные списки числа элементов `rcounts` и
             соответствующих смещений "displs", определяющие
             передачу данных каждому процессу.
    """
    # Введём два новых списка для описания того, как именно
    # матрицы и векторы будут распределяться по всем процессам.
    # Здесь `rcounts` будет содержать число элементов, хранимое
    # каждым процессом (это K+1 для первых L процессов,
    # и K для оставшихся процессов).
    # Другой список `displs` будет содержать индекс смещений
    # - то есть, номер первой строки, начиная с которой будут
    # храниться `rcounts[m]` строк на процессе `m`.
    # При этом мы предполагаем, что все элементы, которые
    # хранятся на каждом процессе, идут подряд.
    rcounts_from_0 = np.empty(P, dtype=np.int32)
    displs_from_0 = np.empty(P, dtype=np.int32)

    rcounts_from_0[0] = rcounts[0] + 1
    displs_from_0[0] = 0

    # Цикл по всем процессам (они все "рабочие"):
    for m in range(1, P-1):
        # Процессы от 0 до L-1 содержат по K+1 строк матрицы
        rcounts_from_0[m] = rcounts[m] + 2
        # Индекс смещений равен 0 для процесса 0 и сдвигается
        # для каждого следующего процесса на число строк,
        # хранимых в предыдущем процессе:
        displs_from_0[m] = displs[m] - 1

    rcounts_from_0[P-1] = rcounts[P-1] + 1
    displs_from_0[P-1] = displs[P-1] - 1

    return rcounts_from_0, displs_from_0


#------------------------------------------------------------------
# Начинаем выполнение программы - первым делом, настроим MPI:
#------------------------------------------------------------------

# Включаем таймер:
start_time1 = MPI.Wtime()

# Работаем с коммуникатором по всем доступным процессам:
comm = MPI.COMM_WORLD

# Число P доступных процессов в этом коммуникаторе:
P = comm.Get_size()

# Номер текущего процесса (от 0 до P-1):
rank = comm.Get_rank()

# Реализуемый алгоритм требует использование хотя бы двух
# MPI процессов (из-за используемых способов выбора списков
# `rcounts`, `displs`, `rcounts_from_0`, `displs_from_0`,
# и способа задания граничных условий):

if P == 1:
    raise ValueError(
        "\n At least 2 MPI processes are needed to run this program!"
        "\n Please launch it with the command:"
        "\n mpiexec.exe -n 2 python.exe Example-08-1.py [your args]"
    )

#------------------------------------------------------------------
# Шаг 0:
# Распарсим аргументы запуска программы.

parser = argparse.ArgumentParser(
            prog='python Example-08-1.py',
            description='Решение 1D ДУЧП параболического типа с использованием '
                        '"явной" разностной схемы с MPI параллелизацией.',
)
parser.add_argument('-N', default=800,
                    help='Число `N` интервалов сетки по координате `x`. '
                         'По умолчанию равно 800.')
parser.add_argument('-M', default=100_000,
                    help='Число `M` интервалов сетки по времени `t`. '
                         'По умолчанию равно 100_000.')
parser.add_argument('-T', default=2.0,
                    help='Максимальное время `T`, до которого должны проводиться '
                         'расчёты. По умолчанию равно 2.0.')
parser.add_argument('--slow', action="store_true",
                    help='Использовать медленную реализацию решения, '
                         'с циклом по `n` при обновлении `u[m,n]` '
                         'вместо использования срезов `u[m,:]`.')
parser.add_argument('--noheader', action="store_true",
                    help='Не печатать названия колонок в выводе времени счёта.')
parser.add_argument('--save', action="store_true",
                    help='Сохранить результаты расчётов в файл '
                         '"Example-08-1_Results.npz".')
parser.add_argument('--plot', action="store_true",
                    help='Нарисовать решение для последнего момента времени.')

args = parser.parse_args()

#------------------------------------------------------------------
# Шаг 1:
# Задаём значения для всех констант, определяемых условиями задачи.
eps = 10**(-1.5)
a = 0.0; b = 1.0
t_0 = 0.0

T = float(args.T)
N = int(args.N)
M = int(args.M)

#------------------------------------------------------------------
# Шаг 2:
# Находим значения для всех вспомогательных констант.

# Шаг по пространству:
dx = (b - a) / N

# Шаг по времени:
dt = (T - t_0) / M

# Константы для "быстрого" использования внутри разностной схемы:
eps_dt_dx2 = eps * dt / dx ** 2
dt_2dx = dt / (2 * dx)

# Условие устойчивости "явной" разностной схемы `dt < dx^2 / (2*eps)`
# соответсвтует условию `eps_dt_dx2 < 0.5` - проверим его:
if eps_dt_dx2 >= 0.5:
    msg = f'WARNING: Нарушено условие устойчивости "явной" разностной схемы: ' + \
          f'eps_dt_dx2 = {eps_dt_dx2:.3f} (а должно быть меньше 0.5!).'
    print(msg)

# Массивы для хранения точек используемой сетки по пространству:
x = np.linspace(a, b, N+1)
# и по времени:
t = np.linspace(t_0, T, M+1)

#------------------------------------------------------------------
# Шаг 3:
# Задаём /////.

if rank == 0:
    rcounts, displs = auxiliary_arrays_determination(N+1, P)
else:
    rcounts = displs = None

N_part = np.array(0, dtype=np.int32)

comm.Scatter([rcounts, 1, MPI.INT],
             [N_part, 1, MPI.INT], root=0)

if rank == 0:
    rcounts_from_0, displs_from_0 = auxiliary_arrays_from_0(P, rcounts, displs)
else:
    rcounts_from_0 = displs_from_0 = None

N_part_aux = np.array(0, dtype=np.int32)

comm.Scatter([rcounts_from_0, 1, MPI.INT],
             [N_part_aux, 1, MPI.INT], root=0)

#------------------------------------------------------------------
# Шаг 4:
# Задаём начальное условие.

if rank == 0:
    # Будем хранить все насчитанные данные (для создания анимации)
    # только на процессе 0:
    u = np.empty((M+1, N+1), dtype=np.float64)
    # Начальное условие:
    u[0, :] = u_init(x)
else:
    u = np.empty((M+1, 0), dtype=np.float64)

u_part = np.empty(N_part, dtype=np.float64)
u_part_aux = np.empty(N_part_aux, dtype=np.float64)

#------------------------------------------------------------------
# Шаг 5:
# Собственно, и сама нужная нам работа - решение нашего уравнения
# с помощью "явной" разностной схемы. По умолчанию, решаем уравнение
# быстро, но через переключатель '--slow' решаем медленно, как это
# было сделано в оригинальной версии программы. Сравните код обеих
# функций, `slow_pde_solution()` и `fast_pde_solution()`.

start_time2 = MPI.Wtime()

if args.slow:
    slow_pde_solution()
else:
    fast_pde_solution()

end_time = MPI.Wtime()

#------------------------------------------------------------------
# Шаг 6:
# Печатаем время выполнения и, если нужно, сохраняем результаты
# расчётов в файл и/или рисуем график решения.

if rank == 0:
    duration1 = end_time - start_time1
    duration2 = end_time - start_time2

    if not args.noheader:
        print('N\t M\t Procs\t time_tot\t time_sol')
    print(f'{N}\t {M}\t {P}\t {duration1:.6f}\t {duration2:.6f}')

    # Если нужно, сохраняем данные в файл:
    if args.save:
        filename = 'Example-08-1_Results.npz'
        print(f"Сохраняем данные в файл '{filename}'.")
        np.savez(filename, x=x, t=t, u=u)

    # Если нужно, рисуем решение для последнего момента времени:
    if args.plot:
        from matplotlib import pyplot as plt
        plt.style.use('dark_background')
        fig = plt.figure()
        ax = plt.axes(xlim=(a, b), ylim=(-2.0, 2.0))
        ax.set_xlabel('x');
        ax.set_ylabel('u')
        for m in [0, M // 4, M // 2, -1]:
            ax.plot(x, u[m, :], ls='-', lw=2,
                    label=f'T = {t[m]}')
        plt.legend()
        plt.show()

#------------------------------------------------------------------
