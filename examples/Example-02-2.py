#------------------------------------------------------------------
# Пример 2.2: Параллельное умножение транспонированной матрицы
#             на вектор.
#
# Задача: найти вектор b = dot(A.T, x), максимально эффективно
#         параллелизуя работу между процессами с использованием
#         MPI интерфейса. При этом требуется переиспользовать
#         матрицу A из примера 1.1 в том виде, в котором она
#         уже была распределена (по строкам) между всеми
#         "рабочими" процессами - чтобы избежать затрат времени и
#         памяти на повторное распределение этой матрицы между
#         процессами.
#
# Считаем, что общее число MPI процессов равно P.
# При этом процесс 0 будет занят исключительно синхронизацией
# данных между процессами, а реальная работа будет выполняться
# процессами от 1 до P-1 (будем называть их "рабочими" процессами).
#
# Размер матрицы A равен (M, N) - M строк и N колонок.
# Эта матрица будет в полном виде зачитана (и храниться в массиве A)
# на процессе 0 (точно как в примере 1.1).
#
# Размер транспонированной матрицы A.T, соответственно, равен (N, M) -
# N строк и M колонок. Считая, что при выполнении примера 1.1 мы
# уже распределили матрицу A между всеми "рабочими" процессами по 
# строкам в форме матрицы A_part (с одинаковым названием, но разным 
# содержимым на каждом процессе) с размером (M_part, N), мы должны 
# будем продолжать работать с этой же A_part внутри каждого процесса.
# При этом транспонированные матрицы A_part.T будут иметь размер
# (N, M_part) - то есть, будут группироваться по колонкам, а не 
# по строкам. Для их умножения на вектор x (c размером M), его 
# теперь нужно будет разбить по разным процессам на кусочки 
# x_part с размером M_part, как мы это делали в примере 2.1.
#
# Считая, что M = (P-1) * K + L, где K и L - это целые числа,
# причём 0 <= L <= P-2, мы можем держать на каждом процессе
# либо по K+1 либо по К строк, для максимальной балансировки памяти
# и вычислений по всем "рабочим" процессам.
# Конкретно, пусть процессы от 1 до L содержат по K+1 строк (если L=0,
# то таких процессов не будет!), а процессы от L+1 до P-1 содержат
# по K строк. Тогда на первой группе процессов размер матрицы A_part
# будет равен (K+1, N), а на второй - (K, N).
#
# Размер вектора x равен M. Как уже говорилось выше, этот вектор 
# будет нужно будет разбить по разным процессам на кусочки 
# x_part с размером M_part.
#
# Каждый "рабочий" процесс посчитает свою часть матрично-векторного
# умножения b_temp = dot(A_part.T, x_part). Важно отметить, что это
# будет полноразмерные векторы, с числом элементов равным N - но
# для получения окончательного нужного нам вектора b нужно будет
# просуммировать все b_temp между собой: b = sum(b_temp).
# Мы соберём вектор b только на процессе 0.
#
#------------------------------------------------------------------
# Этот пример (в его оригинальном виде) детально обсуждается
# в лекции Д.В. Лукьяненко "2. Введение в основы MPI на Python":
# https://youtu.be/6gkDvIrQqDc?list=PLcsjsqLLSfNCxGJjuYNZRzeDIFQDQ9WvC
#------------------------------------------------------------------

import numpy as np
from mpi4py import MPI

# Работаем с коммуникатором по всем доступным процессам:
comm = MPI.COMM_WORLD

# Число P доступных процессов в этом коммуникаторе:
P = comm.Get_size()

# Номер текущего процесса (от 0 до P-1):
rank = comm.Get_rank()

#------------------------------------------------------------------
# Шаг 0:
# Реализуемый алгоритм требует для работы хотя бы два MPI процесса -
# один будет командовать, а второй работать. Если процесс всего один,
# то работать просто некому!

if P == 1:
    raise ValueError(
        "\nAt least 2 MPI processes are needed to run this program!\n"
        "Please launch it with the command:\n"
        "mpiexec.exe -n * python.exe Example-02-2.py"
    )

#------------------------------------------------------------------
# Шаг 1 (точно такой же, как в примере 1.1!):
# Зачитаем из файла размер (M, N) матрицы A.
# Сделаем это только на процессе 0 - считаем,
# что входной файл доступен только на нём:

if rank == 0:
    with open('Example-02-2_in.dat', 'r') as f1:
        M = np.array(np.int32(f1.readline()))
        N = np.array(np.int32(f1.readline()))
else:
    # На "рабочих" процессах значение `M` не
    # используется - ставим пустую "заглушку":
    M = None
    # и подготавливаем на них "хранилище" для `N`:
    N = np.array(0, dtype=np.int32)

# Раздадим значение `N`, зачитанное процессом 0,
# всем остальным процессам:
comm.Bcast(N, root=0)
# Альтернативно, более общая форма записи, явно указывающая,
# что мы хотим раздать массив N с размером 1 и типом данных
# MPI.INT от процесса 0 всем остальным процессам:
# comm.Bcast([N, 1, MPI.INT], root=0)

# NOTE: Значение `M` не используется на "рабочих" процессах,
#       так что мы его не будем раздавать - лишняя трата времени.

#------------------------------------------------------------------
# Шаг 2 (точно такой же, как в примере 1.1!):
# Разберёмся, сколько строк (и какие именно!) из матрицы `A` мы будем
# хранить в форме частичной матрицы `A_part` на каждом из "рабочих"
# процессов. Сделаем такой анализ только на процессе 0:

if rank == 0:
    # Найдём целые числа K и L из описания алгоритма выше:
    K, L = divmod(np.int32(M), P - 1)

    # Введём два новых списка для описания того,
    # как именно массив `A` распределяется по всем
    # процессам. Здесь `rcounts` будет содержать
    # число строк, хранимое каждым процессом в массиве
    # `A_part` (это 0 для процесса 0, K+1 для первых L
    # "рабочих" процессов, и K для оставшихся процессов).
    # Другой список `displs` будет содержать индекс смещений
    # - то есть, номер первой строки, начиная с которой будут
    # храниться `rcounts[m]` строк на процессе `m`.
    # При этом мы предполагаем, что все строки, которые
    # хранятся на каждом процессе, идут подряд.
    rcounts = np.empty(P, dtype=np.int32)
    displs = np.empty(P, dtype=np.int32)

    # Процесс 0 не рабочий, и он не будет содержать
    # никаких строк в `A_part` - хотя саму матрицу (но пустую -
    # ноль строк!) мы будем создавать и на нём:
    rcounts[0] = displs[0] = 0

    # Цикл по всем "рабочим" процессам:
    for m in range(1, P):
        if m <= L:
            rcounts[m] = K + 1
        else:
            rcounts[m] = K
        # Индекс смещений сдвигается каждый раз на число
        # строк, хранимых в процессе:
        displs[m] = displs[m - 1] + rcounts[m - 1]
else:
    # На "рабочих" процессах списки `rcounts` и `displs`
    # используются (в `comm.Scatterv` ниже) в качестве
    # пустых заглушек:
    rcounts = displs = None

# Подготовим "хранилище" для `M_part` на всех процессах:
M_part = np.array(0, dtype=np.int32)

# И разбросаем рассчитанные выше `rcounts` равномерно по всем
# процессам - то есть, каждому процессу m достанется по одному
# элементу из массива `rcounts` и оно запишется в виде одного
# значения `M_part = rcounts[m]` в переменную `M_part` (с разными
# значениями на разных процессах!):
comm.Scatter([rcounts, 1, MPI.INT],
             [M_part, 1, MPI.INT], root=0)

# Альтернативно, если бы нам нужно было разбросать элементы
# `rcounts` по процессам неравномерно, то мы могли бы это сделать,
# используя метод `comm.Scatterv`. При этом нужно было бы создать
# два дополнительных списка с размером, равным числу процессов:
# 1) список, указывающий, сколько элементов нужно отдать
# соответствующему процессу:
#num_elem = np.ones(P, dtype=np.int32)
# 2) список, указывающий смещение - то есть индекс первого элемента,
# который нужно отдать соответствующему процессу:
#disp_elem = np.array(range(P))
#comm.Scatterv([rcounts, num_elem, disp_elem, MPI.INT],
#              [M_part, 1, MPI.INT], root=0)

#------------------------------------------------------------------
# Шаг 3:
# Зачитаем из файла матрицу `A`. В отличие от примера 1.1, теперь
# мы будем экономить память! Зачитаем матрицу `A` не всю сразу,
# а по кусочкам, сразу отдавая каждый кусочек нужному процессу.

# Подготовим "хранилище" для кусочков `A_part` на всех процессах -
# каждое со своим числом строк `M_part`:
A_part = np.empty((M_part, N), dtype=np.float64)
# NOTE: Поскольку на процессе 0 значение `M_part` равно нулю, то
# матрица `A_part` здесь будет пустой и не займёт памяти.

if rank == 0:
    # Зачитаем на процессе 0 файл с матрицей `A` не сразу весь,
    # а по кусочкам - сразу отдавая каждый кусочек своему
    # "рабочему" процессу (на процессе 0 при этом данных 
    # матрицы `A` совсем не останется - экономим память!):
    with open('Example-02-2_AData.dat', 'r') as f2:
        for m in range(1, P):
            # Кусочек матрицы `A`, который мы отдадим процессу `m`:
            A_part_m = np.empty((rcounts[m], N), dtype=np.float64)
            # Зачитали данные:
            for j in range(rcounts[m]):
                for i in range(N):
                    A_part_m[j,i] = np.float64(f2.readline())
            # И сразу же отдали их процессу, причём с блокировкой (!):
            comm.Send([A_part_m, rcounts[m]*N, MPI.DOUBLE], dest=m, tag=0)

            # Может показаться, что более эффективной будет такая же,
            # но неблокирующая пересылка:
            #comm.Isend([A_part_m, rcounts[m]*N, MPI.DOUBLE], dest=m, tag=0)
            # Но её использование здесь приведёт к ошибке! - массив `A_part_m`
            # будет пересоздан в цикле по m сразу же, и данные просто не успеют
            # переслаться процессу `m`.

            # Поможем сборщику мусора поскорее избавиться от больших
            # ненужных данных:
            del A_part_m
else:
    # Каждый "рабочий" процесс получает свою часть строк и записывает 
    # их в свой массив `A_part` - опять с блокировкой:
    comm.Recv([A_part, M_part*N, MPI.DOUBLE], source=0, tag=0, status=None)

#------------------------------------------------------------------
# Шаг 4:
# Наконец, зачитаем из файла вектор `x`. Зачитаем его на процессе 0
# (считаем, что входной файл доступен только на нём) - и потом
# раздадим его по кусочки x_part всем "рабочим" процессам.

# Зачитаем файл x на процессе 0:
if rank == 0:
    x = np.empty(M, dtype=np.float64)
    with open('Example-02-2_xData.dat', 'r') as f3:
        for j in range(M):
            x[j] = np.float64(f3.readline())
else:
    # На "рабочих" процессах вектор `x` используется
    # (в `comm.Scatterv` ниже) в качестве пустой заглушки:
    x = None

# Подготовим "хранилище" для `x_part` на всех процессах:
x_part = np.empty(M_part, dtype=np.float64)

# И разбросаем зачитанный выше вектор `x` по всем процессам
# в виде кусочков `x_part` (с размером `M_part`) этого вектора:
comm.Scatterv([x, rcounts, displs, MPI.DOUBLE], 
              [x_part, M_part, MPI.DOUBLE], root=0)

#------------------------------------------------------------------
# Шаг 5:
# Собственно, и сама нужная нам работа - умножение транспонированной
# матрицы на вектор!

# Делаем частичное умножение на каждом процессе. На каждом "рабочем"
# процессе мы получим полноразмерный, состоящий из `N` элементов,
# вектор `b_temp`:
b_temp = np.dot(A_part.T, x_part)

# NOTE: На процессе 0 результат умножения пустой матрицы `A_part.T`
# на пустой вектор `x_part` даст также полноразмерный, состоящий из
# `N` элементов, вектор `b_temp` - но с нулевыми значениями у всех
# элементов.

# Любопытства ради, напечатаем его на всех процессах:
print(f'b_temp on process {rank} = {b_temp}')

# Подготовим "хранилище" для результирующего вектора `b` на процессе 0:
b = None
if rank == 0:
    b = np.empty(N, dtype=np.float64)

# И соберём вектор `b` на процессе 0 (root), просуммировав между собой 
# все векторы `b_temp`, присланные всеми процессами (включая и нулевой
# вектор от самого процесса 0):
comm.Reduce([b_temp, N, MPI.DOUBLE],
            [b, N, MPI.DOUBLE], op=MPI.SUM, root=0)

#------------------------------------------------------------------
# Шаг 6:
# Окончательно, сохраним посчитанный вектор `b` в файл - как
# и все остальные операции с файлами, делаем это на процессе 0.

if rank == 0:
    # Проверим окончательный результат:
    print(f"\nb = {b}")

    # Сохраним результат вычислений в файл:
    with open('Example-02-2_Results.dat', 'w') as f4:
        for j in range(M):
            f4.write(f'{b[j]}\n')

#------------------------------------------------------------------
