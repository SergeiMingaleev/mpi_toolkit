#------------------------------------------------------------------
# Пример 2.2: Параллельное умножение транспонированной матрицы
#             A.T на вектор x.
#
# Задача: найти вектор b = A.T*x, максимально эффективно параллелизуя
#         работу с использованием MPI интерфейса. При этом требуется
#         переиспользовать матрицу A из примера 1.1 в том виде, в
#         котором она уже была распределена (по строкам) между всеми
#         "рабочими" процессами - чтобы избежать затрат времени и
#         памяти на повторное распределение этой матрицы между
#         процессами.
#
# Считаем, что общее число MPI процессов равно P.
# При этом процесс 0 будет занят исключительно синхронизацией
# данных между процессами, а реальная работа будет выполняться
# процессами от 1 до P-1 (будем называть их "рабочими" процессами).
#
# Размер матрицы A равен (M, N) - M строк и N колонок.
# Эта матрица будет в полном виде зачитана (и храниться в массиве A)
# на процессе 0 (точно как в примере 1.1).
#
# Размер транспонированной матрицы A.T, соответственно, равен (N, M) -
# N строк и M колонок. Считая, что при выполнении примера 1.1 мы
# уже распределили матрицу A между всеми "рабочими" процессами по 
# строкам в форме матрицы A_part (с одинаковым названием, но разным 
# содержимым на каждом процессе) с размером (M_part, N), мы должны 
# будем продолжать работать с A_part внутри каждого процесса.
# При этом транспонированные матрицы A_part.T будут иметь размер
# (N, M_part) - то есть, будут группироваться по колонкам, а не 
# по строкам. Для их умножения на вектор x (c размером M), его 
# теперь нужно будет разбить по разным процессам на кусочки 
# x_part c размером M_part, как мы это делали в примере 2.1.
#
# Считая, что M = (P-1) * K + L, где K и L - это целые числа,
# причём 0 <= L <= P-2, мы можем держать на каждом процессе
# либо по K+1 либо по К строк, для максимальной балансировки памяти
# и вычислений по всем "рабочим" процессам.
# Конкретно, пусть процессы от 1 до L содержат по K+1 строк (если L=0,
# то таких процессов не будет!), а процессы от L+1 до P-1 содержат
# по K строк. Тогда на первой группе процессов размер матрицы A_part
# будет равен (K+1, N), а на второй - (K, N).
#
# Размер вектора x равен M. Как уже говорилось выше, этот вектор 
# будет нужно будет разбить по разным процессам на кусочки 
# x_part c размером M_part.
#
# Каждый "рабочий" процесс посчитает свою часть матрично-векторного
# умножения b_part = A_part.T * x, а потом мы соберём из b_part полный
# вектор b (только на процессе 0), который и будет требуемым нам
# результатом вычислений.
#------------------------------------------------------------------

import numpy as np
from mpi4py import MPI

# Работаем с коммуникатором по всем доступным процессам:
comm = MPI.COMM_WORLD

# Число P доступных процессов в этом коммуникаторе:
P = comm.Get_size()

# Номер текущего процесса (от 0 до P-1):
rank = comm.Get_rank()

#------------------------------------------------------------------
# Шаг 1 (точно такой же, как в примере 1.1!):
# Зачитаем из файла размер (M, N) матрицы A.
# Сделаем это только на процессе 0 - считаем,
# что входной файл доступен только на нём:

if rank == 0:
    with open('Example-02-2_in.dat', 'r') as f1:
        N = np.array(np.int32(f1.readline()))
        M = np.array(np.int32(f1.readline()))
else:
    # На "рабочих" процессах значение `M` не
    # используется - ставим пустую "заглушку":
    M = None
    # и подготавливаем на них "хранилище" для `N`:
    N = np.array(0, dtype=np.int32)

# Раздадим значение `N`, зачитанное процессом 0,
# всем остальным процессам (включая и сам процесс 0):
comm.Bcast(N, root=0)
# Альтернативно, более общая форма записи:
# comm.Bcast([N, 1, MPI.INT], root=0)

# NOTE: Значение `M` не используется на "рабочих" процессах,
#       так что мы его не будем раздавать - лишняя трата времени.

#------------------------------------------------------------------
# Шаг 2 (точно такой же, как в примере 1.1!):
# Разберёмся, сколько строк (и какие именно!) из матрицы `A` мы будем
# хранить в форме частичной матрицы `A_part` на каждом из "рабочих"
# процессов. Сделаем такой анализ только на процессе 0:

if rank == 0:
    # Найдём целые числа K и L из описания алгоритма выше:
    K, L = divmod(np.int32(M), P - 1)

    # Введём два новых списка для описания того,
    # как именно массив `A` распределяется по всем
    # процессам. Здесь `rcounts` будет содержать
    # число строк, хранимое каждым процессом в массиве
    # `A_part` (это 0 для процесса 0, K+1 для первых L
    # "рабочих" процессов, и K для оставшихся процессов).
    # Другой список `displs` будет содержать индекс смещений
    # - то есть, номер первой строки, начиная с которой будут
    # храниться `rcounts[m]` строк на процессе `m`.
    # При этом мы предполагаем, что все строки, которые
    # хранятся на каждом процессе, идут подряд.
    rcounts = np.empty(P, dtype=np.int32)
    displs = np.empty(P, dtype=np.int32)

    # Процесс 0 не рабочий, и он не будет содержать 
    # никаких строк в `A_part` - хотя саму матрицу (но пустую -
    # ноль строк!) мы будем создавать и на нём:
    rcounts[0] = displs[0] = 0

    # Цикл по всем "рабочим" процессам:
    for m in range(1, P):
        if m <= L:
            rcounts[m] = K + 1
        else:
            rcounts[m] = K
        # Индекс смещений сдвигается каждый раз на число
        # строк, хранимых в процессе:
        displs[m] = displs[m - 1] + rcounts[m - 1]
else:
    # На "рабочих" процессах списки `rcounts` и `displs`
    # используются (в `comm.Scatterv` ниже) в качестве
    # пустых заглушек:
    rcounts = displs = None

# Подготовим "хранилище" для `M_part` на всех процессах:
M_part = np.array(0, dtype=np.int32)

# И разбросаем рассчитанные выше `rcounts` по всем процессам
# в виде одного значения `M_part = rcounts[m]` на каждом
# процессе `m`:
comm.Scatter([rcounts, 1, MPI.INT],
             [M_part, 1, MPI.INT], root=0)

#------------------------------------------------------------------
# Шаг 3:
# Зачитаем из файла матрицу `A`. В отличие от примера 1.1, теперь
# мы будем экономить память! Зачитаем не всю сразу матрицу A, 
# а по кусочкам, сразу отдавая каждый кусочек нужному процессу.

# Подготовим "хранилище" для кусочков `A_part` на всех процессах -
# каждое со своим числом строк `M_part`:
A_part = np.empty((M_part, N), dtype=np.float64)
# NOTE: Поскольку на процессе 0 значение M_part равно нулю, то
# матрица `A_part` здесь будет пустой и не займёт памяти.

if rank == 0:
    # Зачитаем на процессе 0 файл с матрицей A не сразу весь,
    # а по кусочкам - сразу отдавая каждый кусочек своему
    # "рабочему" процессу (на процессе 0 при этом данных 
    # матрицы A совсем не останется - экономим память!):
    with open('Example-02-2_AData.dat', 'r') as f2:
        for m in range(1, P):
            # Кусочек матрицы A, который мы отдадим процессу m:
            A_part_m = np.empty((rcounts[m], N), dtype=np.float64)
            # Зачитали данные:
            for j in range(rcounts[m]):
                for i in range(N):
                    A_part_m[j,i] = np.float64(f2.readline())
            # И сразу же отдали их процессу, причём с блокировкой (!):
            comm.Send([A_part_m, rcounts[m]*N, MPI.DOUBLE], dest=m, tag=0)

            # Может показаться, что более эффективной будет такая же,
            # но неблокирующая пересылка:
            #comm.Isend([A_part_m, rcounts[m]*N, MPI.DOUBLE], dest=m, tag=0)
            # Но её использование здесь приведёт к ошибке! - массив `A_part_m`
            # пересоздан в цикле по m сразу же, и данные просто не успеют
            # переслаться процессу m.

            # Поможем сборщику мусора поскорее избавиться от больших
            # ненужных данных:
            del A_part_m
else:
    # Каждый "рабочий" процесс получает свою часть строк и записывает 
    # их в свой массив `A_part` - теперь можно и нужно с блокировкой:
    comm.Recv([A_part, M_part*N, MPI.DOUBLE], source=0, tag=0, status=None)

#------------------------------------------------------------------
# Шаг 4:
# Наконец, зачитаем из файла вектор `x`. Зачитаем его на процессе 0
# (считаем, что входной файл доступен только на нём) - и потом
# раздадим его по кусочки x_part всем "рабочим" процессам.

# Зачитаем файл x на процессе 0:
if rank == 0:
    x = np.empty(M, dtype=np.float64)
    with open('Example-02-2_xData.dat', 'r') as f3:
        for j in range(M):
            x[j] = np.float64(f3.readline())
else:
    # На "рабочих" процессах вектор `x` используется
    # (в `comm.Scatterv` ниже) в качестве пустой заглушки:
    x = None

# Подготовим "хранилище" для `x_part` на всех процессах:
x_part = np.empty(M_part, dtype=np.float64)

# И разбросаем зачитанный выше вектор `x` по всем процессам
# в виде кусочков `x_part` (с размером `M_part`) этого вектора:
comm.Scatterv([x, rcounts, displs, MPI.DOUBLE], 
              [x_part, M_part, MPI.DOUBLE], root=0)

#------------------------------------------------------------------
# Шаг 5:
# Собственно и сама нужная нам работа - умножение транспонированной 
# матрицы на вектор!

# Делаем частичное умножение на каждом процессе. На процессе 0
# результат умножения пустой матрицы `A_part.T` будет пустым вектором
# `b_part`. А на каждом "рабочем" процессе мы получим вектор `b_part`,
# состоящий из `N` элементов:
b_part = np.dot(A_part.T, x_part)

# Подготовим "хранилище" для результирующего вектора `b` на процессе 0:
b = None
if rank == 0:
    b = np.empty(N, dtype=np.float64)

# И соберём вектор `b` на процессе 0 (root), просуммировав между собой 
# все векторы `b_part`, присланными всеми процессами (включая и нулевой
# вектор от самого процесса 0):
comm.Reduce([b_part, N, MPI.DOUBLE], 
            [b, N, MPI.DOUBLE], op=MPI.SUM, root=0)

#------------------------------------------------------------------
# Шаг 6:
# Окончательно, сохраним посчитанный вектор `b` в файл - как
# и все остальные операции с файлами, делаем это на процессе 0.

if rank == 0:
    # Сохраняем результат вычислений в файл:
    with open('Example-02-2_Results.dat', 'w') as f4:
        for j in range(M):
            f4.write(f'{b[j]}\n')

    # Для контроля, напечатаем этот вектор и в консоли:
    print(b)

#------------------------------------------------------------------
