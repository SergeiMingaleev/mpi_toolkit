#------------------------------------------------------------------
# ИЗУЧИТЬ:
# -- Вспоминаем интерфейс метода `comm_world.Create_cart()` и его
#    агрумента `reorder=True` (на хорошем суперкомпьютере может
#    пригодиться для ускорения скорости обмена данными между процессами).
# -- Сравниваем функции `fast_pde_solution()` и `slow_pde_solution()`.
#    Первая функция отличается только использованием "срезов"
#    в массивах библиотеки `numpy` вместо циклов по индексу `n`,
#    что позволяет ускорить вычисления в ~18 раз.
#    Обратите внимание на использование `comm.Sendrecv()` в конце
#    этих функций. Именно они связывают между собой MPI процессы -
#    и делают это гораздо более эффективным способом, чем мы делали
#    в примере 8.1 (сравните!). Впрочем, возможен еще чуть более
#    эффективный способ, который будет проиллюстрирован в примере 8.3.
#------------------------------------------------------------------
#    Пример 8.2: Параллельное решение одномерного (1D)
#                дифференциального уравнения в частных производных
#                (ДУЧП) параболического типа с использованием
#                "явной" схемы метода конечных разностей.
# Реализация №2: Гораздо более эффективно, чем в примере 8.1,
#                обмениваемся между соседними процессами только их
#                пограничными точками, необходимыми для выполнения
#                следующего шага по времени.
#
# Задача: найти решение уравнения:
#            du/dt = eps*(d^2 u/dx^2) + u*(du/dx) + u^3
#         где `eps` - это некоторая константа, а `u(x,t)` - это
#         функция от времени `t` и одной пространственной
#         координаты `x`. Считаем, что эта функция определена
#         на интервале по `x` от `a` до `b` и на интервале
#         по времени от `t0` до `T`.
#
# Для решения такой задачи нам нужно дополнительно знать начальное
# и граничные условия. Будем считать, что в начальный момент времени
# значения этой функции определяются начальным условием:
#    u(x, t0) = u_init(x)
# где функция `u_init(x)` определяется ниже в этом файле.
# Также считаем, что значения функции на границах `x=a` и `x=b`
# определяются граничными условиями:
#    u(a, t) = u_left(t)
#    u(b, t) = u_right(t)
# где функции `u_left(t)` и `u_right(t)` также определяется ниже
# в этом файле.
#
# Для решения уравнения будем использовать метод конечных разностей,
# разбивая диапазон по времени [t0, T] на `M` интервалов и диапазон
# по пространству [a, b] на `N` интервалов.
#
# Для простоты реализации, в этом примере мы будем использовать
# "явную" разностную схему решения уравнения, которая обладает
# первым порядком точности по времени и вторым порядком точности
# по координате:
#    error ~ O(dt, dx^2)
# где `dt = (T-t0) / M` - это шаг по времени `t`,
# а `dx = (b-a) / N` - это шаг по координате `x`.
#
# Такая "явная" схема является условно устойчивой только при выполнении
# жёсткого условия на максимальный шаг по времени:
#    dt < dx^2 / (2*eps)
# что приводит к требованию, что число интервалов по времени `M` должно
# расти как квадрат числа интервалов по пространству `N`: M ~ N^2.
# То есть, при увеличении `N` в 10 раз, нужно увеличить `M` в сто раз!
#
# Более эффективная "неявная" схема решения этого же уравнения будет
# рассмотрена в примерах 9.1 и 9.2 (но она потребует более сложной
# программной реализации, основанной на решении трёхдиагональной
# системы линейных уравнений).
#
#------------------------------------------------------------------
# Этот пример (в его оригинальном виде "Example-8-2.py")
# детально обсуждается в лекции Д.В. Лукьяненко "8. Решение задач
# для уравнений в частных производных. Ч.1" начиная с 62-й минуты:
# https://youtu.be/6SYN28B_iyE?list=PLcsjsqLLSfNCxGJjuYNZRzeDIFQDQ9WvC&t=3761
#
# Объяснения математической части разностных методов для решения
# такого уравнения даётся в лекции Д.В. Лукьяненко
# "Лекция 19. Уравнения в частных производных: продолжение":
# https://teach-in.ru/lecture/2021-07-19-Lukyanenko
# Короткое изложение этих методов можно также найти, например, в:
# https://www.rsatu.ru/upload/medialibrary/ac1/Lektsiya-13.pdf
#------------------------------------------------------------------

import argparse
import numpy as np
from mpi4py import MPI


#------------------------------------------------------------------
def u_init(x):
    """
    Функция, определяющая начальное условие для u(x, t):
               u(x, t0) = u_init(x)
    :param x: Значение пространственной координаты `x`.
              Может быть либо одним числом, либо `numpy`
              массивом, задающим все нужные точки.
    :return: Значение функции для задания `u(x, t0)`.
    """
    res = np.sin(3*np.pi*(x - 1/6))
    return res


#------------------------------------------------------------------
def u_left(t):
    """
    Функция, определяющая граничное условие для u(x, t)
    слева, в точке `x=a`:
               u(a, t) = u_left(t)
    :param t: Значение момента времени `t`.
              Может быть либо одним числом, либо `numpy`
              массивом, задающим все нужные точки.
    :return: Значение функции для задания `u(a, t)`.
    """
    res = -1.0
    return res


#------------------------------------------------------------------
def u_right(t):
    """
    Функция, определяющая граничное условие для u(x, t)
    справа, в точке `x=b`:
               u(b, t) = u_right(t)
    :param t: Значение момента времени `t`.
              Может быть либо одним числом, либо `numpy`
              массивом, задающим все нужные точки.
    :return: Значение функции для задания `u(b, t)`.
    """
    res = 1.0
    return res


#------------------------------------------------------------------
def slow_pde_solution():
    """
    Медленная реализация "явной" разностной схемы - основана
    на использовании медленного цикла по отдельным элементам.
    """
    # Используемые глобальные переменные (определяем их здесь
    # вместо передачи в качестве аргументов функции):
    global comm, P, rank, M, u_part_aux, N_part_aux, \
           eps_dt_dx2, dt_2dx, dt

    # Цикл по всем моментам времени:
    for m in range(M):
        # Обновляем внутренние точки на каждом процессе (медленно, циклом по `n`):
        for n in range(1, N_part_aux-1):
            u_part_aux[m+1, n] = u_part_aux[m, n] + \
                                 eps_dt_dx2 * (u_part_aux[m, n+1] - 2*u_part_aux[m, n] + u_part_aux[m, n-1]) + \
                                 dt_2dx * u_part_aux[m, n] * (u_part_aux[m, n+1] - u_part_aux[m, n-1]) + \
                                 dt * u_part_aux[m, n]**3

        # Обмениваемся полученными решениями на пограничных точках с соседними
        # процессами.
        if rank == 0:
            # Процесс 0 посылает своё решение для своей последней вычисленной
            # точки (n=N_part_aux-2) процессу 1 и получает от него решение для
            # его первой вычисленной точки (n=1), записывая его на место своего
            # самого последнего элемента (n=N_part_aux-1).
            # NOTE: обратите внимание, что в аргументе `recvbuf` мы должны указать
            # не значение последнего элемента `u_part_aux[m+1, N_part_aux-1]`,
            # а срез массива `u_part_aux[m+1, N_part_aux-1:]`, состоящий из одного
            # элемента, чтобы значение, полученное от процесса 1, было записано
            # по нужному адресу массива `u_part_aux`. То же самое мы делаем
            # и во всех остальных вызовах метода `comm.Sendrecv` в коде ниже.
            comm.Sendrecv(sendbuf=[u_part_aux[m+1, N_part_aux-2], 1, MPI.DOUBLE],
                          dest=1, sendtag=0,
                          recvbuf=[u_part_aux[m+1, N_part_aux-1:], 1, MPI.DOUBLE],
                          source=1, recvtag=MPI.ANY_TAG, status=None)
        elif rank == P-1:
            # Последний процесс посылает своё решение для своей первой
            # вычисленной точки (n=1) предпоследнему процессу и получает от него
            # решение для его последней вычисленной точки (n=N_part_aux-2),
            # записывая его на место своего самого первого элемента (n=0).
            comm.Sendrecv(sendbuf=[u_part_aux[m+1, 1], 1, MPI.DOUBLE],
                          dest=P-2, sendtag=0,
                          recvbuf=[u_part_aux[m+1, 0:], 1, MPI.DOUBLE],
                          source=P-2, recvtag=MPI.ANY_TAG, status=None)
        else:  # if rank in range(1, P-1)
            # Промежуточные же процессы посылают своё решение для своих первой
            # (n=1) и последней (n=N_part_aux-2) из вычисленных точек соседним
            # процессам слева (rank-1) и справа (rank+1) от себя, соответственно,
            # и получают решения для первой вычисленной точки от процесса
            # справа (записывая его на место своего самого последнего элемента,
            # n=N_part_aux-1) и для последней вычисленной точки от процесса
            # слева (записывая его на место своего самого первого элемента, n=0).
            comm.Sendrecv(sendbuf=[u_part_aux[m+1, 1], 1, MPI.DOUBLE],
                          dest=rank-1, sendtag=0,
                          recvbuf=[u_part_aux[m+1, 0:], 1, MPI.DOUBLE],
                          source=rank-1, recvtag=MPI.ANY_TAG, status=None)
            comm.Sendrecv(sendbuf=[u_part_aux[m+1, N_part_aux-2], 1, MPI.DOUBLE],
                          dest=rank+1, sendtag=0,
                          recvbuf=[u_part_aux[m+1, N_part_aux-1:], 1, MPI.DOUBLE],
                          source=rank+1, recvtag=MPI.ANY_TAG, status=None)


#------------------------------------------------------------------
def fast_pde_solution():
    """
    Быстрая реализация "явной" разностной схемы - основана
    на использовании "срезов" вместо цикла по отдельным элементам.
    """
    # Используемые глобальные переменные (определяем их здесь
    # вместо передачи в качестве аргументов функции):
    global comm, P, rank, M, u_part_aux, N_part_aux, \
           eps_dt_dx2, dt_2dx, dt

    # Цикл по всем моментам времени:
    for m in range(M):
        # Обновляем внутренние точки на каждом процессе (быстро, используя "срезы"):
        u_part_aux[m+1, 1:-1] = u_part_aux[m, 1:-1] + \
                                eps_dt_dx2 * (u_part_aux[m, 2:] - 2*u_part_aux[m, 1:-1] + u_part_aux[m, :-2]) + \
                                dt_2dx * u_part_aux[m, 1:-1] * (u_part_aux[m, 2:] - u_part_aux[m, :-2]) + \
                                dt * u_part_aux[m, 1:-1]**3

        # Обмениваемся полученными решениями на пограничных точках с соседними
        # процессами.
        if rank == 0:
            # Процесс 0 посылает своё решение для своей последней вычисленной
            # точки (n=N_part_aux-2) процессу 1 и получает от него решение для
            # его первой вычисленной точки (n=1), записывая его на место своего
            # самого последнего элемента (n=N_part_aux-1).
            # NOTE: обратите внимание, что в аргументе `recvbuf` мы должны указать
            # не значение последнего элемента `u_part_aux[m+1, N_part_aux-1]`,
            # а срез массива `u_part_aux[m+1, N_part_aux-1:]`, состоящий из одного
            # элемента, чтобы значение, полученное от процесса 1, было записано
            # по нужному адресу массива `u_part_aux`. То же самое мы делаем
            # и во всех остальных вызовах метода `comm.Sendrecv` в коде ниже.
            comm.Sendrecv(sendbuf=[u_part_aux[m+1, N_part_aux-2], 1, MPI.DOUBLE],
                          dest=1, sendtag=0,
                          recvbuf=[u_part_aux[m+1, N_part_aux-1:], 1, MPI.DOUBLE],
                          source=1, recvtag=MPI.ANY_TAG, status=None)
        elif rank == P-1:
            # Последний процесс посылает своё решение для своей первой
            # вычисленной точки (n=1) предпоследнему процессу и получает от него
            # решение для его последней вычисленной точки (n=N_part_aux-2),
            # записывая его на место своего самого первого элемента (n=0).
            comm.Sendrecv(sendbuf=[u_part_aux[m+1, 1], 1, MPI.DOUBLE],
                          dest=P-2, sendtag=0,
                          recvbuf=[u_part_aux[m+1, 0:], 1, MPI.DOUBLE],
                          source=P-2, recvtag=MPI.ANY_TAG, status=None)
        else:  # if rank in range(1, P-1)
            # Промежуточные же процессы посылают своё решение для своих первой
            # (n=1) и последней (n=N_part_aux-2) из вычисленных точек соседним
            # процессам слева (rank-1) и справа (rank+1) от себя, соответственно,
            # и получают решения для первой вычисленной точки от процесса
            # справа (записывая его на место своего самого последнего элемента,
            # n=N_part_aux-1) и для последней вычисленной точки от процесса
            # слева (записывая его на место своего самого первого элемента, n=0).
            comm.Sendrecv(sendbuf=[u_part_aux[m+1, 1], 1, MPI.DOUBLE],
                          dest=rank-1, sendtag=0,
                          recvbuf=[u_part_aux[m+1, 0:], 1, MPI.DOUBLE],
                          source=rank-1, recvtag=MPI.ANY_TAG, status=None)
            comm.Sendrecv(sendbuf=[u_part_aux[m+1, N_part_aux-2], 1, MPI.DOUBLE],
                          dest=rank+1, sendtag=0,
                          recvbuf=[u_part_aux[m+1, N_part_aux-1:], 1, MPI.DOUBLE],
                          source=rank+1, recvtag=MPI.ANY_TAG, status=None)


# ------------------------------------------------------------------
def auxiliary_arrays_determination(M, P):
    """
    Расчёт списков числа элементов `rcounts` и соответствующих
    смещений `displs`, определяющих распределение элементов вектора
    решения `u[m]` по всем процессам MPI коммуникатора в виде
    частичных векторов `u_part`.
    Наша задача при этом - разбросать вектор `u[m]` по всем
    процессам максимально равномерно и без пересечений.

    :param M: Общее число элементов вдоль нужной оси матрицы.
    :param P: Общее число процессов вдоль нужной оси сетки процессов,
              работающих над параллелизацией вычислений.
    :return: Рассчитанные списки числа элементов `rcounts` и
             соответствующих смещений "displs", определяющие
             передачу данных каждому процессу.
    """
    # Считая, что M = P * K + L, где K и L - это целые числа,
    # причём 0 <= L <= P-1, мы можем держать на каждом процессе
    # либо по K+1 либо по К элементов, для максимальной балансировки
    # памяти и вычислений по всем "рабочим" процессам.
    # Найдём целые числа K и L из описания алгоритма выше:
    K, L = divmod(np.int32(M), P)

    # Введём два новых списка для описания того, как именно
    # матрицы и векторы будут распределяться по всем процессам.
    # Здесь `rcounts` будет содержать число элементов, хранимое
    # каждым процессом (это K+1 для первых L процессов,
    # и K для оставшихся процессов).
    # Другой список `displs` будет содержать индекс смещений
    # - то есть, номер первого элемента, начиная с которой будут
    # храниться `rcounts[m]` элементов на процессе `m`.
    # При этом мы предполагаем, что все элементы, которые
    # хранятся на каждом процессе, идут подряд.
    rcounts = np.empty(P, dtype=np.int32)
    displs = np.empty(P, dtype=np.int32)

    # Цикл по всем процессам (они все "рабочие"):
    for m in range(0, P):
        if m < L:
            # Процессы от 0 до L-1 содержат по K+1 элементов
            # (если L=0, то таких процессов не будет!):
            rcounts[m] = K + 1
        else:
            # Оставшиеся процессы от L до P-1 содержат по K элементов:
            rcounts[m] = K
        # Индекс смещений равен 0 для процесса 0 и сдвигается
        # для каждого следующего процесса на число элементов,
        # хранимых в предыдущем процессе:
        if m == 0:
            displs[m] = 0
        else:
            displs[m] = displs[m - 1] + rcounts[m - 1]
    return rcounts, displs


# ------------------------------------------------------------------
def auxiliary_arrays_from_0(rcounts, displs):
    """
    Расчёт списков числа элементов `rcounts_from_0` и соответствующих
    смещений `displs_from_0`, определяющих распределение элементов
    вектора решения `u[m]` по всем процессам MPI коммуникатора в виде
    вспомогательных частичных векторов `u_part_aux`.
    В качестве основы берём уже рассчитанные списки числа элементов
    `rcounts` и соответствующих смещений `displs`, определяющих
    распределение элементов вектора решения `u[m]` по всем процессам
    MPI коммуникатора в виде частичных векторов `u_part`.
    Наша задача при этом - разбросать вектор `u[m]` по всем процессам
    таким образом, чтобы по сравнению с частичными векторами `u_part`
    каждому процессу достались дополнительно ещё и по одному
    дополнительному элементу слева и/или справа, рассчитанных
    на соседних процессах.

    :param rcounts: Список числа элементов, определяющий распределение
              элементов вектора решения `u[m]` по всем процессам MPI
              коммуникатора в виде частичных векторов `u_part`.
    :param displs: Список смещений, определяющий распределение
              элементов вектора решения `u[m]` по всем процессам MPI
              коммуникатора в виде частичных векторов `u_part`.
    :return:  Рассчитанные списки числа элементов `rcounts_from_0`
              и соответствующих смещений `displs_from_0`, определяющих
              распределение элементов вектора решения `u[m]` по всем
              процессам MPI коммуникатора в виде вспомогательных
              частичных векторов `u_part_aux`.
    """
    # Возьмём в качестве основы копии входных списков:
    rcounts_from_0 = rcounts.copy()
    displs_from_0 = displs.copy()

    # На процессе 0 вектор `u_part_aux` по сравнению с вектором
    # `u_part` содержит один дополнительный элемент справа:
    rcounts_from_0[0] += 1
    # И поскольку этот элемент добавляется справа,
    # смещение остаётся неизменным (нулевым).
    # displs_from_0[0] = 0

    # Цикл по всем промежуточным процессам (если они есть):
    for m in range(1, P-1):
        # На таких промежуточных процессах вектор `u_part_aux`
        # по сравнению с вектором `u_part` содержит два дополнительных
        # элемента (один слева и второй справа):
        rcounts_from_0[m] += 2
        # И поскольку мы добавляем один элемент слева,
        # индекс смещений уменьшается на единицу (то есть,
        # смещается на один элемент влево):
        displs_from_0[m] -= 1

    # На последнем процессе вектор `u_part_aux` по сравнению с вектором
    # `u_part` содержит один дополнительный элемент слева:
    rcounts_from_0[P-1] += 1
    # А индекс смещений, соответственно, уменьшается на единицу
    # (то есть, смещается на один элемент влево):
    displs_from_0[P-1] -= 1

    return rcounts_from_0, displs_from_0


#------------------------------------------------------------------
# Начинаем выполнение программы - первым делом, настроим MPI:
#------------------------------------------------------------------

# Включаем таймер:
start_time1 = MPI.Wtime()

# Работаем с коммуникатором по всем доступным процессам:
comm_world = MPI.COMM_WORLD

# Число P доступных процессов в этом коммуникаторе:
P = comm_world.Get_size()

# Номер текущего процесса (от 0 до P-1):
rank_world = comm_world.Get_rank()

# Создадим одномерный декартовый коммуникатор на основе `comm_world` -
# при работе на реальном суперкомпьютере, аргумент `reorder=True`
# позволит нам оптимизировать нумерацию процессов для обеспечения
# наибольшей скорости коммуникаций между соседними процессами:
comm = comm_world.Create_cart(dims=[P], periods=[False], reorder=True)
rank = comm.Get_rank()

# Проверим, изменился ли номер процесса
# (NOTE: на Windows кластере, работающем под MS MPI, ничего не меняется):
if rank != rank_world:
    print(f'MPI Cartezian Communicator is reordered: rank {rank_world} --> {rank}')

# Реализуемый алгоритм требует хотя бы двух MPI процессов
# (из-за используемых способов выбора списков `rcounts_from_0`
# и `displs_from_0`, и задания граничных условий - основанных
# на явном выделении нулевого и последнего процессов):

if P == 1:
    raise ValueError(
        "\n Необходимы по крайней мере два MPI процесса для работы этой программы!"
        "\n Пример запуска:"
        "\n mpiexec.exe -n 2 python.exe Example-08-2.py [your args]"
    )

#------------------------------------------------------------------
# Шаг 0:
# Распарсим аргументы запуска программы.

parser = argparse.ArgumentParser(
            prog='python Example-08-2.py',
            description='Решение 1D ДУЧП параболического типа с использованием '
                        '"явной" разностной схемы с MPI параллелизацией.',
)
parser.add_argument('-N', default=800,
                    help='Число `N` интервалов сетки по координате `x`. '
                         'По умолчанию равно 800.')
parser.add_argument('-M', default=100_000,
                    help='Число `M` интервалов сетки по времени `t`. '
                         'По умолчанию равно 100_000.')
parser.add_argument('-T', default=2.0,
                    help='Максимальное время `T`, до которого должны проводиться '
                         'расчёты. По умолчанию равно 2.0.')
parser.add_argument('--slow', action="store_true",
                    help='Использовать медленную реализацию решения, '
                         'с циклом по `n` при обновлении `u[m,n]` '
                         'вместо использования срезов `u[m,:]`.')
parser.add_argument('--noheader', action="store_true",
                    help='Не печатать названия колонок в выводе времени счёта.')
parser.add_argument('--save', action="store_true",
                    help='Сохранить результаты расчётов в файл '
                         '"Example-08-2_Results.npz".')
parser.add_argument('--plot', action="store_true",
                    help='Нарисовать решение для последнего момента времени.')

args = parser.parse_args()

#------------------------------------------------------------------
# Шаг 1:
# Задаём значения для всех констант, определяемых условиями задачи.
eps = 10**(-1.5)
a = 0.0; b = 1.0
t_0 = 0.0

T = float(args.T)
N = int(args.N)
M = int(args.M)

#------------------------------------------------------------------
# Шаг 2:
# Находим значения для всех вспомогательных констант.

# Шаг по пространству:
dx = (b - a) / N

# Шаг по времени:
dt = (T - t_0) / M

# Константы для "быстрого" использования внутри разностной схемы:
eps_dt_dx2 = eps * dt / dx ** 2
dt_2dx = dt / (2 * dx)

# Условие устойчивости "явной" разностной схемы `dt < dx^2 / (2*eps)`
# соответсвтует условию `eps_dt_dx2 < 0.5` - проверим его:
if eps_dt_dx2 >= 0.5:
    msg = f'WARNING: Нарушено условие устойчивости "явной" разностной схемы: ' + \
          f'eps_dt_dx2 = {eps_dt_dx2:.3f} (а должно быть меньше 0.5!).'
    print(msg)

# Массивы для хранения точек используемой сетки по пространству:
x = np.linspace(a, b, N+1)
# и по времени:
t = np.linspace(t_0, T, M+1)

#------------------------------------------------------------------
# Шаг 3:
# Задаём списки числа элементов и соответствующих смещений,
# определяющих распределение элементов решения `u[m, n]` по
# частичным матрицам `u_part_aux[m, n]` на всех процессах
# MPI коммуникатора.

# Списки числа элементов и смещений для максимально равномерного
# (без изъятий или пересечений) распределения элементов решения
# `u[m, n]` по всем MPI процессам.
# Всю логику расчёта этих списков мы перенесём в отдельную
# функцию `auxiliary_arrays_determination()`, определённую
# выше в этом файле - смотри комментарии в ней.
if rank == 0:
    # Полные списки нужны только на процессе 0:
    rcounts, displs = auxiliary_arrays_determination(N+1, P)
else:
    # На остальных процессах эти списки используются только
    # в качестве пустых заглушек (в `Scatter` ниже):
    rcounts = displs = None

# Подготовим "хранилище" для `N_part` на всех процессах:
N_part = np.array(0, dtype=np.int32)

# Разбросаем теперь список числа элементов `rcounts`
# по всем процессам в виде значений `N_part`:
comm.Scatter([rcounts, 1, MPI.INT],
             [N_part, 1, MPI.INT], root=0)

# Списки числа элементов и смещений для частичных матриц
# `u_part_aux` - тут мы разбрасываем вектор `u[m]` по всем
# MPI процессам с пересечениями, добавляя по одному (для
# нулевого и последнего процесса) или по два (для всех
# остальных процессов) дополнительных элементов слева и/или
# справа от элементов, включённых в список `rcounts`.
# Всю логику расчёта этих списков мы перенесём в отдельную
# функцию `auxiliary_arrays_from_0()`, определённую
# выше в этом файле - смотри комментарии в ней.
if rank == 0:
    # Полные списки нужны только на процессе 0:
    rcounts_from_0, displs_from_0 = auxiliary_arrays_from_0(rcounts, displs)
else:
    # На остальных процессах эти списки используются только
    # в качестве пустых заглушек (в `Scatter` ниже):
    rcounts_from_0 = displs_from_0 = None

# Подготовим "хранилища" для числа элементов
# `N_part_aux = rcounts_from_0[rank]` и соответствующего
# смещения `displ_aux = displs_from_0[rank]` на текущем
# процессе `rank`:
N_part_aux = np.array(0, dtype=np.int32)
displ_aux = np.array(0, dtype=np.int32)

# Разбросаем теперь списки `rcounts_from_0` и `displs_from_0`
# по всем процессам в виде значений `N_part_aux` и `displ_aux`:
comm.Scatter([rcounts_from_0, 1, MPI.INT],
             [N_part_aux, 1, MPI.INT], root=0)
comm.Scatter([displs_from_0, 1, MPI.INT],
             [displ_aux, 1, MPI.INT], root=0)

#------------------------------------------------------------------
# Шаг 4:
# Создаём массивы для хранения решения и задаём начальное и граничные
# условия.

# Считаем, что решение нашего ДУЧП накапливается по частям на каждом
# из MPI процессов и сохраняется в частичных массивах `u_part_aux[m, n]`.
# Здесь `m=0..M` - это индекс по моментам времени `t`, а `n=0..N_part_aux-1` -
# это индекс по пространственной переменной `x`.
# То есть, на каждом MPI процессе мы храним решение для всех моментов
# времени, но только для тех точек по пространству, которые включаются
# в "расширенную" (с включением дополнительных точек на левой и правой границе)
# область каждого данного процесса. Доступ к элементу `u_part_aux[m, n]`
# возможен также как к `u_part_aux[m][n]` - при этом вектор `u_part_aux[m]`
# соответствует частичному решению ДУЧП на данном процессе в момент времени `m`.

# Сделаем "хранилище" для массива `u_part_aux` на каждом процессе:
u_part_aux = np.empty((M+1, N_part_aux), dtype=np.float64)

# Начальное условие (на всех процессах):
u_part_aux[0, :] = u_init(x[displ_aux:displ_aux+N_part_aux])

# Граничное условие слева (только на нулевом процессе):
if rank == 0:
    u_part_aux[:, 0] = u_left(t)

# Граничное условие справа (только на последнем процессе):
if rank == P-1:
    u_part_aux[:, N_part_aux-1] = u_right(t)

# NOTE: Можно более педантично задавать граничные условия как
# u_part_aux[1:, 0] = u_left(t[1:])
# u_part_aux[1:, N_part_aux-1] = u_right(t[1:])
# Но мы исходим из того, что начальные и граничные условия должны
# быть согласованы, то есть предполагаем, что выполняется условие:
# u_left(t0) == u_init(a) and u_right(t0) == u_init(b)

#------------------------------------------------------------------
# Шаг 5:
# Собственно, и сама нужная нам работа - решение нашего уравнения
# с помощью "явной" разностной схемы. По умолчанию, решаем уравнение
# быстро, но через переключатель '--slow' решаем медленно, как это
# было сделано в оригинальной версии программы. Сравните код обеих
# функций, `slow_pde_solution()` и `fast_pde_solution()`.
# Обратите внимание, что хотя эти функции вызываются без аргументов,
# они переиспользуют многие из введённых выше переменных как глобальные
# переменные (все такие переменные указаны в функциях как `global`).

start_time2 = MPI.Wtime()

if args.slow:
    slow_pde_solution()
else:
    fast_pde_solution()

# Соберём теперь на процессе 0 полученное решение для последнего момента
# времени в виде полного (с размером `N+1`) вектора `u_T`.

# Создадим "хранилище" для этого вектора решения `u_T`:
if rank == 0:
    u_T = np.empty(N+1, dtype=np.float64)
else:
    u_T = None

# И соберём все рассчитанные данные из частичных векторов `u_part_aux[M]`
# на всех процессах в полный вектор `u_T` на процессе 0.
# При этом мы выбрасываем из `u_part_aux[M]` лишние для `u_T` пограничные
# точки, которые дублируются на соседних процессах:
if rank == 0:
    comm.Gatherv([u_part_aux[M, 0:N_part_aux-1], N_part, MPI.DOUBLE],
                 [u_T, rcounts, displs, MPI.DOUBLE], root=0)
elif rank == P-1:
    comm.Gatherv([u_part_aux[M, 1:N_part_aux], N_part, MPI.DOUBLE],
                 [u_T, rcounts, displs, MPI.DOUBLE], root=0)
else:  # rank in range(1, P-1)
    comm.Gatherv([u_part_aux[M, 1:N_part_aux-1], N_part, MPI.DOUBLE],
                 [u_T, rcounts, displs, MPI.DOUBLE], root=0)

end_time = MPI.Wtime()

#------------------------------------------------------------------
# Шаг 6:
# Печатаем время выполнения и, если нужно, сохраняем результаты
# расчётов в файл и/или рисуем график решения.

if rank == 0:
    duration1 = end_time - start_time1
    duration2 = end_time - start_time2

    if not args.noheader:
        print('N\t M\t Procs\t time_tot\t time_sol')
    print(f'{N}\t {M}\t {P}\t {duration1:.6f}\t {duration2:.6f}')

    # Если нужно, сохраняем данные в файл:
    # (NOTE: В отличие от примеров 8.1 и 8.2, здесь мы сохраняем
    # данные только для последнего момента времени, собранные на
    # процессе 0 в конце предыдущего шага программы)
    if args.save:
        filename = 'Example-08-2_Results.npz'
        print(f"Сохраняем данные в файл '{filename}'.")
        np.savez(filename, x=x, t=t, u=u_T)

    # Если нужно, рисуем решение для первого и последнего
    # момента времени:
    if args.plot:
        from matplotlib import pyplot as plt
        plt.style.use('dark_background')
        fig = plt.figure()
        ax = plt.axes(xlim=(a, b), ylim=(-2.0, 2.0))
        ax.set_xlabel('x'); ax.set_ylabel('u')
        ax.plot(x, u_init(x), ls='-', lw=2, label=f'T = {t[0]}')
        ax.plot(x, u_T, ls='-', lw=2, label=f'T = {t[-1]}')
        plt.legend()
        plt.show()

#------------------------------------------------------------------
