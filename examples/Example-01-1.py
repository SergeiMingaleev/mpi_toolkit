#------------------------------------------------------------------
# Пример 1.1: Параллельное умножение матрицы на вектор.
#
# Задача: найти вектор b = dot(A, x), максимально эффективно
#         параллелизуя работу между процессами с использованием
#         MPI интерфейса.
#
# Считаем, что общее число MPI процессов равно P.
# При этом процесс 0 будет занят исключительно синхронизацией
# данных между процессами, а реальная работа будет выполняться
# процессами от 1 до P-1 (будем называть их "рабочими" процессами).
#
# Размер матрицы A равен (M, N) - M строк и N колонок.
# Эта матрица будет в полном виде зачитана (и храниться в массиве A)
# на процессе 0.
# Мы также распределим эту матрицу между всеми "рабочими" процессами
# по строкам в форме массива A_part (с одинаковым названием, но разным
# содержимым на каждом процессе).
# Считая, что M = (P-1) * K + L, где K и L - это целые числа,
# причём 0 <= L <= P-2, мы можем держать на каждом процессе
# либо по K+1 либо по К строк, для максимальной балансировки памяти
# и вычислений по всем "рабочим" процессам.
# Конкретно, пусть процессы от 1 до L содержат по K+1 строк (если L=0,
# то таких процессов не будет!), а процессы от L+1 до P-1 содержат
# по K строк. Тогда на первой группе процессов размер матрицы A_part
# будет равен (K+1, N), а на второй - (K, N).
#
# Размер вектора x равен N. Этот вектор будет в полном виде зачитан
# (и храниться в массиве x) на процессе 0.
# Мы также будем держать полную копию этого вектора на всех "рабочих"
# процессах.
#
# Каждый "рабочий" процесс посчитает свою часть матрично-векторного
# умножения b_part = dot(A_part, x), а потом мы соберём из b_part
# полный вектор b (только на процессе 0), который и будет требуемым
# нам результатом вычислений.
#
#------------------------------------------------------------------
# Этот пример (в его оригинальном виде) детально обсуждается
# в лекции Д.В. Лукьяненко "1. Введение в основы MPI на Python":
# https://youtu.be/OADFY0i3lrg?list=PLcsjsqLLSfNCxGJjuYNZRzeDIFQDQ9WvC
#------------------------------------------------------------------

import numpy as np
from mpi4py import MPI

# Работаем с коммуникатором по всем доступным процессам:
comm = MPI.COMM_WORLD

# Число P доступных процессов в этом коммуникаторе:
P = comm.Get_size()

# Номер текущего процесса (от 0 до P-1):
rank = comm.Get_rank()

#------------------------------------------------------------------
# Шаг 0:
# Реализуемый алгоритм требует для работы хотя бы два MPI процесса -
# один будет командовать, а второй работать. Если процесс всего один,
# то работать просто некому!

if P == 1:
    raise ValueError(
        "\nAt least 2 MPI processes are needed to run this program!\n"
        "Please launch it with the command:\n"
        "mpiexec.exe -n * python.exe Example-01-1.py"
    )

#------------------------------------------------------------------
# Шаг 1:
# Зачитаем из файла размер (M, N) матрицы A.
# Сделаем это только на процессе 0 - считаем,
# что входной файл доступен только на нём:

if rank == 0:
    with open('Example-01-1_in.dat', 'r') as f1:
        M = np.array(np.int32(f1.readline()))
        N = np.array(np.int32(f1.readline()))
else:
    # На "рабочих" процессах значение `M` не
    # используется - ставим пустую "заглушку":
    M = None
    # и подготавливаем на них "хранилище" для `N`:
    N = np.array(0, dtype=np.int32)

# Раздадим значение `N`, зачитанное процессом 0,
# всем остальным процессам:
comm.Bcast(N, root=0)
# Альтернативно, более общая форма записи, явно указывающая,
# что мы хотим раздать массив N с размером 1 и типом данных
# MPI.INT от процесса 0 всем остальным процессам:
# comm.Bcast([N, 1, MPI.INT], root=0)

# NOTE: Значение `M` не используется на "рабочих" процессах,
#       так что мы его не будем раздавать - лишняя трата времени.

#------------------------------------------------------------------
# Шаг 2:
# Разберёмся, сколько строк (и какие именно!) из матрицы `A` мы будем
# хранить в форме частичной матрицы `A_part` на каждом из "рабочих"
# процессов. Сделаем такой анализ только на процессе 0:

if rank == 0:
    # Найдём целые числа K и L из описания алгоритма выше:
    K, L = divmod(np.int32(M), P - 1)

    # Введём два новых списка для описания того,
    # как именно массив `A` распределяется по всем
    # процессам. Здесь `rcounts` будет содержать
    # число строк, хранимое каждым процессом в массиве
    # `A_part` (это 0 для процесса 0, K+1 для первых L
    # "рабочих" процессов, и K для оставшихся процессов).
    # Другой список `displs` будет содержать индекс смещений
    # - то есть, номер первой строки, начиная с которой будут
    # храниться `rcounts[m]` строк на процессе `m`.
    # При этом мы предполагаем, что все строки, которые
    # хранятся на каждом процессе, идут подряд.
    rcounts = np.empty(P, dtype=np.int32)
    displs = np.empty(P, dtype=np.int32)

    # Процесс 0 не рабочий, и он не будет содержать 
    # никаких строк в `A_part` - хотя саму матрицу (но пустую -
    # ноль строк!) мы будем создавать и на нём:
    rcounts[0] = displs[0] = 0

    # Цикл по всем "рабочим" процессам:
    for m in range(1, P):
        if m <= L:
            rcounts[m] = K + 1
        else:
            rcounts[m] = K
        # Индекс смещений сдвигается каждый раз на число
        # строк, хранимых в процессе:
        displs[m] = displs[m - 1] + rcounts[m - 1]
else:
    # На "рабочих" процессах списки `rcounts` и `displs`
    # используются (в `comm.Scatterv` ниже) в качестве
    # пустых заглушек:
    rcounts = displs = None

# Подготовим "хранилище" для `M_part` на всех процессах:
M_part = np.array(0, dtype=np.int32)

# И разбросаем рассчитанные выше `rcounts` равномерно по всем
# процессам - то есть, каждому процессу m достанется по одному
# элементу из массива `rcounts` и оно запишется в виде одного
# значения `M_part = rcounts[m]` в переменную `M_part` (с разными
# значениями на разных процессах!):
comm.Scatter([rcounts, 1, MPI.INT],
             [M_part, 1, MPI.INT], root=0)

# Альтернативно, если бы нам нужно было разбросать элементы
# `rcounts` по процессам неравномерно, то мы могли бы это сделать,
# используя метод `comm.Scatterv`. При этом нужно было бы создать
# два дополнительных списка с размером, равным числу процессов:
# 1) список, указывающий, сколько элементов нужно отдать
# соответствующему процессу:
#num_elem = np.ones(P, dtype=np.int32)
# 2) список, указывающий смещение - то есть индекс первого элемента,
# который нужно отдать соответствующему процессу:
#disp_elem = np.array(range(P))
#comm.Scatterv([rcounts, num_elem, disp_elem, MPI.INT],
#              [M_part, 1, MPI.INT], root=0)

#------------------------------------------------------------------
# Шаг 3:
# Зачитаем из файла матрицу A. Для простоты, зачитаем
# её полностью на процессе 0 (считаем, что входной файл доступен
# только на нём, и памяти на процессе 0 хватает сразу на всю
# матрицу) - потом разделим на строки (так, как было решено на шаге 2)
# и раздадим каждому процессу свою часть строк в виде частичного массива
# `A_part` (0 строк процессу 0 и по M_part = K+1 или K строк всем
# "рабочим" процессам).

# Подготовим "хранилище" для `A_part` на всех процессах -
# каждое со своим числом строк `M_part`:
A_part = np.empty((M_part, N), dtype=np.float64)
# NOTE: Поскольку на процессе 0 значение M_part равно нулю, то
# матрица `A_part` здесь будет пустой и не займёт памяти.

if rank == 0:
    # Зачитаем весь файл сразу на процессе 0:
    with open('Example-01-1_AData.dat', 'r') as f2:
        A = np.empty((M,N), dtype=np.float64)
        for j in range(M):
            for i in range(N):
                A[j,i] = np.float64(f2.readline())

    # Теперь разобъём его на строки и раздадим каждому "рабочему"
    # процессу свою часть строк, записав их в массивы с одинаковым
    # на всех процессах именем `A_part`.
    # Поскольку нам нужно разбросать строки матрицы `A` по процессам 
    # неравномерно, мы должны это делать, используя "векторный" метод 
    # `comm.Scatterv`. При этом нам нужно создать два дополнительных 
    # списка с размером, равным числу процессов:
    # 1) список, указывающий, сколько всего элементов нужно отдать
    # соответствующему процессу. Поскольку мы хотим передавать
    # строки, состоящие из `N` элементов типа `MPI.DOUBLE` каждая, 
    # то мы умножим на `N` число строк `rcounts[m]`, передаваемых 
    # процессу `m`:
    num_elem = rcounts*N
    # 2) список, указывающий смещение - то есть индекс первого элемента,
    # который нужно отдать соответствующему процессу. Опять же, из-за 
    # того, что каждая строка состоит из `N` элементов типа `MPI.DOUBLE`, 
    # мы должны умножить список смещений строк `displs` на `N`, чтобы 
    # получить список смещений по всем элементам матрицы:
    disp_elem = displs*N
    # Наконец, для буфера получателя указываем `A_part` как переменную, 
    # куда посланная часть матрицы `A` будет записана, и количество 
    # элементов `M_part*N` в этой части:
    comm.Scatterv([A, num_elem, disp_elem, MPI.DOUBLE],
                  [A_part, M_part*N, MPI.DOUBLE], root=0)
    # NOTE: процесс 0 тоже получает свой набор данных из 0 строк!
else:
    # Каждый "рабочий" процесс ничего не посылает, а просто получает
    # свою часть строк и записывает их в свой массив `A_part`:
    comm.Scatterv([None, None, None, None], 
                  [A_part, M_part*N, MPI.DOUBLE], root=0)    

#------------------------------------------------------------------
# Шаг 4:
# Наконец, зачитаем из файла вектор `x`. Зачитаем его на процессе 0
# (считаем, что входной файл доступен только на нём) - и потом
# раздадим его полную копию каждому процессу.

# Подготовим "хранилище" для `x` на всех процессах - везде
# с одинаковым размером `N`:
x = np.empty(N, dtype=np.float64)

if rank == 0:
    # Зачитаем файл на процессе 0:
    with open('Example-01-1_xData.dat', 'r') as f3:
        for i in range(N):
            x[i] = np.float64(f3.readline())

# Раздадим значение `x` (все его N элементов!), зачитанное
# процессом 0, всем остальным процессам:
comm.Bcast([x, N, MPI.DOUBLE], root=0)

#------------------------------------------------------------------
# Шаг 5:
# Собственно, и сама нужная нам работа - умножение матрицы на вектор!

# Делаем частичное умножение на каждом процессе. На процессе 0
# результат умножения пустой матрицы `A_part` будет пустым вектором
# `b_part`. А на каждом "рабочем" процессе мы получим вектор `b_part`,
# состоящий из `M_part` элементов:
b_part = np.dot(A_part, x)

# Любопытства ради, напечатаем его на всех процессах:
print(f'b_part = {b_part} on process {rank}')

# Подготовим "хранилище" для результирующего вектора `b` на процессе 0:
b = None
if rank == 0:
    b = np.empty(M, dtype=np.float64)

# И соберём вектор `b` на процессе 0 (root) из кусочков `b_part`,
# присланными всеми процессами (включая и пустой кусочек от самого
# процесса 0):
comm.Gatherv([b_part, M_part, MPI.DOUBLE],
             [b, rcounts, displs, MPI.DOUBLE], root=0)

#------------------------------------------------------------------
# Шаг 6:
# Окончательно, сохраним посчитанный вектор `b` в файл - как
# и все остальные операции с файлами, делаем это на процессе 0.

if rank == 0:
    # Проверим окончательный результат:
    print(f"\nb = {b}")

    # Сохраним результат вычислений в файл:
    with open('Example-01-1_Results.dat', 'w') as f4:
        for j in range(M):
            f4.write(f'{b[j]}\n')

#------------------------------------------------------------------
