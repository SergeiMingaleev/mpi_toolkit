#------------------------------------------------------------------
# Пример 1.1: Параллельное умножение матрицы A на вектор x.
#
# Задача: найти вектор b = A*x, максимально эффективно параллелизуя
#         работу с использованием MPI интерфейса.
#
# Считаем, что общее число MPI процессов равно P.
# При этом процесс 0 будет занят исключительно синхронизацией
# данных между процессами, а реальная работа будет выполняться
# процессами от 1 до P-1 (будем называть их "рабочими" процессами).
#
# Размер матрицы A равен (M, N) - M строк и N колонок.
# Эта матрица будет в полном виде зачитана (и храниться в массиве A)
# на процессе 0.
# Мы также распределим эту матрицу между всеми "рабочими" процессами
# по строкам в форме массива A_part (с одинаковым названием, но разным
# содержимым на каждом процессе).
# Считая, что M = (P-1) * K + L, где K и L - это целые числа,
# причём 0 <= L <= P-2, мы можем держать на каждом процессе
# либо по K+1 либо по К строк, для максимальной балансировки памяти
# и вычислений по всем "рабочим" процессам.
# Конкретно, пусть процессы от 1 до L содержат по K+1 строк (если L=0,
# то таких процессов не будет!), а процессы от L+1 до P-1 содержат
# по K строк. Тогда на первой группе процессов размер матрицы A_part
# будет равен (K+1, N), а на второй - (K, N).
#
# Размер вектора x равен N. Этот вектор будет в полном виде зачитан
# (и храниться в массиве x) на процессе 0.
# Мы также будем держать полную копию этого вектора на всех "рабочих"
# процессах.
#
# Каждый "рабочий" процесс посчитает свою часть матрично-векторного
# умножения b_part = A_part * x, а потом мы соберём из b_part полный
# вектор b (только на процессе 0), который и будет требуемым нам
# результатом вычислений.
#------------------------------------------------------------------

import numpy as np
from mpi4py import MPI

# Работаем с коммуникатором по всем доступным процессам:
comm = MPI.COMM_WORLD

# Число P доступных процессов в этом коммуникаторе:
P = comm.Get_size()

# Номер текущего процесса (от 0 до P-1):
rank = comm.Get_rank()

#------------------------------------------------------------------
# Шаг 1:
# Зачитаем из файла размер (M, N) матрицы A.
# Сделаем это только на процессе 0 - считаем,
# что входной файл доступен только на нём:

if rank == 0:
    with open('Example-01-1_in.dat', 'r') as f1:
        M = np.array(np.int32(f1.readline()))
        N = np.array(np.int32(f1.readline()))
else:
    # На "рабочих" процессах значение `M` не
    # используется - ставим пустую "заглушку":
    M = None
    # и подготавливаем на них "хранилище" для `N`:
    N = np.array(0, dtype=np.int32)

# Раздадим значение `N`, зачитанное процессом 0,
# всем остальным процессам (включая и сам процесс 0):
comm.Bcast(N, root=0)
# Альтернативно, более общая форма записи:
# comm.Bcast([N, 1, MPI.INT], root=0)

# NOTE: Значение `M` не используется на "рабочих" процессах,
#       так что мы его не будем раздавать - лишняя трата времени.

#------------------------------------------------------------------
# Шаг 2:
# Разберёмся, сколько строк (и какие именно!) из матрицы `A` мы будем
# хранить в форме частичной матрицы `A_part` на каждом из "рабочих"
# процессов. Сделаем такой анализ только на процессе 0:

if rank == 0:
    # Найдём целые числа K и L из описания алгоритма выше:
    K, L = divmod(np.int32(M), P - 1)

    # Введём два новых списка для описания того,
    # как именно массив `A` распределяется по всем
    # процессам. Здесь `rcounts` будет содержать
    # число строк, хранимое каждым процессом в массиве
    # `A_part` (это 0 для процесса 0, K+1 для первых L
    # "рабочих" процессов, и K для оставшихся процессов).
    # Другой список `displs` будет содержать индекс смещений
    # - то есть, номер первой строки, начиная с которой будут
    # храниться `rcounts[m]` строк на процессе `m`.
    # При этом мы предполагаем, что все строки, которые
    # хранятся на каждом процессе, идут подряд.
    rcounts = np.empty(P, dtype=np.int32)
    displs = np.empty(P, dtype=np.int32)

    # Процесс 0 не рабочий, и он не будет содержать 
    # никаких строк в `A_part` - хотя саму матрицу (но пустую -
    # ноль строк!) мы будем создавать и на нём:
    rcounts[0] = displs[0] = 0

    # Цикл по всем "рабочим" процессам:
    for m in range(1, P):
        if m <= L:
            rcounts[m] = K + 1
        else:
            rcounts[m] = K
        # Индекс смещений сдвигается каждый раз на число
        # строк, хранимых в процессе:
        displs[m] = displs[m - 1] + rcounts[m - 1]
else:
    # На "рабочих" процессах список смещений `displs`
    # не используется и значит не нужен - а из списка
    # `rcounts` каждый процесс использует только одно
    # число - ниже мы его назовём `M_part`, так что
    # сам список `rcounts` тоже не понадобится:
    rcounts = displs = None

# Подготовим "хранилище" для `M_part` на всех процессах:
M_part = np.array(0, dtype=np.int32)

# И разбросаем рассчитанные выше `rcounts` по всем процессам
# в виде одного значения `M_part = rcounts[m]` на каждом
# процессе `m`:
comm.Scatterv([rcounts, np.ones(P, dtype=np.int32),
               np.array(range(P)), MPI.INT],
              [M_part, 1, MPI.INT], root=0)
# Альтернативно:
# comm.Scatter([rcounts, 1, MPI.INT], [M_part, 1, MPI.INT], root=0)

#------------------------------------------------------------------
# Шаг 3:
# Зачитаем из файла матрицу A. Для простоты, зачитаем
# её полностью на процессе 0 (считаем, что входной файл доступен
# только на нём, и памяти на процессе 0 хватает сразу на всю
# матрицу) - потом разделим на строки (так, как было решено на шаге 2)
# и раздадим каждому процессу свою часть строк в виде частичного массива
# `A_part` (0 строк процессу 0 и по M_part = K+1 или K строк всем
# "рабочим" процессам).

# Подготовим "хранилище" для `A_part` на всех процессах -
# каждое со своим числом строк `M_part`:
A_part = np.empty((M_part, N), dtype=np.float64)

if rank == 0:
    # Зачитаем весь файл сразу на процессе 0:
    with open('Example-01-1_AData.dat', 'r') as f2:
        A = np.empty((M,N), dtype=np.float64)
        for j in range(M):
            for i in range(N):
                A[j,i] = np.float64(f2.readline())

    # Теперь разобъём его на строки и раздадим каждому "рабочему"
    # процессу свою часть строк, записав их в массивы с одинаковым
    # на всех процессах именем `A_part`:
    comm.Scatterv([A, rcounts*N, displs*N, MPI.DOUBLE],
                  [A_part, M_part*N, MPI.DOUBLE], root=0)
    # NOTE: процесс 0 тоже получает свой набор данных из 0 строк!
else:
    # Каждый "рабочий" процесс ничего не посылает, а просто получает
    # свою часть строк и записывает их в свой массив `A_part`:
    comm.Scatterv([None, None, None, None], 
                  [A_part, M_part*N, MPI.DOUBLE], root=0)    

#------------------------------------------------------------------
# Шаг 4:
# Наконец, зачитаем из файла вектор `x`. Зачитаем его на процессе 0
# (считаем, что входной файл доступен только на нём) - и потом
# раздадим его полную копию каждому процессу.

# Подготовим "хранилище" для `x` на всех процессах - везде
# с одинакомым размером `N`:
x = np.empty(N, dtype=np.float64)

if rank == 0:
    # Зачитаем файл на процессе 0:
    with open('Example-01-1_xData.dat', 'r') as f3:
        for i in range(N):
            x[i] = np.float64(f3.readline())

# Раздадим значение `x`, зачитанное процессом 0,
# всем остальным процессам (включая и сам процесс 0):
comm.Bcast([x, N, MPI.DOUBLE], root=0)

#------------------------------------------------------------------
# Шаг 5:
# Собственно и сама нужная нам работа - умножение матрицы на вектор!

# Делаем частичное умножение на каждом процессе. На процессе 0
# результат умножения пустой матрицы `A_part` будет пустым вектором
# `b_part`. А на каждом "рабочем" процессе мы получим вектор `b_part`,
# состоящий из `M_part` элементов:
b_part = np.dot(A_part, x)

# Подготовим "хранилище" для результирующего вектора `b` на процессе 0:
b = None
if rank == 0:
    b = np.empty(M, dtype=np.float64)

# И соберём вектор `b` на процессе 0 (root) из кусочков `b_part`, присланными
# всеми процессами (включая и пустой кусочек от самого процесса 0):
comm.Gatherv([b_part, M_part, MPI.DOUBLE],
             [b, rcounts, displs, MPI.DOUBLE], root=0)

#------------------------------------------------------------------
# Шаг 6:
# Окончательно, сохраним посчитанный вектор `b` в файл - как
# и все остальные операции с файлами, делаем это на процессе 0.

if rank == 0:
    # Сохраняем результат вычислений в файл:
    with open('Example-01-1_Results.dat', 'w') as f4:
        for j in range(M):
            f4.write(f'{b[j]}\n')

    # Для контроля, напечатаем этот вектор и в консоли:
    print(b)

#------------------------------------------------------------------
