#------------------------------------------------------------------
# Пример 6.1: Параллельное решение системы линейных уравнений
#             методом сопряжённых градиентов.
# Реализация №4: Пытаемся распараллелить все вычисления, используя
#                более эффективный с точки зрения межпроцессорных
#                коммуникаций блочный алгоритм умножения матрицы
#                на вектор и нахождения скалярного произведения
#                векторов. При этом скалярные произведения векторов
#                будут, с одной стороны, распараллелены (а именно,
#                будут выполняться параллельно на всех процессах
#                одной строки нашей сетки процессов), но с другой
#                стороны, одни и те же вычисления будут повторяться
#                на каждой такой строке процессов (в отличие от
#                примера 5.1, где работала только первая строка
#                процессов - а остальные строки просто ждали
#                получения результатов её работы).
#
#                С точки зрения эффективности распараллеливания,
#                данный алгоритм должен быть более-менее эквивалентен
#                алгоритму из примера 5.1 - но при этом он подготовит
#                нас к предстоящим более существенным изменениям
#                алгоритма работы в примере 6.2.
#
# Задача: найти вектор x, являющийся решением системы линейных
#         уравнений A*x = b, максимально эффективно параллелизуя
#         работу между процессами с использованием MPI интерфейса.
#
# Решение системы линейных уравнений A*x = b методом сопряжённых
# градиентов требует выполнения N итераций, где N - это размер
# вектора x. На каждой итерации мы будем обновлять значение
# вектора x в виде последовательности приближённых решений x_{s}
# на итерациях s от 1 до N+1.
#
# Чтобы начать итерации, нам нужно взять какое-то начальное
# приближение для x_{1} на первой итерации - но в целом, оно может
# быть любым, и обычно начинают с нулевого значения.
#
# Нам также понадобятся три вспомогательных вектора r, p, и q,
# значения которых также будут обновляться на каждой итерации s
# в виде последовательностей r_{s}, p_{s}, и q_{s}.
# Считаем при этом, что вектор p инициализирован нулём до начала
# итераций (то есть, для s=0).
#
# Тогда решение системы линейных уравнений может быть найдено
# методом сопряжённых градиентов по такому алгоритму:
#
#    x_{1} = 0 # или ваше приближение для решения!
#    p_{0} = 0
#    for s in range(1, N+1):
#        if s == 1:
#            r_{s} = dot(A.T, dot(A, x_{s}) - b)
#        else:
#            dot_pq = dot(p_{s-1}, q_{s-1})
#            r_{s} = r_{s-1} - q_{s-1} / dot_pq
#        dot_rr = dot(r_{s}, r_{s})
#        p_{s} = p_{s-1} + r_{s} / dot_rr
#        q_{s} = dot(A.T, dot(A, p_{s}))
#        dot_pq = dot(p_{s}, q_{s})
#        x_{s+1} = x_{s} - p_{s} / dot_pq
#
# После выполнения этого алгоритма, мы получим приближённое
# решение x_{N+1}, которое для случая квадратной матрицы A будет
# в действительности точным решением (с точностью до ошибок
# численного округления при выполнении арифметических операций).
#
# При параллелизации этого алгоритма мы будем считать, что общее
# число MPI процессов равно P.
# При этом процесс 0 будет занят одновременно и синхронизацией
# данных между процессами, и реальной работой - то есть, все P
# процессов будут "рабочими".
#
# Размер матрицы A равен (M, N) - M строк и N столбцов.
# Мы распределим эту матрицу между всеми процессами по блокам,
# считая, что у нас есть num_row строк из таких блоков матрицы
# и num_col столбцов из таких блоков матрицы.
# Блочная структура матрицы будет напрямую отображаться на
# прямоугольную сеть MPI процессов. То есть, считаем, что
# что используемое число процессов P равно num_row*num_col,
# и каждый блок матрицы хранится на одном процессе в виде
# массива A_part.
#
#          Processes     Blocks of matrix A
#          0  1  2       (0,0) (0,1) (0,2)
#          3  4  5  ==>  (1,0) (1,1) (1,2)
#          6  7  8       (2,0) (2,1) (2,2)
#          9 10 11       (3,0) (3,1) (3,2)
#
# Строки и столбцы нашей прямоугольной сетки MPI процессов
# напрямую соответствуют строкам и столбцам блоков матрицы.
# При этом M строк элементов матрицы нужно будет максимально
# равномерно распределить по num_row строк блоков матрицы,
# а N столбцов элементов матрицы нужно будет максимально
# равномерно распределить по num_col столбцов блоков матрицы.

# Вектор x с размером N элементов будет разбит на num_col
# кусочков x_part, которые будут храниться на процессах
# первой строки нашей сетки процессов и будут в точности
# повторяться на всех остальных строках сетки процессов.

# Вектор b с размером M элементов будет разбит на num_row
# кусочков b_part, которые будут храниться на процессах
# первого столбца нашей сетки процессов и будут в точности
# повторяться на всех остальных столбцах сетки процессов.

# Реализация самого метода сопряжённых градиентов вынесена в функцию
# `conjugate_gradient_method()` - остальная же часть кода занимается
# подготовкой и пересылкой данных между MPI процессами.
#
#------------------------------------------------------------------
# Этот пример (в его оригинальном виде) детально обсуждается
# в лекции Д.В. Лукьяненко "6. Виртуальные топологии"
# с 39 по 61 минуту:
# https://youtu.be/M3zpgT3Ji18?list=PLcsjsqLLSfNCxGJjuYNZRzeDIFQDQ9WvC
#------------------------------------------------------------------

from mpi4py import MPI
import numpy as np


#------------------------------------------------------------------
def auxiliary_arrays_determination(M, P):
    """
    Расчёт списков числа элементов `rcounts` и соответствующих
    смещений "displs", определяющих распределение больших матриц
    и векторов по всем процессам MPI коммуникатора, включая
    и процесс 0.

    :param M: Общее число элементов вдоль нужной оси матрицы.
    :param P: Общее число процессов вдоль нужной оси сетки процессов,
              работающих над параллелизацией вычислений.
    :return: Рассчитанные списки числа элементов `rcounts` и
             соответствующих смещений "displs", определяющие
             передачу данных каждому процессу.
    """
    # Считая, что M = P * K + L, где K и L - это целые числа,
    # причём 0 <= L <= P-1, мы можем держать на каждом процессе
    # либо по K+1 либо по К строк, для максимальной балансировки
    # памяти и вычислений по всем "рабочим" процессам.
    # Найдём целые числа K и L из описания алгоритма выше:
    K, L = divmod(np.int32(M), P)

    # Введём два новых списка для описания того, как именно
    # матрицы и векторы будут распределяться по всем процессам.
    # Здесь `rcounts` будет содержать число элементов, хранимое
    # каждым процессом (это K+1 для первых L процессов,
    # и K для оставшихся процессов).
    # Другой список `displs` будет содержать индекс смещений
    # - то есть, номер первой строки, начиная с которой будут
    # храниться `rcounts[m]` строк на процессе `m`.
    # При этом мы предполагаем, что все элементы, которые
    # хранятся на каждом процессе, идут подряд.
    rcounts = np.empty(P, dtype=np.int32)
    displs = np.empty(P, dtype=np.int32)

    # Цикл по всем процессам (они все "рабочие"):
    for m in range(0, P):
        if m < L:
            # Процессы от 0 до L-1 содержат по K+1 строк матрицы
            # (если L=0, то таких процессов не будет!):
            rcounts[m] = K + 1
        else:
            # Оставшиеся процессы от L до P-1 содержат по K строк:
            rcounts[m] = K
        # Индекс смещений равен 0 для процесса 0 и сдвигается
        # для каждого следующего процесса на число строк,
        # хранимых в предыдущем процессе:
        if m == 0:
            displs[m] = 0
        else:
            displs[m] = displs[m - 1] + rcounts[m - 1]
    return rcounts, displs


#------------------------------------------------------------------
def conjugate_gradient_method(A_part, b_part, x_part,
                              N_part, M_part, N,
                              comm_row, comm_col):
    """
    Параллельное решение системы линейных уравнений A*x = b
    методом сопряжённых градиентов.

    Реализация №4: Пытаемся распараллелить все вычисления, используя
                   более эффективный с точки зрения межпроцессорных
                   коммуникаций блочный алгоритм умножения матрицы
                   на вектор и нахождения скалярного произведения
                   векторов. При этом скалярные произведения векторов
                   будут, с одной стороны, распараллелены (а именно,
                   будут выполняться параллельно на всех процессах
                   одной строки нашей сетки процессов), но с другой
                   стороны, одни и те же вычисления будут повторяться
                   на каждой такой строке процессов (в отличие от
                   примера 5.1, где работала только первая строка
                   процессов - а остальные строки просто ждали
                   получения результатов её работы).

    Считаем, что на каждом процессе хранится только часть `A_part`
    матрицы `A`, разбитая между процессами по блокам.
    Также на каждом процессе хранится свой кусочек `b_part` вектора `b`
    (в точности повторяясь в каждом столбце сетки процессов) и свой
    кусочек `x_part` вектора `x` (в точности повторяясь в каждой
    строке сетки процессов).

    :param A_part: Часть матрицы `A`, хранимая на текущем процессе.
                   Предполагается, что матрица `A` была разбита
                   между процессами по блокам.
    :param b_part: Часть вектора `b`, хранимая на текущем процессе.
    :param x_part: Часть вектора `x`, хранимая на текущем процессе.
    :param N_part: Размер части `x_part` вектора `x` и ему подобных,
                   используемых на текущем процессе.
    :param M_part: Размер части `b_part` вектора `b` и ему подобных,
                   используемых на текущем процессе.
    :param N:      Требуемое число итераций.
    :param comm_row:  Коммуникатор для связи данного процесса с другими
                      процессами из своей строки сетки процессов.
    :param comm_col:  Коммуникатор для связи данного процесса с другими
                      процессами из своего столбца сетки процессов.

    :return: Часть `x_part` приближённого (или точного, для квадратной матрицы)
             решения. Окончательное решение в виде полного вектора `x` должно
             быть собрано за пределами этой функции.
    """
    # На момент вызова этой функции на каждом процессе всей сетки процессов
    # уже есть свой кусочек `A_part` матрицы `A`. Также на каждом процессе
    # хранится свой кусочек `b_part` вектора `b` (в точности повторяясь
    # в каждом столбце сетки процессов) и свой кусочек `x_part` вектора `x`
    # (в точности повторяясь в каждой строке сетки процессов).

    # Каждый процесс будет хранить свою часть вспомогательных векторов
    # `r` и `q` в массивах `r_part` и `q_part`:
    r_part = np.empty(N_part, dtype=np.float64)
    q_part = np.empty(N_part, dtype=np.float64)

    # Также каждый процесс будет хранить свою часть
    # вспомогательного вектора `p` в массиве `p_part`.
    # Сразу инициализируем его нулём (на итерации s=0):
    p_part = np.zeros(N_part, dtype=np.float64)

    # Объявим также переменные `dot_vec` (для хранения окончательного
    # значения скалярного произведения векторов) и `dot_vec_temp`
    # (для хранения частичных сумм скалярного произведения векторов,
    # собранных на отдельных процессах для пополнения значения `dot_vec`).
    # В отличие от примера 5.1, в этот раз мы будем делать скалярное
    # умножение векторов на всех процессах, так что создадим
    # такие переменные также на всех процессах:
    dot_vec = np.array(0, dtype=np.float64)
    dot_vec_temp = np.empty(1, dtype=np.float64)

    # Цикл по итерациям s от 1 до N включительно:
    for s in range(1, N + 1):
        #------------------------------------------------------------
        # Шаг 1:
        # Обновим или создадим вектор `r`.
        if s == 1:
            # Сначала, на первой итерации s=1, нужно создать вектор `r`:
            #   r_{s} = A.T.dot(A.dot(x_{s}) - b)

            # Но сейчас у нас нет полного вектора `x` - и для
            # уменьшения коммуникаций между процессами, мы не
            # хотим его собирать из кусочков на каждом из процессов,
            # как делали это в примере 3.1.
            # Поэтому разобъём вычисление по формуле выше на
            # несколько этапов.

            # Используем тот факт, что в отличие от примера 5.1,
            # сейчас каждый процесс уже хранит нужную ему копию `x_part`.
            # Поэтому мы сразу можем сделать параллельно на каждом
            # процессе умножение матрицы `A_part`на вектор `x_part`:
            Ax_part_temp = np.dot(A_part, x_part)
            # И просуммировать полученные вектора по всем
            # процессам каждой строки сетки процессов, чтобы собрать
            # на каждом процессе свою часть вектора `Ax_part`,
            # соответствующего произведению `A.dot(x_{s})`.
            # Отметим, что при этом векторы `Ax_part` в каждом стролбце
            # сетки процессов будут просто повторяться:
            Ax_part = np.empty(M_part, dtype=np.float64)
            comm_row.Allreduce([Ax_part_temp, M_part, MPI.DOUBLE],
                               [Ax_part, M_part, MPI.DOUBLE], op=MPI.SUM)
            # Теперь на каждом процессе можно найти свою часть
            # вектора `A.dot(x_{s}) - b`. При этом вычисления
            # в каждом стролбце сетки процессов будут просто
            # повторяться - но зато не нужно будет передавать
            # найденный вектор своим соседям по строке процессов:
            Axb_part = Ax_part - b_part
            # Наконец, мы можем сделать сразу на всех процессах
            # параллельное умножение матрицы `A_part.T` на вектор
            # `Axb_part`:
            r_part_temp = np.dot(A_part.T, Axb_part)
            # И просуммировать полученные временные вектора по всем
            # процессам своего столбца сетки процессов, одновременно
            # раздав всем этим же процессам нужную им часть вектора `r_part`:
            comm_col.Allreduce([r_part_temp, N_part, MPI.DOUBLE],
                               [r_part, N_part, MPI.DOUBLE], op=MPI.SUM)
        else:
            # На всех последующих итерациях, вектор `r` нужно просто
            # обновить:
            #    dot_pq = dot(p_{s-1}, q_{s-1})
            #    r_{s} = r_{s-1} - q_{s-1} / dot_pq

            # Это можно сделать для каждого кусочка `r_part` отдельно,
            # параллельно на каждом процессе каждой строки сетки процессов
            # - нет нужды собирать полный вектор `r`.

            # Но первым делом нам нужно знать скалярное произведение
            # векторов `p_{s-1}` и `q_{s-1}`, вычисленных на предыдущей
            # итерации.
            # Важно, что именно это скалярное произведение мы уже вычислили
            # в самом конце предыдущей итерации и поместили его в переменную
            # `dot_vec`, где оно до сих пор ещё и хранится (одно и то же
            # значение на каждом процессе!).
            # Просто воспользуемся им для обновления `r_part`:
            r_part = r_part - q_part/dot_vec
        
        #------------------------------------------------------------
        # Шаг 2:
        # Посчитаем скалярное произведение векторов dot(r, r):
        #    dot_rr = dot(r_{s}, r_{s})
        # И обновим вектор `p`:
        #    p_{s} = p_{s-1} + r_{s} / dot_rr

        # Сделаем это независимо на процессах каждой строки
        # сетки процессов (повторяя для каждой строки одни
        # и те же расчёты - но без необходимости передавать
        # потом результаты этих расчётов на другие строки):
        dot_vec_temp[0] = np.dot(r_part, r_part)
        comm_row.Allreduce([dot_vec_temp, 1, MPI.DOUBLE],
                           [dot_vec, 1, MPI.DOUBLE], op=MPI.SUM)
        p_part = p_part + r_part/dot_vec

        #------------------------------------------------------------
        # Шаг 3:
        # Обновим вектор `q`:
        #    q_{s} = A.T.dot(A.dot(p_{s}))

        # Для минимизации объёма данных, которые мы должны будем
        # передавать между процессами, разобъём этот шаг на два этапа.
        # Сначала параллельно на каждом процессе умножим `A_part` на `p_part`
        # и соберём в пределах каждой строки нашей сетки процессов свою 
        # часть `Ap_part` такого произведения, раздав его всем процессам
        # в пределах своей строки сетки процессов:
        Ap_part_temp = np.dot(A_part, p_part)
        Ap_part = np.empty(M_part, dtype=np.float64)
        comm_row.Allreduce([Ap_part_temp, M_part, MPI.DOUBLE], 
                           [Ap_part, M_part, MPI.DOUBLE], op=MPI.SUM)
        # ВАЖНО: последняя команда пересылает *частичный* вектор
        #        с размером `M_part` *всем* процессам - такая операция
        #        плохо масштабируется, если `M_part` будет большим.
        #        Из-за этого данный алгоритм может хуже параллелизоваться,
        #        чем алгоритмы в примерах 3.1-3.2 при M >> N.

        # Теперь снова параллельно на каждом процессе умножим `A_part.T` 
        # на `Ap_part`:
        q_part_temp = np.dot(A_part.T, Ap_part)
        # И соберём - но на этот раз в пределах каждого *столбца*
        # нашей сетки процессов (из-за того, что `A_part.T` - это
        # *транспонированная* часть матрицы) свою часть `q_part`
        # такого произведения, раздав его копии всем процессам
        # каждого столбца:
        comm_col.Allreduce([q_part_temp, N_part, MPI.DOUBLE],
                           [q_part, N_part, MPI.DOUBLE], op=MPI.SUM)
        
        #------------------------------------------------------------
        # Шаг 4:
        # Посчитаем скалярное произведение векторов dot(p, q):
        #    dot_pq = dot(p_{s}, q_{s})
        # и обновим вектор `x`:
        #    x_{s+1} = x_{s} - p_{s} / dot_pq

        # Сделаем это одновременно на всех процессах (повторяя
        # одни и те же вычисления в каждой строке сетки процессов).
        # Первым делом, на каждом процессе получим скалярное
        # произведение кусочков `p_part` и `q_part`:
        dot_vec_temp[0] = np.dot(p_part, q_part)
        # и командой `comm_row.Allreduce()` сначала просуммируем все
        # полученные произведения (суффикс `reduce`!), собрав
        # их в скалярное произведение полных векторов `p` и `q`,
        # а затем раздадим полученное значение каждому процессу
        # (префикс `All`) из своей строки сетки процессов:
        comm_row.Allreduce([dot_vec_temp, 1, MPI.DOUBLE],
                           [dot_vec, 1, MPI.DOUBLE], op=MPI.SUM)

        # Сейчас мы, наконец, можем обновить `x_part` на каждом
        # процессе:
        x_part = x_part - p_part/dot_vec
        # Объединять эти кусочки в полный вектор `x` нет нужды -
        # в полном виде этот вектор будет нам нужен только
        # после завершения работы этой функции.

    return x_part


#------------------------------------------------------------------
# Начинаем выполнение программы - первым делом, настроим MPI:
#------------------------------------------------------------------

# Работаем с коммуникатором по всем доступным процессам:
comm = MPI.COMM_WORLD

# Число P доступных процессов в этом коммуникаторе:
P = comm.Get_size()

# Номер текущего процесса (от 0 до P-1):
rank = comm.Get_rank()

#------------------------------------------------------------------
# Шаг 0:
# Реализуемый в этот раз алгоритм будет работать даже на одном
# MPI процессе - процесс 0 в этот раз будет и командовать и работать!
# Однако (см. Шаг 2 ниже), он использует простую квадратную сетку
# разбиения матрицы на процессы, и поэтому требует использования R**2
# процессов, где R - положительное целое число. То есть, допустимое
# число процессов: 1, 4, 9, 16, 25, 36, 49, 64, 81, 100 и т.д.

if np.int32(np.sqrt(P))**2 != P:
    raise ValueError(
        "\nThe number of processes is not a square of some integer "
        "number. Please launch the program with the command:\n"
        ">> mpiexec.exe -n P python.exe Example-05-1.py\n"
        "where `P=R*R` with an integer `R`\n"
        "(say, P=1, 4, 9, 16, 25, 36, 49, 64, 81, 100)"
    )

#------------------------------------------------------------------
# Шаг 1 (точно такой же, как в примерах 1.1 и 2.2):
# Зачитаем из файла размер (M, N) матрицы A.
# Сделаем это только на процессе 0 - считаем,
# что входной файл доступен только на нём:

if rank == 0:
    with open('Example-03_in.dat', 'r') as f1:
        M = np.array(np.int32(f1.readline()))
        N = np.array(np.int32(f1.readline()))
else:
    # На "рабочих" процессах значение `M` не
    # используется - ставим пустую "заглушку":
    M = None
    # и подготавливаем на них "хранилище" для `N`:
    N = np.array(0, dtype=np.int32)

# Раздадим значение `N`, зачитанное процессом 0,
# всем остальным процессам. Это значение будет передаваться
# каждым процессом внутрь функции `conjugate_gradient_method()`
# для указания требуемого числа итераций:
comm.Bcast(N, root=0)
# Альтернативно, более общая форма записи:
# comm.Bcast([N, 1, MPI.INT], root=0)

# NOTE: Значение `M` не используется на "рабочих" процессах,
#       так что мы его не будем раздавать - лишняя трата времени.

#------------------------------------------------------------------
# Шаг 2:
# Считаем для простоты разбиения матрицы на блоки, что наше число
# процессов P=R*R является квадратом некоторого целого числа R:
R = np.int32(np.sqrt(P))

# Тогда будет естественно разбить матрицу на R*R блоков, так что
# число `num_col` столбцов блоков и число `num_row` строк блоков
# будет одинаковым и равным `R`:
num_col = num_row = R

# Разберёмся для нашей матрицы `A` с размером MxN, как именно
# `M` строк элементов матрицы должны разбиться на `num_row` блоков
# по вертикали - посчитаем для этого списки числа строк `rcounts_M`
# и их смещений `displs_M`.
# Точно так же разберёмся, как именно `N` столбцов элементов матрицы
# должны разбиться на `num_col` блоков по горизонтали - посчитаем для
# этого списки числа столбцов `rcounts_N` и их смещений `displs_N`.

if rank == 0:
    # Сделаем такой анализ только на процессе 0.
    # Всю логику расчёта этих списков мы перенесём в отдельную
    # функцию `auxiliary_arrays_determination()`, определённую
    # выше в этом файле:
    rcounts_M, displs_M = auxiliary_arrays_determination(M, num_row)
    rcounts_N, displs_N = auxiliary_arrays_determination(N, num_col)
else:
    # На "рабочих" процессах все эти списки используются только
    # в качестве пустых заглушек (в нескольких `Scatter` ниже):
    rcounts_M = displs_M = None
    rcounts_N = displs_N = None

# Подготовим "хранилища" для `M_part` и `N_part` на всех процессах:
M_part = np.array(0, dtype=np.int32)
N_part = np.array(0, dtype=np.int32)

# ВАЖНЫЙ МОМЕНТ:
# На каждом процессе создадим два новых коммуникатора,
# которые позволят общаться этому процессу со своими соседями
# в пределах одной и той же строки или одного и того же
# столбца.

# Один коммуникатор, `comm_col`, будет использоваться для
# общения данного процесса со всеми процессами, которые
# образуют один и тот же столбец блочной матрицы.
# Всего будет `num_row` процессов, связанных между собой
# таким образом - при этом всего будет создано `num_col`
# таких коммуникаторов:

color_col = rank % num_col
comm_col = comm.Split(color_col, rank)

# Например, для случая 9 процессов:
#
#          Processes     Blocks of matrix A
#          0  1  2       (0,0) (0,1) (0,2)
#          3  4  5  ==>  (1,0) (1,1) (1,2)
#          6  7  8       (2,0) (2,1) (2,2)
#
# мы получим три возможных значения для `color_col`
# (каждое такое значение создаёт новый коммуникатор):
#   0 для процессов 0, 3, 6 - они образуют первый столбец блочной матрицы
#   1 для процессов 1, 4, 7 - они образуют второй столбец блочной матрицы
#   2 для процессов 2, 5, 8 - они образуют третий столбец блочной матрицы

# Второй коммуникатор, `comm_row`, будет использоваться для
# общения данного процесса со всеми процессами, которые
# образуют одну и ту же строку блочной матрицы.
# Всего будет `num_col` процессов, связанных между собой
# таким образом - при этом всего будет создано `num_row`
# таких коммуникаторов:

color_row = rank // num_col
comm_row = comm.Split(color_row, rank)

# Например, для случая 9 процессов, мы получим три возможных значения
# для `color_row`:
#   0 для процессов 0, 1, 2 - они образуют первую строку блочной матрицы
#   1 для процессов 3, 4, 5 - они образуют вторую строку блочной матрицы
#   2 для процессов 6, 7, 8 - они образуют третью строку блочной матрицы

# Проверим созданные коммуникаторы:
# print(f"comm = {comm}")
# print(f"comm.Get_size() = {comm.Get_size()}")
# print(f"comm.Get_rank() = {comm.Get_rank()}")
# print(f"comm_col = {comm_col}")
# print(f"comm_col.Get_size() = {comm_col.Get_size()}")
# print(f"comm_col.Get_rank() = {comm_col.Get_rank()}")
# print(f"comm_row = {comm_row}")
# print(f"comm_row.Get_size() = {comm_row.Get_size()}")
# print(f"comm_row.Get_rank() = {comm_row.Get_rank()}")

# Для удобства, введём список процессов из первой строки
# сетки процессов:
procs_first_row = np.arange(num_col)
# и список процессов из первого столбца сетки процессов:
procs_first_col = np.arange(0, P, num_col)

# Разбросаем теперь список числа столбцов `rcounts_N`
# элементов в блоках матрицы в виде значений `N_part`
# по всем процессам, которые образуют первую строку
# блочной матрицы:
if rank in procs_first_row:
    comm_row.Scatter([rcounts_N, 1, MPI.INT], 
                     [N_part, 1, MPI.INT], root=0)

# И размножим полученное `N_part` (разное для разных столбцов,
# но одинаковое в пределах одного и того же столбца) на все
# процессы в пределах одного и того же столбца:
comm_col.Bcast([N_part, 1, MPI.INT], root=0)

# Аналогично, разбросаем теперь список числа строк
# `rcounts_M` элементов в блоках матрицы в виде значений
# `M_part` по всем процессам, которые образуют первый
# столбец блочной матрицы:
if rank in procs_first_col:
    comm_col.Scatter([rcounts_M, 1, MPI.INT],
                     [M_part, 1, MPI.INT], root=0)

# И размножим полученное `M_part` (разное для разных строк,
# но одинаковое в пределах одной и той же строки) на все
# процессы в пределах одной и той же строки:
comm_row.Bcast([M_part, 1, MPI.INT], root=0)

#------------------------------------------------------------------
# Шаг 3:
# Зачитаем из файла матрицу `A`. Как и в примере 2.2, мы будем
# экономить память - будем зачитывать матрицу по кусочкам,
# по возможности сразу отдавая каждый кусочек нужному процессу.

# Подготовим "хранилище" для кусочков `A_part` на всех процессах -
# каждое со своим числом строк `M_part` и столбцов `N_part`:
A_part = np.empty((M_part, N_part), dtype=np.float64)
# NOTE: В это раз процесс 0 рабочий - так что матрица `A_part`
# здесь также будет занимать память.

# Поскольку теперь мы должны разбросать матрицу `A` по процессам
# в виде блоков `A_part`, элементы для которых лежат в памяти
# не одним сплошным куском, то алгоритм работы существенно усложнится.

# Для облегчения его реализации нам будет удобно создавать временные
# вспомогательные коммуникаторы для обмена сообщения между временными
# подгруппами процессов, каждый из которых будут включать в себя
# процесс 0 (раздающий матрицу `A`) и все процессы, образующие одну
# из строк блочной матрицы.

# Для создания таких подгрупп нам нужно будет знать всю
# текущую группу процессов:
group = comm.Get_group()

if rank == 0:
    # Зачитаем на процессе 0 файл с матрицей `A` не сразу весь,
    # а по кусочкам - по возможности сразу отдавая каждый кусочек
    # своему "рабочему" процессу.
    #
    # Дополнительная сложность при этом заключается в том, что
    # теперь мы хотим, чтобы процесс 0 был также и "рабочим" -
    # то есть, он тоже должен получить свой кусочек матрицы `A`:
    with open('Example-03_AData.dat', 'r') as f2:
        # Цикл по строкам блоков матрицы `A`:
        for m in range(num_row):
            # Зачитаем сначала всю строку блоков `m` и поместим
            # её во временный массив `A_row_m`:
            A_row_m = np.empty(rcounts_M[m]*N, dtype=np.float64)
            # Цикл по строкам элементов матрицы `A`
            # в пределах строки `m` блоков матрицы `A`:
            for j in range(rcounts_M[m]):
                # Цикл по столбцам блоков матрицы `A`:
                for n in range(num_col):
                    # Цикл по столбцам элементов матрицы `A`
                    # в пределах столбца `n` блоков матрицы `A`:
                    for i in range(rcounts_N[n]):
                        # Индекс `k` внутри временной матрицы `A_row_m`
                        # для считываемого элемента матрицы `A`:
                        k = rcounts_M[m]*displs_N[n] + j*rcounts_N[n] + i
                        # Зачитаем этот элемент в нужную ячейку памяти:
                        A_row_m[k] = np.float64(f2.readline())
            # Мы наконец зачитали всю строку блоков `m` и поместили
            # её во временный массив `A_row_m`. Разбросаем эту строку
            # блоков по отдельным процессам, входящим в эту строку
            # сетки процессов.
            if m == 0:
                # Первую строку блоков разбросать легко - можно просто
                # воспользоваться уже созданным коммуникатором `comm_row` -
                # для процесса 0 (на котором мы сейчас работаем) он связывает
                # между собой все процессы, входящие в первую строку
                # сетки процессов.
                # Размеры и смещения для разбрасывания `A_row_m` по частям `A_part`:
                size_m = rcounts_M[m]*rcounts_N
                displ_m = rcounts_M[m]*displs_N
                # Процесс 0 разбросает `A_row_m` по частям, отдав свой кусочек
                # также и себе (то есть, он будет и посылать и получать данные):
                comm_row.Scatterv([A_row_m, size_m, displ_m, MPI.DOUBLE],
                                  [A_part, M_part*N_part, MPI.DOUBLE], root=0)
            else:
                # Вторую и последующие строки блоков разбросать сложнее -
                # для них нужно создать новый временный коммуникатор, который
                # будет включать в себя процесс 0 и все процессы, образующие
                # строку `m` сетки процессов.
                # Создадим сначала нужную подгруппу процессов так, чтобы
                # глобальный процесс 0 остался процессом 0 и в новой подгруппе:
                group_temp = group.Range_incl([(0,0,1), (m*num_col,(m+1)*num_col-1,1)])
                # Теперь создадим временный коммуникатор для этой подгруппы:
                comm_temp = comm.Create(group_temp)
                # Временная подгруппа процессов больше не нужна - освободим её
                # не откладывая, чтобы не забыть:
                group_temp.Free()
                # Размеры и смещения для разбрасывания `A_row_m` по частям `A_part` -
                # теперь вычисляются чуть сложнее:
                rcounts_N_temp = np.hstack((np.array(0, dtype=np.int32), rcounts_N))
                displs_N_temp = np.hstack((np.array(0, dtype=np.int32), displs_N))
                size_m_temp = rcounts_M[m]*rcounts_N_temp
                displ_m_temp = rcounts_M[m]*displs_N_temp
                # Пустая заглушка для процесса 0 - на этот раз он ничего получать не будет:
                A_empty = np.empty(0, dtype=np.float64)
                # Теперь процесс 0 разбросает `A_row_m` по частям всем процессам
                # временного коммуникатора, кроме себя:
                comm_temp.Scatterv([A_row_m, size_m_temp, displ_m_temp, MPI.DOUBLE],
                                   [A_empty, 0, MPI.DOUBLE], root=0)
                # Не забудем обязательно освободить временный коммуникатор:
                comm_temp.Free()
            # Поможем сборщику мусора поскорее найти мусор:
            del A_row_m
else:
    # Все остальные процессы должны просто получить свою часть матрицы
    # `A` и записать её в свой массив `A_part`.

    # Но здесь также возникают тонкости. Если процесс входит в состав
    # первой строки сетки процессов, то он должен получить данные
    # от процесса 0, который также входит в эту строку сетки процессов,
    # через коммуникатор `comm_row` для данной строки сетки процессов:
    if rank in procs_first_row:
        comm_row.Scatterv([None, None, None, None], 
                          [A_part, M_part*N_part, MPI.DOUBLE], root=0)
    # Все остальные процессы должны получить свои данные через временный
    # коммуникатор, который будет включать в себя глобальный процесс 0 и
    # все процессы, образующие строку `m` сетки процессов.
    for m in range(1, num_row):
        # Как мы уже делали для процесса 0, создадим сначала нужную подгруппу
        # процессов так, чтобы глобальный процесс 0 остался процессом 0 и
        # в новой подгруппе:
        group_temp = group.Range_incl([(0,0,1), (m*num_col,(m+1)*num_col-1,1)])
        # И создадим временный коммуникатор для этой подгруппы:
        comm_temp = comm.Create(group_temp)
        # Временная подгруппа процессов больше не нужна - освободим её
        # не откладывая, чтобы не забыть:
        group_temp.Free()

        # Теперь мы готовы принять данные, посланные нам процессом 0 через
        # этот временный коммуникатор:
        if rank in range(m*num_col, (m+1)*num_col):
            # Принимаем данные только на процессах, входящих в подгруппу
            # строки `m` сетки процессов - для всех других процессов созданный выше
            # `comm_temp` будет "чужим" - им нужно дождаться создания "своего"
            # временного коммуникатора:
            comm_temp.Scatterv([None, None, None, None], 
                               [A_part, M_part*N_part, MPI.DOUBLE], root=0)
            # Не забудем обязательно освободить временный коммуникатор:
            comm_temp.Free()

#------------------------------------------------------------------
# Шаг 4:
# Зачитаем из файла вектор `b`. Зачитаем его полностью на процессе 0
# (считаем, что входной файл доступен только на нём) - и потом
# раздадим его по кусочкам `b_part` всем процессам первого столбца
# сетки процессов.

if rank == 0:
    # Зачитаем файл `b` на процессе 0:
    b = np.empty(M, dtype=np.float64)
    with open('Example-03_bData.dat', 'r') as f3:
        for j in range(M):
            b[j] = np.float64(f3.readline())
else:
    # На всех остальных процессах вектор `b` используется
    # (в `comm_col.Scatterv` ниже) в качестве пустой заглушки:
    b = None

# На всех процессах (включая и сам процесс 0) будут храниться
# кусочки вектора `b` в виде векторов `b_part`. Подготовим
# "хранилище" для них:
b_part = np.empty(M_part, dtype=np.float64)

# Разбросаем зачитанный выше вектор `b` по всем процессам
# первого столбца сетки процессов в виде кусочков `b_part`
# с размерами `M_part = rcounts_M[m]`, используя смещения
# `displs_M[m]`:
if rank in procs_first_col:
    comm_col.Scatterv([b, rcounts_M, displs_M, MPI.DOUBLE],
                      [b_part, M_part, MPI.DOUBLE], root=0)

# И раздадим копии полученных `b_part` с процессов первого столбца
# (root=0) всем своим соседям в каждой строке сетки процессов:
comm_row.Bcast([b_part, M_part, MPI.DOUBLE], root=0)

#------------------------------------------------------------------
# Шаг 5:
# Подготовим "хранилища" для вектора `x`.
# Полная версия вектора `x` будет храниться только на процессе 0:
if rank == 0:
    # ВАЖНО: если вы знаете хорошее начальное приближение для `x`,
    # дайте его здесь!
    x = np.zeros(N, dtype=np.float64)
else:
    # На всех остальных процессах вектор `x` используется
    # (в `comm_row.Scatterv` ниже) в качестве пустой заглушки:
    x = None

# На всех процессах (включая и сам процесс 0) будут храниться
# кусочки вектора `x` в виде векторов `x_part`. Подготовим
# "хранилище" для них:
x_part = np.empty(N_part, dtype=np.float64)

# И разбросаем вектор `x` по всем процессам первой строки сетки процессов
# в виде кусочков `x_part` с размерами `N_part = rcounts_N[m]`,
# используя смещения `displs_N[m]`:
if rank in procs_first_row:
    comm_row.Scatterv([x, rcounts_N, displs_N, MPI.DOUBLE], 
                    [x_part, N_part, MPI.DOUBLE], root=0)

# Наконец, раздадим копии полученных `x_part` с процессов первой
# строки (root=0) всем своим соседям в каждом столбце сетки процессов:
comm_col.Bcast([x_part, N_part, MPI.DOUBLE], root=0)

#------------------------------------------------------------------
# Шаг 6:
# Собственно, и сама нужная нам работа - решение системы линейных
# уравнений итерационным методом сопряжённых градиентов.

# Вызываемый на каждом процессе нашей сетки процессов, этот алгоритм
# отработает со своими кусками массивов `A`, `b`, и `x` в виде `A_part`,
# `b_part`, и `x_part`, и возвратит свой кусок найденного решения
# `x_part` (детали описаны внутри самой функции):
x_part = conjugate_gradient_method(A_part, b_part, x_part, N_part, M_part,
                                   N, comm_row, comm_col)

# ВАЖНО: Реальные значения для `x_part` вернут все процессы из
# каждой строки сетки процессов - проверим это (детали описаны
# внутри самой функции):
print(f"Vector `x_part` on process {rank} consists of {len(x_part)} elements:")
print(f"  x_part = {x_part}")

# Соберём вектор `x` на процессе 0 (root=0) из кусочков `x_part`,
# присланных всеми процессами первой строки сетки процессов
# (хотя мы могли бы собрать такой полный вектор `x` из кусочков
# `x_part` любой строки сетки процессов - каждая строка хранит
# сейчас одинаковые копии `x_part`):
if rank in procs_first_row:
    comm_row.Gatherv([x_part, N_part, MPI.DOUBLE], 
                     [x, rcounts_N, displs_N, MPI.DOUBLE], root=0)

#------------------------------------------------------------------
# Шаг 7:
# Окончательно, нарисуем посчитанный вектор `x`, используя
# библиотеку matplotlib. Делаем это только на процессе 0.

if rank == 0:
    # Для контроля, напечатаем найденное решение на консоли:
    print(f"\nFinal solution:\nx = {x}")

    # Подготовим рисунок и данные для него:
    import matplotlib.pyplot as plt
    plt.style.use('dark_background')
    fig = plt.figure()
    ax = plt.axes(xlim=(0, N), ylim=(-1.5, 1.5))
    ax.set_xlabel('i'); ax.set_ylabel('x[i]')
    # индексы элементов вектора `x`:
    ii = np.arange(np.int32(N))

    # Нарисуем полное решение:
    #ax.plot(ii, x, '-y', lw=3)

    # А лучше, нарисуем кусочки, которые были посчитаны
    # на каждом отдельном процессе (поиграйтесь с числом
    # процессов!):
    for m in procs_first_row:
        procs = list(range(m, P, num_row))
        r = rcounts_N[m]
        d = displs_N[m]
        ax.plot(ii[d:d+r], x[d:d+r], '-', lw=3,
                label=f"Processes {procs} (x_part size {r})")
    plt.legend()
    plt.show()

#------------------------------------------------------------------
